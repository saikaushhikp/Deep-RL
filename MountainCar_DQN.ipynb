{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium\n",
        "# !pip install \"gymnasium[atari, accept-rom-license]\"\n",
        "# !pip install ale-py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BW20WK6efPK",
        "outputId": "2c39e82e-0619-4d8f-b676-1226f0dcc81b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "%%writefile MountainCar-DQN-a.py\n",
        "\"\"\"\n",
        "dqn_part1a_random.py\n",
        "\n",
        "Part (a) for the DQN assignment: load MountainCar-v0, print spaces, run a random agent,\n",
        "record rewards and successes, and save a plot of performance + action-choice scatter.\n",
        "\n",
        "Usage:\n",
        "    python3 dqn_part1a_random.py --environment \"MountainCar-v0\" --episodes 40 --mean_n 5 --seed 0\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import random\n",
        "from itertools import count\n",
        "import os\n",
        "\n",
        "def parse_args():\n",
        "    p = argparse.ArgumentParser(description=\"Running with different episodic counts and mean rewards\")\n",
        "    p.add_argument(\"--environment\", type=str, default=\"MountainCar-v0\", help=\"Gym environment id\")\n",
        "    p.add_argument(\"--episodes\", type=int, default=100, help=\"Number of episodes to run\")\n",
        "    p.add_argument(\"--mean_n\", type=int, default=5, help=\"n for rolling mean plot\")\n",
        "    p.add_argument(\"--seed\", type=int, default=None, help=\"Random seed (optional)\")\n",
        "    p.add_argument(\"--render\", action=\"store_true\", help=\"Render environment (slows down execution)\")\n",
        "    return p.parse_args()\n",
        "    # Fix for Colab: parse known arguments and ignore the rest\n",
        "    # args, unknown = p.parse_known_args()\n",
        "    # return args\n",
        "    # Simpler fix for this specific case in Colab: pass an empty list to parse_args\n",
        "    # return p.parse_args([])\n",
        "\n",
        "\n",
        "def safe_reset(env):\n",
        "    \"\"\"Handle gym vs gymnasium reset return types.\"\"\"\n",
        "    out = env.reset()\n",
        "    # gym classic: obs\n",
        "    # newer gym returns (obs, info)\n",
        "    if isinstance(out, tuple) and len(out) >= 1:\n",
        "        return out[0]\n",
        "    return out\n",
        "\n",
        "def safe_step(env, action):\n",
        "    \"\"\"Handle gym vs gymnasium step return types.\"\"\"\n",
        "    out = env.step(action)\n",
        "    # old: obs, reward, done, info\n",
        "    # new: obs, reward, terminated, truncated, info\n",
        "    if len(out) == 4:\n",
        "        obs, reward, done, info = out\n",
        "        return obs, reward, done, info\n",
        "    elif len(out) == 5:\n",
        "        obs, reward, terminated, truncated, info = out\n",
        "        done = terminated or truncated\n",
        "        return obs, reward, done, info\n",
        "    else:\n",
        "        raise RuntimeError(\"Unexpected step output format: len = {}\".format(len(out)))\n",
        "\n",
        "def get_state_tensor(obs):\n",
        "    \"\"\"Return numpy array representation for plotting / visualization. Kept simple.\"\"\"\n",
        "    return np.array(obs, dtype=np.float32)\n",
        "\n",
        "def select_random_action(action_space):\n",
        "    \"\"\"Return a single integer action chosen uniformly at random.\"\"\"\n",
        "    return action_space.sample()\n",
        "\n",
        "def plot_results(rewards_mean, steps, best_rewards_mean, env_name, file_name, action_scatter):\n",
        "    \"\"\"Create a 2-panel plot: performance and action choices scatter.\"\"\"\n",
        "    fig = plt.figure(figsize=(12,5))\n",
        "\n",
        "    # Performance subplot\n",
        "    ax1 = fig.add_subplot(121)\n",
        "    ax1.plot(steps, rewards_mean, label=f\"{len(steps)}-point rolling mean\")\n",
        "    ax1.plot(steps, best_rewards_mean, label=\"Best mean reward\")\n",
        "    ax1.grid(True)\n",
        "    ax1.set_xlabel(\"Total environment steps\")\n",
        "    ax1.set_ylabel(\"Reward (higher is better)\")\n",
        "    ax1.legend()\n",
        "    ax1.set_title(f\"Performance of random agent on {env_name}\")\n",
        "\n",
        "    # Action choices subplot\n",
        "    ax2 = fig.add_subplot(122)\n",
        "    # action_scatter: list of (pos, vel, action)\n",
        "    if len(action_scatter) > 0:\n",
        "        arr = np.array(action_scatter)\n",
        "        X = arr[:,0].astype(float)\n",
        "        Y = arr[:,1].astype(float)\n",
        "        Z = arr[:,2].astype(int)\n",
        "        # color map for 3 discrete actions\n",
        "        cmap = {0: 'lime', 1: 'red', 2: 'blue'}\n",
        "        colors = [cmap[int(a)] for a in Z]\n",
        "        ax2.scatter(X, Y, c=colors, s=12, alpha=0.7)\n",
        "        action_names = ['Left (0)', 'No-Op (1)', 'Right (2)']\n",
        "        # legend patches\n",
        "        legend_recs = [mpatches.Rectangle((0,0),1,1,fc=cmap[i]) for i in range(3)]\n",
        "        ax2.legend(legend_recs, action_names, loc='best')\n",
        "    ax2.set_title(\"Random agent action choices (sampled states)\")\n",
        "    ax2.set_xlabel(\"Position\")\n",
        "    ax2.set_ylabel(\"Velocity\")\n",
        "\n",
        "    plt.suptitle(f\"{env_name} - Random Agent Analysis\")\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.savefig(file_name, dpi=200)\n",
        "    print(f\"Saved plot to {file_name}\")\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "    return\n",
        "\n",
        "def run_random_agent(env_id, episodes, mean_n, seed=None, render=False):\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "        random.seed(seed)\n",
        "\n",
        "    env = gym.make(env_id)\n",
        "    # print spaces and ranges\n",
        "    print(\"Environment:\", env_id)\n",
        "    print(\"Observation space:\", env.observation_space)\n",
        "    print(\"Action space:\", env.action_space)\n",
        "\n",
        "    episode_rewards = []\n",
        "    best_reward = -float('inf')\n",
        "    rewards_mean = []\n",
        "    best_rewards_mean = []\n",
        "    steps = []\n",
        "\n",
        "    total_steps = 0\n",
        "    success_count = 0\n",
        "\n",
        "    action_scatter = []  # store (pos, vel, action) samples for plotting\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        obs = safe_reset(env)\n",
        "        state = get_state_tensor(obs)\n",
        "        total_reward = 0.0\n",
        "\n",
        "        for t in count():\n",
        "            if render:\n",
        "                env.render()\n",
        "\n",
        "            action = select_random_action(env.action_space)\n",
        "            # sample a few states for action-choices scatter (not every step)\n",
        "            if len(action_scatter) < 2000 and (total_steps % max(1, int(100/episodes)) == 0):\n",
        "                action_scatter.append((state[0], state[1], action))\n",
        "\n",
        "            obs, reward, done, info = safe_step(env, action)\n",
        "            state = get_state_tensor(obs)\n",
        "            total_reward += reward\n",
        "            total_steps += 1\n",
        "\n",
        "            if done or t >= 10000:  # safety cap\n",
        "                # MountainCar success condition: position >= 0.5 at termination\n",
        "                try:\n",
        "                    pos = state[0]\n",
        "                except:\n",
        "                    pos = None\n",
        "                if pos is not None and pos >= 0.5:\n",
        "                    success_count += 1\n",
        "                break\n",
        "\n",
        "        episode_rewards.append(total_reward)\n",
        "\n",
        "        if len(episode_rewards) >= mean_n:\n",
        "            present_mean = float(np.mean(episode_rewards[-mean_n:]))\n",
        "            rewards_mean.append(present_mean)\n",
        "            best_reward = max(present_mean, best_reward)\n",
        "            best_rewards_mean.append(best_reward)\n",
        "            steps.append(total_steps)\n",
        "\n",
        "        print(f\"Episode {ep+1}/{episodes} | Reward = {total_reward:.2f} | Successes so far = {success_count}\")\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    # prepare plot filename\n",
        "    fn = f\"{env_id}_random_{episodes}ep_mean{mean_n}.png\"\n",
        "    plot_results(rewards_mean, steps, best_rewards_mean, env_id, fn, action_scatter)\n",
        "\n",
        "    # Summarize observations\n",
        "    summary = {\n",
        "        \"total_episodes\": episodes,\n",
        "        \"mean_n\": mean_n,\n",
        "        \"final_mean_reward\": rewards_mean[-1] if rewards_mean else None,\n",
        "        \"best_mean_reward\": best_rewards_mean[-1] if best_rewards_mean else None,\n",
        "        \"success_count\": success_count,\n",
        "        \"total_steps\": total_steps\n",
        "    }\n",
        "    return summary\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = parse_args()\n",
        "    summary = run_random_agent(args.environment, args.episodes, args.mean_n, args.seed, args.render)\n",
        "    print(\"\\n=== Summary ===\")\n",
        "    for k, v in summary.items():\n",
        "        print(f\"{k}: {v}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hL6senMep85",
        "outputId": "a203e486-7831-40bf-d959-50bfb40a1afd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing MountainCar-DQN-a.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 MountainCar-DQN-a.py --environment \"MountainCar-v0\" --episodes 100 --mean_n 5 --seed 303\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4xqnADvfPmZ",
        "outputId": "1b0f2c0e-470c-435d-b0fb-9567ba89c551"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment: MountainCar-v0\n",
            "Observation space: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
            "Action space: Discrete(3)\n",
            "Episode 1/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 2/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 3/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 4/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 5/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 6/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 7/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 8/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 9/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 10/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 11/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 12/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 13/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 14/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 15/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 16/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 17/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 18/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 19/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 20/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 21/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 22/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 23/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 24/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 25/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 26/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 27/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 28/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 29/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 30/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 31/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 32/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 33/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 34/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 35/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 36/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 37/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 38/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 39/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 40/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 41/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 42/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 43/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 44/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 45/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 46/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 47/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 48/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 49/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 50/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 51/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 52/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 53/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 54/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 55/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 56/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 57/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 58/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 59/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 60/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 61/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 62/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 63/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 64/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 65/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 66/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 67/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 68/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 69/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 70/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 71/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 72/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 73/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 74/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 75/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 76/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 77/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 78/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 79/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 80/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 81/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 82/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 83/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 84/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 85/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 86/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 87/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 88/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 89/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 90/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 91/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 92/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 93/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 94/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 95/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 96/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 97/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 98/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 99/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 100/100 | Reward = -200.00 | Successes so far = 0\n",
            "Saved plot to MountainCar-v0_random_100ep_mean5.png\n",
            "Figure(1200x500)\n",
            "\n",
            "=== Summary ===\n",
            "total_episodes: 100\n",
            "mean_n: 5\n",
            "final_mean_reward: -200.0\n",
            "best_mean_reward: -200.0\n",
            "success_count: 0\n",
            "total_steps: 20000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## part (b)"
      ],
      "metadata": {
        "id": "ledtqRUcjFRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "%%writefile MountainCar-DQN-b.py\n",
        "\"\"\"\n",
        "Part (b) — Deep Q-Network (DQN) implementation for MountainCar-v0\n",
        "Based on assignment guidelines and optimized for reward shaping and convergence.\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import random\n",
        "from itertools import count\n",
        "from collections import namedtuple, deque\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import argparse\n",
        "import pandas as pd\n",
        "\n",
        "# ------------------------- #\n",
        "# Argument Parsing\n",
        "# ------------------------- #\n",
        "parser = argparse.ArgumentParser(description=\"DQN for MountainCar-v0\")\n",
        "parser.add_argument(\"--environment\", type=str, default=\"MountainCar-v0\")\n",
        "parser.add_argument(\"--num_episodes\", type=int, default=200)\n",
        "parser.add_argument(\"--batch\", type=int, default=64)\n",
        "parser.add_argument(\"--gamma\", type=float, default=0.99)\n",
        "parser.add_argument(\"--learning_rate\", type=float, default=1e-3)\n",
        "parser.add_argument(\"--mean_n\", type=int, default=5)\n",
        "parser.add_argument(\"--seed\", type=int, default=0)\n",
        "args = parser.parse_args()\n",
        "\n",
        "# ------------------------- #\n",
        "# Setup\n",
        "# ------------------------- #\n",
        "env = gym.make(args.environment)\n",
        "env.reset(seed=args.seed)\n",
        "np.random.seed(args.seed)\n",
        "random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ------------------------- #\n",
        "# DQN Model Definition\n",
        "# ------------------------- #\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_dim=2, hidden_dim=200, action_dim=3):\n",
        "        super(DQN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, action_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# ------------------------- #\n",
        "# Replay Memory\n",
        "# ------------------------- #\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "class ReplayMemory:\n",
        "    def __init__(self, capacity=10000):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "# ------------------------- #\n",
        "# Helper Functions\n",
        "# ------------------------- #\n",
        "def get_state(obs):\n",
        "    \"\"\"Convert observation to tensor.\"\"\"\n",
        "    s = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "    return s\n",
        "\n",
        "def select_action(state, eps_threshold):\n",
        "    \"\"\"ε-greedy action selection.\"\"\"\n",
        "    if random.random() > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            return policy_net(state).argmax(dim=1).view(1, 1)\n",
        "    else:\n",
        "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
        "\n",
        "def optimize_model():\n",
        "    \"\"\"Perform a single optimization step.\"\"\"\n",
        "    if len(memory) < batch_size:\n",
        "        return\n",
        "    transitions = memory.sample(batch_size)\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # Prepare batch tensors\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)),\n",
        "                                  device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
        "\n",
        "    # Compute Q(s_t, a)\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Compute V(s_{t+1})\n",
        "    next_state_values = torch.zeros(batch_size, device=device)\n",
        "    with torch.no_grad():\n",
        "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
        "    expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
        "\n",
        "    loss = F.mse_loss(state_action_values.squeeze(), expected_state_action_values)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "# ------------------------- #\n",
        "# Training Loop\n",
        "# ------------------------- #\n",
        "n_actions = env.action_space.n\n",
        "batch_size = args.batch\n",
        "gamma = args.gamma\n",
        "num_episodes = args.num_episodes\n",
        "lr = args.learning_rate\n",
        "mean_n = args.mean_n\n",
        "\n",
        "policy_net = DQN().to(device)\n",
        "target_net = DQN().to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
        "memory = ReplayMemory(10000)\n",
        "\n",
        "eps_start, eps_end, eps_decay = 1.0, 0.02, 800\n",
        "target_update = 10\n",
        "initial_memory = 1000\n",
        "total_steps = 0\n",
        "\n",
        "rewards_list, mean_rewards, best_mean_rewards, steps_list = [], [], [], []\n",
        "best_mean = -float(\"inf\")\n",
        "successes = 0\n",
        "\n",
        "for i_episode in range(num_episodes):\n",
        "    obs, _ = env.reset()\n",
        "    state = get_state(obs)\n",
        "    total_reward = 0.0\n",
        "\n",
        "    for t in count():\n",
        "        eps_threshold = eps_end + (eps_start - eps_end) * math.exp(-1. * total_steps / eps_decay)\n",
        "        action = select_action(state, eps_threshold)\n",
        "        obs_next, reward, done, truncated, _ = env.step(action.item())\n",
        "        done = done or truncated\n",
        "\n",
        "        # --- Reward shaping: encourage forward progress and momentum ---\n",
        "        # Standard MountainCar reward is -1 per step. Goal is at position >= 0.5.\n",
        "        # Reward shaping can help the agent learn faster.\n",
        "        # shaped_reward = -1.0 # Base penalty per step\n",
        "\n",
        "        # Encourage moving towards the goal (positive position)\n",
        "        # The closer to 0.5, the higher the shaped_reward\n",
        "        # shaped_reward += obs_next[0]\n",
        "\n",
        "        # Encourage positive velocity (moving right)\n",
        "        # shaped_reward += abs(obs_next[1]) * 10 # Reward for velocity magnitude\n",
        "\n",
        "        # Another common shaping: reward proportional to position and velocity\n",
        "        # This encourages moving right (positive pos) and building momentum (positive vel)\n",
        "        shaped_reward = obs_next[0] + 0.5 + abs(obs_next[1]*10)\n",
        "\n",
        "        # Additional reward for reaching the goal\n",
        "        if obs_next[0] >= 0.5:\n",
        "            shaped_reward += 100 # Large bonus for success\n",
        "\n",
        "\n",
        "        # Convert to tensor\n",
        "        reward_tensor = torch.tensor([shaped_reward], device=device)\n",
        "\n",
        "        next_state = None if done else get_state(obs_next)\n",
        "        memory.push(state, action, next_state, reward_tensor)\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward # Keep track of the TRUE environment reward\n",
        "        total_steps += 1\n",
        "\n",
        "        if total_steps > initial_memory:\n",
        "            optimize_model()\n",
        "\n",
        "        if done:\n",
        "            if obs_next[0] >= 0.5:\n",
        "                successes += 1\n",
        "            break\n",
        "\n",
        "    rewards_list.append(total_reward)\n",
        "\n",
        "    if i_episode >= mean_n:\n",
        "        mean_r = np.mean(rewards_list[-mean_n:])\n",
        "        mean_rewards.append(mean_r)\n",
        "        best_mean = max(best_mean, mean_r)\n",
        "        best_mean_rewards.append(best_mean)\n",
        "        steps_list.append(total_steps)\n",
        "\n",
        "    if i_episode % target_update == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "    print(f\"Episode {i_episode+1}/{num_episodes} | Reward: {total_reward:.2f} | \"\n",
        "          f\"Success: {successes} | ε={eps_threshold:.3f}\")\n",
        "\n",
        "# ------------------------- #\n",
        "# Plotting\n",
        "# ------------------------- #\n",
        "env.close()\n",
        "\n",
        "file_name = f\"{args.environment}_DQN_{num_episodes}_episodes.png\"\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Performance Plot\n",
        "# Ensure steps_list and mean_rewards are not empty\n",
        "if steps_list and mean_rewards:\n",
        "    ax1.plot(steps_list, mean_rewards, label=f\"{mean_n}-episode mean\")\n",
        "    ax1.plot(steps_list, best_mean_rewards, label=\"Best mean reward\")\n",
        "ax1.set_xlabel(\"Steps\")\n",
        "ax1.set_ylabel(\"Reward\")\n",
        "ax1.set_title(\"DQN Performance on MountainCar-v0\")\n",
        "ax1.grid(True)\n",
        "ax1.legend()\n",
        "\n",
        "# Action-choice Plot\n",
        "# X = np.random.uniform(-1.2, 0.6, 1000) # Original observation space ranges\n",
        "# Y = np.random.uniform(-0.07, 0.07, 1000)\n",
        "# Using slightly expanded ranges for better visualization of policy boundaries\n",
        "X = np.linspace(-1.5, 0.6, 50) # Sample more systematically\n",
        "Y = np.linspace(-1, 1, 50)\n",
        "X, Y = np.meshgrid(X, Y)\n",
        "X, Y = X.flatten(), Y.flatten()\n",
        "\n",
        "\n",
        "Z = []\n",
        "# Use policy_net to determine the action for each state\n",
        "states_to_evaluate = torch.tensor(np.array(list(zip(X, Y))), dtype=torch.float32, device=device)\n",
        "with torch.no_grad():\n",
        "    actions_tensor = policy_net(states_to_evaluate).argmax(dim=1).cpu().numpy() # Get index of max Q-value\n",
        "Z = actions_tensor\n",
        "\n",
        "colors = ['lime', 'red', 'blue'] # 0: left, 1: no-op, 2: right\n",
        "ax2.scatter(X, Y, c=[colors[z] for z in Z], s=1, alpha=0.7)\n",
        "ax2.set_xlabel(\"Position\")\n",
        "ax2.set_ylabel(\"Velocity\")\n",
        "ax2.set_title(\"Trained DQN Action Choices\")\n",
        "legend_recs = [mpatches.Patch(color=colors[i], label=f\"Action {i}\") for i in range(3)]\n",
        "ax2.legend(handles=legend_recs)\n",
        "ax2.set_xlim([-1.5, 0.6])\n",
        "ax2.set_ylim([-1, 1])\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(file_name, dpi=200)\n",
        "print(f\"Saved plot to {file_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djzEyIi4jE2u",
        "outputId": "c9eb1c80-fa3d-4bbb-88ec-775fa6ce4cab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing MountainCar-DQN-b.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 MountainCar-DQN-b.py --environment=\"MountainCar-v0\" --num_episodes=100 --batch=8 --gamma=0.9 --learning_rate=1e-4 --mean_n=5\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pl7h-u9DjUwH",
        "outputId": "0d8bbd26-c096-4c34-eaa9-2aa17dc0d6fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1/100 | Reward: -200.00 | Success: 0 | ε=0.784\n",
            "Episode 2/100 | Reward: -200.00 | Success: 0 | ε=0.615\n",
            "Episode 3/100 | Reward: -200.00 | Success: 0 | ε=0.483\n",
            "Episode 4/100 | Reward: -200.00 | Success: 0 | ε=0.381\n",
            "Episode 5/100 | Reward: -200.00 | Success: 0 | ε=0.301\n",
            "Episode 6/100 | Reward: -200.00 | Success: 0 | ε=0.239\n",
            "Episode 7/100 | Reward: -200.00 | Success: 0 | ε=0.191\n",
            "Episode 8/100 | Reward: -200.00 | Success: 0 | ε=0.153\n",
            "Episode 9/100 | Reward: -200.00 | Success: 0 | ε=0.123\n",
            "Episode 10/100 | Reward: -200.00 | Success: 0 | ε=0.101\n",
            "Episode 11/100 | Reward: -200.00 | Success: 0 | ε=0.083\n",
            "Episode 12/100 | Reward: -200.00 | Success: 0 | ε=0.069\n",
            "Episode 13/100 | Reward: -200.00 | Success: 0 | ε=0.058\n",
            "Episode 14/100 | Reward: -200.00 | Success: 0 | ε=0.050\n",
            "Episode 15/100 | Reward: -200.00 | Success: 0 | ε=0.043\n",
            "Episode 16/100 | Reward: -200.00 | Success: 0 | ε=0.038\n",
            "Episode 17/100 | Reward: -200.00 | Success: 0 | ε=0.034\n",
            "Episode 18/100 | Reward: -200.00 | Success: 0 | ε=0.031\n",
            "Episode 19/100 | Reward: -200.00 | Success: 0 | ε=0.028\n",
            "Episode 20/100 | Reward: -200.00 | Success: 0 | ε=0.027\n",
            "Episode 21/100 | Reward: -200.00 | Success: 0 | ε=0.025\n",
            "Episode 22/100 | Reward: -200.00 | Success: 0 | ε=0.024\n",
            "Episode 23/100 | Reward: -200.00 | Success: 0 | ε=0.023\n",
            "Episode 24/100 | Reward: -200.00 | Success: 0 | ε=0.022\n",
            "Episode 25/100 | Reward: -200.00 | Success: 0 | ε=0.022\n",
            "Episode 26/100 | Reward: -200.00 | Success: 0 | ε=0.021\n",
            "Episode 27/100 | Reward: -200.00 | Success: 0 | ε=0.021\n",
            "Episode 28/100 | Reward: -200.00 | Success: 0 | ε=0.021\n",
            "Episode 29/100 | Reward: -200.00 | Success: 0 | ε=0.021\n",
            "Episode 30/100 | Reward: -200.00 | Success: 0 | ε=0.021\n",
            "Episode 31/100 | Reward: -166.00 | Success: 1 | ε=0.020\n",
            "Episode 32/100 | Reward: -200.00 | Success: 1 | ε=0.020\n",
            "Episode 33/100 | Reward: -200.00 | Success: 1 | ε=0.020\n",
            "Episode 34/100 | Reward: -194.00 | Success: 2 | ε=0.020\n",
            "Episode 35/100 | Reward: -200.00 | Success: 2 | ε=0.020\n",
            "Episode 36/100 | Reward: -200.00 | Success: 2 | ε=0.020\n",
            "Episode 37/100 | Reward: -200.00 | Success: 2 | ε=0.020\n",
            "Episode 38/100 | Reward: -200.00 | Success: 2 | ε=0.020\n",
            "Episode 39/100 | Reward: -200.00 | Success: 2 | ε=0.020\n",
            "Episode 40/100 | Reward: -200.00 | Success: 2 | ε=0.020\n",
            "Episode 41/100 | Reward: -200.00 | Success: 2 | ε=0.020\n",
            "Episode 42/100 | Reward: -200.00 | Success: 2 | ε=0.020\n",
            "Episode 43/100 | Reward: -142.00 | Success: 3 | ε=0.020\n",
            "Episode 44/100 | Reward: -200.00 | Success: 3 | ε=0.020\n",
            "Episode 45/100 | Reward: -200.00 | Success: 3 | ε=0.020\n",
            "Episode 46/100 | Reward: -200.00 | Success: 3 | ε=0.020\n",
            "Episode 47/100 | Reward: -170.00 | Success: 4 | ε=0.020\n",
            "Episode 48/100 | Reward: -148.00 | Success: 5 | ε=0.020\n",
            "Episode 49/100 | Reward: -196.00 | Success: 6 | ε=0.020\n",
            "Episode 50/100 | Reward: -154.00 | Success: 7 | ε=0.020\n",
            "Episode 51/100 | Reward: -200.00 | Success: 7 | ε=0.020\n",
            "Episode 52/100 | Reward: -200.00 | Success: 7 | ε=0.020\n",
            "Episode 53/100 | Reward: -200.00 | Success: 7 | ε=0.020\n",
            "Episode 54/100 | Reward: -200.00 | Success: 7 | ε=0.020\n",
            "Episode 55/100 | Reward: -200.00 | Success: 7 | ε=0.020\n",
            "Episode 56/100 | Reward: -200.00 | Success: 7 | ε=0.020\n",
            "Episode 57/100 | Reward: -200.00 | Success: 7 | ε=0.020\n",
            "Episode 58/100 | Reward: -200.00 | Success: 7 | ε=0.020\n",
            "Episode 59/100 | Reward: -122.00 | Success: 8 | ε=0.020\n",
            "Episode 60/100 | Reward: -142.00 | Success: 9 | ε=0.020\n",
            "Episode 61/100 | Reward: -131.00 | Success: 10 | ε=0.020\n",
            "Episode 62/100 | Reward: -132.00 | Success: 11 | ε=0.020\n",
            "Episode 63/100 | Reward: -200.00 | Success: 11 | ε=0.020\n",
            "Episode 64/100 | Reward: -138.00 | Success: 12 | ε=0.020\n",
            "Episode 65/100 | Reward: -200.00 | Success: 12 | ε=0.020\n",
            "Episode 66/100 | Reward: -124.00 | Success: 13 | ε=0.020\n",
            "Episode 67/100 | Reward: -155.00 | Success: 14 | ε=0.020\n",
            "Episode 68/100 | Reward: -141.00 | Success: 15 | ε=0.020\n",
            "Episode 69/100 | Reward: -119.00 | Success: 16 | ε=0.020\n",
            "Episode 70/100 | Reward: -145.00 | Success: 17 | ε=0.020\n",
            "Episode 71/100 | Reward: -173.00 | Success: 18 | ε=0.020\n",
            "Episode 72/100 | Reward: -164.00 | Success: 19 | ε=0.020\n",
            "Episode 73/100 | Reward: -141.00 | Success: 20 | ε=0.020\n",
            "Episode 74/100 | Reward: -148.00 | Success: 21 | ε=0.020\n",
            "Episode 75/100 | Reward: -200.00 | Success: 21 | ε=0.020\n",
            "Episode 76/100 | Reward: -146.00 | Success: 22 | ε=0.020\n",
            "Episode 77/100 | Reward: -160.00 | Success: 23 | ε=0.020\n",
            "Episode 78/100 | Reward: -159.00 | Success: 24 | ε=0.020\n",
            "Episode 79/100 | Reward: -200.00 | Success: 24 | ε=0.020\n",
            "Episode 80/100 | Reward: -162.00 | Success: 25 | ε=0.020\n",
            "Episode 81/100 | Reward: -185.00 | Success: 26 | ε=0.020\n",
            "Episode 82/100 | Reward: -144.00 | Success: 27 | ε=0.020\n",
            "Episode 83/100 | Reward: -109.00 | Success: 28 | ε=0.020\n",
            "Episode 84/100 | Reward: -135.00 | Success: 29 | ε=0.020\n",
            "Episode 85/100 | Reward: -151.00 | Success: 30 | ε=0.020\n",
            "Episode 86/100 | Reward: -148.00 | Success: 31 | ε=0.020\n",
            "Episode 87/100 | Reward: -154.00 | Success: 32 | ε=0.020\n",
            "Episode 88/100 | Reward: -119.00 | Success: 33 | ε=0.020\n",
            "Episode 89/100 | Reward: -151.00 | Success: 34 | ε=0.020\n",
            "Episode 90/100 | Reward: -152.00 | Success: 35 | ε=0.020\n",
            "Episode 91/100 | Reward: -151.00 | Success: 36 | ε=0.020\n",
            "Episode 92/100 | Reward: -128.00 | Success: 37 | ε=0.020\n",
            "Episode 93/100 | Reward: -160.00 | Success: 38 | ε=0.020\n",
            "Episode 94/100 | Reward: -154.00 | Success: 39 | ε=0.020\n",
            "Episode 95/100 | Reward: -159.00 | Success: 40 | ε=0.020\n",
            "Episode 96/100 | Reward: -137.00 | Success: 41 | ε=0.020\n",
            "Episode 97/100 | Reward: -150.00 | Success: 42 | ε=0.020\n",
            "Episode 98/100 | Reward: -150.00 | Success: 43 | ε=0.020\n",
            "Episode 99/100 | Reward: -151.00 | Success: 44 | ε=0.020\n",
            "Episode 100/100 | Reward: -140.00 | Success: 45 | ε=0.020\n",
            "Saved plot to MountainCar-v0_DQN_100_episodes.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# part (c)"
      ],
      "metadata": {
        "id": "t5CMowyPK7F7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gggJblM6K9bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rMjI8hJbefB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HRBTbpEzee_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Q4BTmbo7ee8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "d2pHR8MSee6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rC2jZdX5ee3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ho3aJMBMee1M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vTzcdpDmeey7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kVxs-pmAeewV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "krYIEgRDeety"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2xkaOfhneeqz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IfRRMc7Yeen3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGeh2tIV3xAW"
      },
      "outputs": [],
      "source": [
        "# !pip install gymnasium\n",
        "# !pip install \"gymnasium[atari, accept-rom-license]\"\n",
        "# !pip install ale-py\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import gymnasium as gym\n",
        "# import time\n",
        "\n",
        "# def inspect_environment(env_name):\n",
        "#     \"\"\"Loads an environment and prints its action and observation space.\"\"\"\n",
        "#     print(f\"--- Inspecting Environment: {env_name} ---\")\n",
        "\n",
        "#     # Load the environment\n",
        "#     # For Pong, render_mode='human' will open a window\n",
        "#     try:\n",
        "#         env = gym.make(env_name, render_mode='human' if 'Pong' in env_name else None)\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error loading {env_name}: {e}\")\n",
        "#         print(\"For 'Pong-v0', did you run: pip install \\\"gymnasium[atari, accept-rom-license]\\\"?\")\n",
        "#         print(\"-\" * 40 + \"\\n\")\n",
        "#         return\n",
        "\n",
        "#     # 1. Print Observation Space (State)\n",
        "#     print(f\"Observation Space (State): {env.observation_space}\")\n",
        "#     print(f\"  - Shape: {env.observation_space.shape}\")\n",
        "#     print(f\"  - Sample State: {env.observation_space.sample()}\\n\")\n",
        "\n",
        "#     # 2. Print Action Space\n",
        "#     print(f\"Action Space: {env.action_space}\")\n",
        "#     print(f\"  - Type: {type(env.action_space)}\")\n",
        "\n",
        "#     # For Discrete spaces, print the number of actions\n",
        "#     if isinstance(env.action_space, gym.spaces.Discrete):\n",
        "#         print(f\"  - Number of Actions: {env.action_space.n}\")\n",
        "\n",
        "#     # For Atari envs, we can get action meanings\n",
        "#     if hasattr(env.unwrapped, 'get_action_meanings'):\n",
        "#         print(f\"  - Action Meanings: {env.unwrapped.get_action_meanings()}\")\n",
        "\n",
        "#     env.close()\n",
        "#     print(\"-\" * 40 + \"\\n\")\n",
        "\n",
        "# # --- Run the Inspection ---\n",
        "# inspect_environment('MountainCar-v0')\n",
        "# # inspect_environment('Pong-v0')"
      ],
      "metadata": {
        "id": "55BzUXDOj1tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import gymnasium as gym\n",
        "# import time\n",
        "\n",
        "# def run_random_agent(env_name, episodes=10):\n",
        "#     \"\"\"Runs a random agent for a few episodes and prints the total reward.\"\"\"\n",
        "#     print(f\"--- Running Random Agent on: {env_name} ---\")\n",
        "\n",
        "#     # Using render_mode='human' to watch the agent play\n",
        "#     env = gym.make(env_name) # Removed render_mode='human'\n",
        "\n",
        "#     for episode in range(episodes):\n",
        "#         # Reset the environment to get the initial state\n",
        "#         observation, info = env.reset() # Modified to unpack both observation and info\n",
        "#         total_reward = 0\n",
        "#         done = False\n",
        "\n",
        "#         while not done:\n",
        "#             # 1. Choose a random action\n",
        "#             # This is the correct way to select a random action\n",
        "#             action = env.action_space.sample()\n",
        "\n",
        "#             # 2. Take the action and get the new state, reward, and done flag\n",
        "#             # Note: gym.step() returns (observation, reward, terminated, truncated, info)\n",
        "#             # 'done' is True if either terminated or truncated is True.\n",
        "#             observation, reward, terminated, truncated, info = env.step(action)\n",
        "#             done = terminated or truncated\n",
        "\n",
        "#             # 3. Accumulate the reward\n",
        "#             total_reward += reward\n",
        "\n",
        "#             # 4. Render the environment to watch\n",
        "#             # env.render() # Removed render as it might not be supported\n",
        "\n",
        "#             # A small delay so we can see what's happening\n",
        "#             if \"Pong\" in env_name:\n",
        "#                 time.sleep(0.01)\n",
        "\n",
        "#         print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
        "\n",
        "#     env.close()\n",
        "#     print(\"-\" * 40 + \"\\n\")\n",
        "\n",
        "# # --- Run the Random Agent Test ---\n",
        "# run_random_agent('MountainCar-v0', episodes=400)\n",
        "# # run_random_agent('Pong-v0', episodes=5) # 5 is enough for Pong"
      ],
      "metadata": {
        "id": "MQ5Smm-bkDvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import gymnasium as gym\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# import matplotlib.patches as mpatches\n",
        "# import random\n",
        "# import time\n",
        "# import warnings\n",
        "\n",
        "# # Suppress warnings from pandas/matplotlib if any\n",
        "# warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# def plot_episode_rewards(rewards_list, env_name):\n",
        "#     \"\"\"\n",
        "#     Plots the total reward received in each episode.\n",
        "#     For a random agent, this should be a noisy, flat line.\n",
        "#     \"\"\"\n",
        "#     plt.figure(figsize=(10, 5))\n",
        "#     plt.plot(rewards_list, alpha=0.7, label='Episode Reward')\n",
        "#     plt.title(f\"Total Reward per Episode (Random Agent) - {env_name}\")\n",
        "#     plt.xlabel(\"Episode Number\")\n",
        "#     plt.ylabel(\"Total Reward\")\n",
        "#     plt.grid(True)\n",
        "\n",
        "#     # Calculate and show the mean reward\n",
        "#     mean_reward = np.mean(rewards_list)\n",
        "#     plt.axhline(mean_reward, color='red', linestyle='--', label=f'Mean Reward: {mean_reward:.2f}')\n",
        "#     plt.legend()\n",
        "\n",
        "#     filename = f\"{env_name}_random_agent_rewards.png\"\n",
        "#     # plt.savefig(filename)\n",
        "#     # print(f\"Saved reward plot to {filename}\")\n",
        "#     plt.show()\n",
        "\n",
        "# def plot_random_policy_mountaincar():\n",
        "#     \"\"\"\n",
        "#     Visualizes the policy of a random agent for MountainCar-v0.\n",
        "#     It shows which action (left, nothing, right) a random agent would pick\n",
        "#     at various randomly sampled states.\n",
        "#     \"\"\"\n",
        "#     print(\"Generating random policy plot for MountainCar-v0...\")\n",
        "#     plt.figure(figsize=(10, 6))\n",
        "#     ax = plt.subplot(111)\n",
        "\n",
        "#     # Define the action colors and labels\n",
        "#     colour_map = {0: 'red', 1: 'lime', 2: 'blue'}\n",
        "#     action_choices = ['Push Left (0)', 'No Push (1)', 'Push Right (2)']\n",
        "\n",
        "#     # Sample 1000 random states from the observation space\n",
        "#     # State = [position, velocity]\n",
        "#     # Pos range: -1.2 to 0.6\n",
        "#     # Vel range: -0.07 to 0.07\n",
        "#     X_pos = np.random.uniform(-1.2, 0.6, 1000)\n",
        "#     Y_vel = np.random.uniform(-0.07, 0.07, 1000)\n",
        "\n",
        "#     # For each state, get a random action (0, 1, or 2)\n",
        "#     # This IS the random policy\n",
        "#     Z_action = [random.randrange(3) for _ in range(1000)]\n",
        "\n",
        "#     # Map actions to colors\n",
        "#     colors = [colour_map[action] for action in Z_action]\n",
        "\n",
        "#     # Create the scatter plot\n",
        "#     ax.scatter(X_pos, Y_vel, c=colors, alpha=0.6, s=10) # s=10 for smaller dots\n",
        "\n",
        "#     ax.set_title(\"Random Agent 'Policy' for MountainCar-v0\")\n",
        "#     ax.set_xlabel(\"Position (Car's x-coordinate)\")\n",
        "#     ax.set_ylabel(\"Velocity (Car's velocity)\")\n",
        "\n",
        "#     # Create a custom legend\n",
        "#     legend_patches = [mpatches.Patch(color=colour_map[i], label=action_choices[i]) for i in range(3)]\n",
        "#     ax.legend(handles=legend_patches, loc='best')\n",
        "\n",
        "#     filename = \"MountainCar-v0_random_policy.png\"\n",
        "#     # plt.savefig(filename)\n",
        "#     # print(f\"Saved policy plot to {filename}\")\n",
        "#     plt.show()\n",
        "\n",
        "\n",
        "# def run_random_agent_with_plotting(env_name, episodes=100):\n",
        "#     \"\"\"\n",
        "#     Runs a random agent for 'episodes' steps and gathers data for plotting.\n",
        "#     Set episodes higher (e.g., 100) for a better average.\n",
        "#     \"\"\"\n",
        "#     print(f\"--- Running Random Agent on: {env_name} ---\")\n",
        "\n",
        "#     # Set render_mode=None to run faster. No need to watch it.\n",
        "#     try:\n",
        "#         env = gym.make(env_name, render_mode=None)\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error loading {env_name}: {e}\")\n",
        "#         print(\"For 'Pong-v0', make sure you have run: pip install \\\"gymnasium[atari, accept-rom-license]\\\"\")\n",
        "#         print(\"-\" * 40 + \"\\n\")\n",
        "#         return\n",
        "\n",
        "#     episode_rewards = [] # Store rewards for plotting\n",
        "\n",
        "#     for episode in range(episodes):\n",
        "#         env.reset()\n",
        "#         total_reward = 0\n",
        "#         done = False\n",
        "\n",
        "#         while not done:\n",
        "#             # Get a random action from the environment's action space\n",
        "#             action = env.action_space.sample()\n",
        "\n",
        "#             observation, reward, terminated, truncated, info = env.step(action)\n",
        "#             done = terminated or truncated\n",
        "#             total_reward += reward\n",
        "\n",
        "#         episode_rewards.append(total_reward)\n",
        "#         if (episode + 1) % 20 == 0: # Print progress every 20 episodes\n",
        "#             print(f\"  ... Episode {episode + 1} finished. Total Reward: {total_reward}\")\n",
        "\n",
        "#     env.close()\n",
        "#     print(f\"Finished {episodes} episodes.\")\n",
        "\n",
        "#     # --- Generate Plots ---\n",
        "#     plot_episode_rewards(episode_rewards, env_name)\n",
        "\n",
        "#     # Only generate the policy plot for MountainCar\n",
        "#     if env_name == 'MountainCar-v0':\n",
        "#         plot_random_policy_mountaincar()\n",
        "\n",
        "#     print(\"-\" * 40 + \"\\n\")\n",
        "\n",
        "# # --- Run the Analysis ---\n",
        "# run_random_agent_with_plotting('MountainCar-v0', episodes=400) # 200 episodes for a smooth average\n",
        "# run_random_agent_with_plotting('Pong-v0', episodes=50)       # 50 episodes is enough for Pong"
      ],
      "metadata": {
        "id": "SNHQIfgbkFiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ujMtSsK1lT3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import gymnasium as gym\n",
        "# import math\n",
        "# import random\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import matplotlib.patches as mpatches\n",
        "# from collections import namedtuple, deque\n",
        "# from itertools import count\n",
        "# import pandas as pd\n",
        "# import warnings\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# # Suppress warnings\n",
        "# warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# # --- Setup Device ---\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(f\"Using device: {device}\")\n",
        "\n",
        "# # --- Replay Memory ---\n",
        "# # (Using your sample's structure, it's good)\n",
        "# Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "# class ReplayMemory(object):\n",
        "#     def __init__(self, capacity):\n",
        "#         # Use a deque for efficient memory management\n",
        "#         self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "#     def push(self, *args):\n",
        "#         \"\"\"Save a transition\"\"\"\n",
        "#         self.memory.append(Transition(*args))\n",
        "\n",
        "#     def sample(self, batch_size):\n",
        "#         return random.sample(self.memory, batch_size)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.memory)\n",
        "\n",
        "# # --- DQN Model ---\n",
        "# # (Using your sample's model, it's perfect for MountainCar)\n",
        "# class DQN(nn.Module):\n",
        "#     def __init__(self, n_observations, n_actions):\n",
        "#         super(DQN, self).__init__()\n",
        "#         self.layer1 = nn.Linear(n_observations, 128)\n",
        "#         self.layer2 = nn.Linear(128, 128)\n",
        "#         self.layer3 = nn.Linear(128, n_actions)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = F.relu(self.layer1(x))\n",
        "#         x = F.relu(self.layer2(x))\n",
        "#         return self.layer3(x)\n",
        "\n",
        "# # --- Plotting Function ---\n",
        "# # (Using your sample's plotting logic, it's great)\n",
        "# def plot_results(all_episode_rewards, all_steps, mean_n, policy_net, env_name, n_actions):\n",
        "#     \"\"\"Plots the reward learning curve and the learned policy.\"\"\"\n",
        "\n",
        "#     # Calculate n-episode moving average\n",
        "#     rewards_t = torch.tensor(all_episode_rewards, dtype=torch.float)\n",
        "#     steps_t = torch.tensor(all_steps, dtype=torch.float)\n",
        "\n",
        "#     means = rewards_t.unfold(0, mean_n, 1).mean(1).view(-1)\n",
        "#     means = torch.cat((torch.zeros(mean_n - 1), means))\n",
        "\n",
        "#     # Get steps corresponding to the means\n",
        "#     mean_steps = steps_t.unfold(0, mean_n, 1).mean(1).view(-1)\n",
        "#     mean_steps = torch.cat((torch.zeros(mean_n - 1), mean_steps)) # Pad for alignment\n",
        "\n",
        "#     best_mean_reward = -float('inf')\n",
        "#     best_means = []\n",
        "#     for m in means:\n",
        "#         best_mean_reward = max(best_mean_reward, m)\n",
        "#         best_means.append(best_mean_reward)\n",
        "\n",
        "#     # Create figure\n",
        "#     fig = plt.figure(figsize=(15, 6))\n",
        "\n",
        "#     # 1. Performance Plot\n",
        "#     ax1 = fig.add_subplot(121)\n",
        "#     ax1.plot(all_steps, all_episode_rewards, label='Episode Reward', alpha=0.3)\n",
        "#     ax1.plot(mean_steps.numpy(), means.numpy(), label=f'{mean_n}-Episode Mean Reward', color='orange')\n",
        "#     ax1.plot(mean_steps.numpy(), best_means, label='Best Mean Reward', color='red', linestyle='--')\n",
        "#     ax1.grid()\n",
        "#     ax1.set_xlabel(\"Total Steps\")\n",
        "#     ax1.set_ylabel(\"Reward\")\n",
        "#     ax1.legend()\n",
        "#     ax1.set_title(f\"DQN Performance on {env_name}\")\n",
        "\n",
        "#     # 2. Policy Plot\n",
        "#     ax2 = fig.add_subplot(122)\n",
        "#     colour_map = {0: 'lime', 1: 'red', 2: 'blue'}\n",
        "#     action_choices = ['Push Left', 'No Push', 'Push Right']\n",
        "\n",
        "#     # Sample 1000 random states\n",
        "#     X_pos = np.random.uniform(-1.2, 0.6, 1000)\n",
        "#     Y_vel = np.random.uniform(-0.07, 0.07, 1000)\n",
        "\n",
        "#     # Get the greedy action for each state\n",
        "#     states = torch.tensor(np.array(list(zip(X_pos, Y_vel))), dtype=torch.float).to(device)\n",
        "#     with torch.no_grad():\n",
        "#         actions_tensor = policy_net(states).max(1)[1] # Get index of max Q-value\n",
        "#     Z_action = actions_tensor.cpu().numpy()\n",
        "\n",
        "#     # Map actions to colors\n",
        "#     colors = [colour_map[a] for a in Z_action]\n",
        "\n",
        "#     ax2.scatter(X_pos, Y_vel, c=colors, alpha=0.6, s=10)\n",
        "#     ax2.set_title(f\"Learned Policy (Action vs. State) - {env_name}\")\n",
        "#     ax2.set_xlabel(\"Position\")\n",
        "#     ax2.set_ylabel(\"Velocity\")\n",
        "\n",
        "#     legend_patches = [mpatches.Patch(color=colour_map[i], label=action_choices[i]) for i in range(n_actions)]\n",
        "#     ax2.legend(handles=legend_patches, loc='best')\n",
        "\n",
        "#     plt.tight_layout()\n",
        "#     filename = f\"{env_name}_dqn_results.png\"\n",
        "#     plt.savefig(filename)\n",
        "#     print(f\"Saved final plot to {filename}\")\n",
        "#     plt.show()\n",
        "\n",
        "\n",
        "# # --- Action Selection ---\n",
        "# def select_action(state, steps_done):\n",
        "#     \"\"\"Selects an action using an epsilon-greedy policy.\"\"\"\n",
        "#     sample = random.random()\n",
        "#     eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "#                     math.exp(-1. * steps_done / EPS_DECAY)\n",
        "\n",
        "#     if sample > eps_threshold:\n",
        "#         with torch.no_grad():\n",
        "#             # t.max(1) returns (value, index) for each row\n",
        "#             # We want the index (the action)\n",
        "#             return policy_net(state).max(1)[1].view(1, 1)\n",
        "#     else:\n",
        "#         # Select a random action\n",
        "#         return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
        "\n",
        "# # --- Model Optimization ---\n",
        "# def optimize_model():\n",
        "#     \"\"\"Performs one step of optimization on the policy network.\"\"\"\n",
        "#     if len(memory) < BATCH_SIZE:\n",
        "#         return  # Not enough samples in memory yet\n",
        "\n",
        "#     transitions = memory.sample(BATCH_SIZE)\n",
        "#     # Transpose the batch (see https://stackoverflow.com/a/19343/3343043)\n",
        "#     # Converts batch-array of Transitions to Transition of batch-arrays.\n",
        "#     batch = Transition(*zip(*transitions))\n",
        "\n",
        "#     # Compute a mask of non-final states and concatenate the batch elements\n",
        "#     non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "#                                           batch.next_state)), device=device, dtype=torch.bool)\n",
        "#     non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "#                                        if s is not None])\n",
        "\n",
        "#     state_batch = torch.cat(batch.state)\n",
        "#     action_batch = torch.cat(batch.action)\n",
        "#     reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "#     # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "#     # columns of actions taken. These are the actions which would've been taken\n",
        "#     # for each batch state according to policy_net\n",
        "#     state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "#     # Compute V(s_{t+1}) for all next states.\n",
        "#     # Expected values of actions for non_final_next_states are computed based\n",
        "#     # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
        "#     # This is merged based on the mask, such that we'll have either the expected\n",
        "#     # state value or 0 in case the state was final.\n",
        "#     next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "#     with torch.no_grad():\n",
        "#         next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
        "\n",
        "#     # Compute the expected Q values\n",
        "#     # Q_target = r + gamma * max_a Q(s', a)\n",
        "#     expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "#     # Compute Huber loss\n",
        "#     criterion = nn.SmoothL1Loss()\n",
        "#     loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "#     # Optimize the model\n",
        "#     optimizer.zero_grad()\n",
        "#     loss.backward()\n",
        "#     # In-place gradient clipping\n",
        "#     torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
        "#     optimizer.step()\n",
        "\n",
        "\n",
        "# # --- Main Training Loop ---\n",
        "\n",
        "# # Hyperparameters\n",
        "# ENV_NAME = 'MountainCar-v0'\n",
        "# NUM_EPISODES = 600\n",
        "# BATCH_SIZE = 128\n",
        "# GAMMA = 0.99\n",
        "# EPS_START = 0.9\n",
        "# EPS_END = 0.05\n",
        "# EPS_DECAY = 1000\n",
        "# TARGET_UPDATE = 5  # Update target network every 10 episodes\n",
        "# LR = 1e-2             # Learning rate\n",
        "# MEMORY_SIZE = 10000   # Replay buffer size\n",
        "# MEAN_N = 25           # For plotting mean reward\n",
        "\n",
        "# # Setup environment\n",
        "# env = gym.make(ENV_NAME)\n",
        "# n_actions = env.action_space.n\n",
        "# state, info = env.reset()\n",
        "# n_observations = len(state)\n",
        "\n",
        "# # Initialize networks\n",
        "# policy_net = DQN(n_observations, n_actions).to(device)\n",
        "# target_net = DQN(n_observations, n_actions).to(device)\n",
        "# target_net.load_state_dict(policy_net.state_dict())\n",
        "# target_net.eval() # Target network is only for evaluation\n",
        "\n",
        "# optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
        "# memory = ReplayMemory(MEMORY_SIZE)\n",
        "\n",
        "# steps_done = 0\n",
        "# all_episode_rewards = []\n",
        "# all_steps = []\n",
        "# total_steps = 0\n",
        "\n",
        "# print(f\"Starting training on {ENV_NAME}...\")\n",
        "\n",
        "# for i_episode in range(NUM_EPISODES):\n",
        "#     # Initialize the environment and state\n",
        "#     state, info = env.reset()\n",
        "#     state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "#     episode_reward = 0\n",
        "\n",
        "#     for t in count():  # count() is an infinite loop\n",
        "#         # Select and perform an action\n",
        "#         action = select_action(state, steps_done)\n",
        "#         steps_done += 1\n",
        "#         total_steps += 1\n",
        "\n",
        "#         \"\"\"observation, reward, terminated, truncated, _ = env.step(action.item())\n",
        "#         episode_reward += reward\n",
        "#         done = terminated or truncated\n",
        "\n",
        "#         reward = torch.tensor([reward], device=device)\n",
        "\n",
        "#         if terminated:\n",
        "#             next_state = None\n",
        "#         else:\n",
        "#             next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "#         # Store the transition in memory\n",
        "#         memory.push(state, action, next_state, reward)\"\"\"\n",
        "#         observation, reward, terminated, truncated, _ = env.step(action.item())\n",
        "#         episode_reward += reward # Track the TRUE score\n",
        "#         done = terminated or truncated\n",
        "\n",
        "#         # --- REWARD SHAPING (from your sample) ---\n",
        "#         # This is the \"tweak\"\n",
        "#         position = observation[0]\n",
        "#         if position >= 0.5: # Goal reached (same as terminated)\n",
        "#             shaped_reward = position + 1.5\n",
        "#         else:\n",
        "#             shaped_reward = position + 0.5\n",
        "\n",
        "#         reward_tensor = torch.tensor([shaped_reward], device=device)\n",
        "#         # --- END OF TWEAK ---\n",
        "\n",
        "#         if terminated:\n",
        "#             next_state = None\n",
        "#         else:\n",
        "#             next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "#         # Store the SHAPED reward in memory for learning\n",
        "#         memory.push(state, action, next_state, reward_tensor)\n",
        "\n",
        "\n",
        "#         # Move to the next state\n",
        "#         state = next_state\n",
        "\n",
        "#         # Perform one step of the optimization (on the policy network)\n",
        "#         optimize_model()\n",
        "\n",
        "#         if done:\n",
        "#             break\n",
        "\n",
        "#     all_episode_rewards.append(episode_reward)\n",
        "#     all_steps.append(total_steps)\n",
        "\n",
        "#     # Update the target network, copying all weights and biases in DQN\n",
        "#     if i_episode % TARGET_UPDATE == 0:\n",
        "#         target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "#     if i_episode % MEAN_N == 0 and i_episode > 0:\n",
        "#         mean_reward = np.mean(all_episode_rewards[-MEAN_N:])\n",
        "#         print(f'Episode {i_episode} | Total Steps: {total_steps} | {MEAN_N}-Ep Mean Reward: {mean_reward:.2f}')\n",
        "\n",
        "# print('Training complete')\n",
        "# env.close()\n",
        "\n",
        "# # --- Plot Final Results ---\n",
        "# plot_results(all_episode_rewards, all_steps, MEAN_N, policy_net, ENV_NAME, n_actions)"
      ],
      "metadata": {
        "id": "0q05osdAn4A0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gz12J9GdoBjA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}