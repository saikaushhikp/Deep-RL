{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!python3 --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDMF331_SW3V",
        "outputId": "cb13b5ec-5c12-493a-8900-09b5984daa37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.12.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install box2d-py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-cNDDACSQo-",
        "outputId": "a027970f-26f1-45e7-c2c5-eb106257bff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting box2d-py\n",
            "  Using cached box2d-py-2.3.8.tar.gz (374 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: box2d-py\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for box2d-py\n",
            "Failed to build box2d-py\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (box2d-py)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBe2VQIMypW7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5874d80e-06be-4a9d-9f98-573902097059"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting box2d-py\n",
            "  Using cached box2d-py-2.3.8.tar.gz (374 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: box2d-py\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for box2d-py\n",
            "Failed to build box2d-py\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (box2d-py)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# !pip install box2d-py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69UJStpRytlg",
        "outputId": "90b92875-cfff-4816-b40c-a42c038fb079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swig\n",
        "!pip install \"gymnasium[box2d]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuCTwzCnTLjI",
        "outputId": "2c6a013b-8b2e-4320-cfe5-2a627f0ddeb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Downloading swig-4.4.0-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.4.0-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.4.0\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (4.4.0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp312-cp312-linux_x86_64.whl size=2399002 sha256=910646b4abdfaa7fa05014d7eebee33749b2a510e429839662c8b10d9aa4c232\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/e9/60/774da0bcd07f7dc7761a8590fa2d065e4069568e78dcdc3318\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRNQhyWeyxHb",
        "outputId": "af64b4f6-f3cb-4dbb-a945-be76983a5125"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting LunarLander-PG-a.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile LunarLander-PG-a.py\n",
        "# ==============================================================\n",
        "# Problem 3(a): Policy Gradient – Environment Exploration\n",
        "# ==============================================================\n",
        "# Author: <Your Name>\n",
        "# Description:\n",
        "#   Loads the given Gym environment, prints state/action space,\n",
        "#   runs a random agent for a few episodes, and plots average rewards.\n",
        "# ==============================================================\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import random\n",
        "import argparse\n",
        "from itertools import count\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Parse command-line arguments\n",
        "# -----------------------------\n",
        "parser = argparse.ArgumentParser(description=\"Policy Gradient for LunarLander-v3\")\n",
        "parser.add_argument(\"--environment\", type=str, required=True,\n",
        "                    help=\"Gym environment name (e.g., LunarLander-v3\")\n",
        "parser.add_argument(\"--iterations\", type=int, default=50,\n",
        "                    help=\"Number of iterations (each iteration = batch of episodes)\")\n",
        "parser.add_argument(\"--batch\", type=int, default=32,\n",
        "                    help=\"Batch size (number of episodes per iteration)\")\n",
        "args = parser.parse_args()\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Utility Functions\n",
        "# -----------------------------\n",
        "def plot_rewards(rewards, filename):\n",
        "    \"\"\"Plot the average total rewards per iteration.\"\"\"\n",
        "    plt.figure(figsize=(10, 5), dpi=100)\n",
        "    plt.plot(rewards, label='Average Reward per Iteration')\n",
        "    plt.title(f\"Random Agent Performance on {args.environment}\")\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Average Total Reward\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def get_state(obs):\n",
        "    \"\"\"Convert observation (state) to PyTorch tensor.\"\"\"\n",
        "    state = torch.from_numpy(np.array(obs)).float().unsqueeze(0)\n",
        "    return state\n",
        "\n",
        "\n",
        "def select_random_action(env):\n",
        "    \"\"\"Select a random valid action from the environment.\"\"\"\n",
        "    return env.action_space.sample()\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Training Loop\n",
        "# -----------------------------\n",
        "def run_random_agent(env, iterations, batch_size):\n",
        "    \"\"\"Run random agent for given environment and collect average rewards.\"\"\"\n",
        "    all_rewards = []\n",
        "\n",
        "    for it in range(iterations):\n",
        "        batch_rewards = []\n",
        "\n",
        "        for ep in range(batch_size):\n",
        "            obs = env.reset()\n",
        "            # Compatible with Gym v0.26+\n",
        "            if isinstance(obs, tuple):\n",
        "                obs = obs[0]\n",
        "\n",
        "            total_reward = 0\n",
        "            for t in count():\n",
        "                action = select_random_action(env)\n",
        "                step_output = env.step(action)\n",
        "\n",
        "                if len(step_output) == 5:\n",
        "                    obs, reward, terminated, truncated, _ = step_output\n",
        "                    done = terminated or truncated\n",
        "                else:\n",
        "                    obs, reward, done, _ = step_output\n",
        "\n",
        "                total_reward += reward\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            batch_rewards.append(total_reward)\n",
        "\n",
        "        avg_reward = np.mean(batch_rewards)\n",
        "        print(f\"Iteration {it+1}/{iterations}: Average Reward = {avg_reward:.2f}\")\n",
        "        all_rewards.append(avg_reward)\n",
        "\n",
        "    env.close()\n",
        "    return all_rewards\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Main Execution\n",
        "# -----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Load Gym environment\n",
        "    env = gym.make(args.environment)\n",
        "    print(f\"\\nLoaded Environment: {args.environment}\")\n",
        "    print(f\"Observation space: {env.observation_space}\")\n",
        "    print(f\"Action space: {env.action_space}\\n\")\n",
        "\n",
        "    # Run random agent\n",
        "    rewards = run_random_agent(env, args.iterations, args.batch)\n",
        "\n",
        "    # Save plot\n",
        "    file_name = f\"{args.environment}_{args.iterations}_{args.batch}_random_agent.png\"\n",
        "    plot_rewards(rewards, file_name)\n",
        "    print(f\"Plot saved as {file_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SPe-ToezaNN",
        "outputId": "f851d5d3-cf53-48a0-e1aa-d2d2d5cd5386"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loaded Environment: LunarLander-v3\n",
            "Observation space: Box([ -2.5        -2.5       -10.        -10.         -6.2831855 -10.\n",
            "  -0.         -0.       ], [ 2.5        2.5       10.        10.         6.2831855 10.\n",
            "  1.         1.       ], (8,), float32)\n",
            "Action space: Discrete(4)\n",
            "\n",
            "Iteration 1/50: Average Reward = -171.23\n",
            "Iteration 2/50: Average Reward = -182.92\n",
            "Iteration 3/50: Average Reward = -213.44\n",
            "Iteration 4/50: Average Reward = -170.92\n",
            "Iteration 5/50: Average Reward = -174.31\n",
            "Iteration 6/50: Average Reward = -151.22\n",
            "Iteration 7/50: Average Reward = -206.65\n",
            "Iteration 8/50: Average Reward = -160.01\n",
            "Iteration 9/50: Average Reward = -182.23\n",
            "Iteration 10/50: Average Reward = -174.77\n",
            "Iteration 11/50: Average Reward = -218.77\n",
            "Iteration 12/50: Average Reward = -154.10\n",
            "Iteration 13/50: Average Reward = -181.44\n",
            "Iteration 14/50: Average Reward = -141.88\n",
            "Iteration 15/50: Average Reward = -176.98\n",
            "Iteration 16/50: Average Reward = -152.25\n",
            "Iteration 17/50: Average Reward = -204.13\n",
            "Iteration 18/50: Average Reward = -188.76\n",
            "Iteration 19/50: Average Reward = -152.85\n",
            "Iteration 20/50: Average Reward = -190.59\n",
            "Iteration 21/50: Average Reward = -198.43\n",
            "Iteration 22/50: Average Reward = -196.68\n",
            "Iteration 23/50: Average Reward = -201.49\n",
            "Iteration 24/50: Average Reward = -217.16\n",
            "Iteration 25/50: Average Reward = -199.03\n",
            "Iteration 26/50: Average Reward = -183.29\n",
            "Iteration 27/50: Average Reward = -157.04\n",
            "Iteration 28/50: Average Reward = -201.39\n",
            "Iteration 29/50: Average Reward = -174.38\n",
            "Iteration 30/50: Average Reward = -168.22\n",
            "Iteration 31/50: Average Reward = -194.17\n",
            "Iteration 32/50: Average Reward = -157.45\n",
            "Iteration 33/50: Average Reward = -192.06\n",
            "Iteration 34/50: Average Reward = -193.67\n",
            "Iteration 35/50: Average Reward = -184.22\n",
            "Iteration 36/50: Average Reward = -182.56\n",
            "Iteration 37/50: Average Reward = -239.88\n",
            "Iteration 38/50: Average Reward = -160.99\n",
            "Iteration 39/50: Average Reward = -225.96\n",
            "Iteration 40/50: Average Reward = -183.31\n",
            "Iteration 41/50: Average Reward = -237.68\n",
            "Iteration 42/50: Average Reward = -145.81\n",
            "Iteration 43/50: Average Reward = -198.92\n",
            "Iteration 44/50: Average Reward = -199.42\n",
            "Iteration 45/50: Average Reward = -183.99\n",
            "Iteration 46/50: Average Reward = -188.13\n",
            "Iteration 47/50: Average Reward = -203.76\n",
            "Iteration 48/50: Average Reward = -207.33\n",
            "Iteration 49/50: Average Reward = -189.99\n",
            "Iteration 50/50: Average Reward = -151.26\n",
            "Plot saved as LunarLander-v3_50_16_random_agent.png\n"
          ]
        }
      ],
      "source": [
        "!python3 LunarLander-PG-a.py --environment LunarLander-v3 --iterations 50 --batch 16\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rULHcnJX1_kE"
      },
      "source": [
        "# part (b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5k2TYtCzoN4",
        "outputId": "0f663b4b-3232-44ac-973b-87203f4ab05d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing LunarLander-PG-b.py\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "%%writefile LunarLander-PG-b.py\n",
        "\"\"\"\n",
        "question3b.py\n",
        "\n",
        "Policy Gradient (REINFORCE) with options:\n",
        " - reward-to-go vs total-return\n",
        " - advantage normalization (baseline + normalization)\n",
        " - batch updates (batch = number of episodes per policy update)\n",
        " - simple reward-scaling and clipping tweaks\n",
        "\n",
        "Usage example:\n",
        "python3 question3b.py --environment CartPole-v0 --iterations 500 --batch 32 \\\n",
        "    --gamma 0.99 --learning_rate 1e-3 --reward_to_go --advantage_norm\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import count\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "\n",
        "# ----------------------------\n",
        "# Policy network\n",
        "# ----------------------------\n",
        "class Policy(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, act_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Helpers\n",
        "# ----------------------------\n",
        "def discount_cumsum(rewards, gamma):\n",
        "    \"\"\"Compute discounted cumulative sums of rewards.\n",
        "    returns[t] = sum_{k=t}^{T-1} gamma^{k-t} * rewards[k]\n",
        "    \"\"\"\n",
        "    discounted = []\n",
        "    R = 0.0\n",
        "    for r in reversed(rewards):\n",
        "        R = r + gamma * R\n",
        "        discounted.append(R)\n",
        "    return list(reversed(discounted))\n",
        "\n",
        "\n",
        "def get_obs(obs):\n",
        "    \"\"\"Handle different gym versions where reset() may return (obs, info)\"\"\"\n",
        "    if isinstance(obs, tuple) or isinstance(obs, list):\n",
        "        return np.array(obs[0])\n",
        "    return np.array(obs)\n",
        "\n",
        "\n",
        "def step_env(env, action):\n",
        "    \"\"\"Handle different gym versions: step() may return 4 or 5 elements.\"\"\"\n",
        "    out = env.step(action)\n",
        "    if len(out) == 5:\n",
        "        obs, reward, terminated, truncated, info = out\n",
        "        done = terminated or truncated\n",
        "    else:\n",
        "        obs, reward, done, info = out\n",
        "    return obs, reward, done, info\n",
        "\n",
        "\n",
        "def plot_rewards(rewards, save_path):\n",
        "    plt.figure(figsize=(10, 5), dpi=120)\n",
        "    plt.plot(rewards, label='Average Return per Iteration')\n",
        "    plt.title(os.path.basename(save_path).replace('.png', ''))\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Average Return')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Training / Optimization\n",
        "# ----------------------------\n",
        "def optimize(policy, optimizer, batch_log_probs, batch_returns, args, device):\n",
        "    \"\"\"\n",
        "    - batch_log_probs: list of lists of log_prob tensors (per episode per timestep)\n",
        "    - batch_returns: list of lists of returns/advantages (per episode per timestep) (numpy floats)\n",
        "    \"\"\"\n",
        "    # Flatten lists to compute baseline/normalization across entire batch (all timesteps)\n",
        "    flat_log_probs = []\n",
        "    flat_advantages = []\n",
        "\n",
        "    for ep_lp, ep_ret in zip(batch_log_probs, batch_returns):\n",
        "        # ep_lp is list of log_prob tensors\n",
        "        # ep_ret is list/np of returns (floats) corresponding to each step in the episode\n",
        "        for lp, r in zip(ep_lp, ep_ret):\n",
        "            flat_log_probs.append(lp)\n",
        "            flat_advantages.append(r)\n",
        "\n",
        "    if len(flat_log_probs) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Convert to tensors\n",
        "    logp_tensor = torch.stack(flat_log_probs).to(device)  # shape (sum_T,)\n",
        "    advantages = torch.tensor(flat_advantages, dtype=torch.float32, device=device)  # shape (sum_T,)\n",
        "\n",
        "    # If advantage normalization option: subtract baseline and standardize\n",
        "    if args.advantage_norm:\n",
        "        # baseline: mean of returns across batch (simple constant baseline)\n",
        "        baseline = advantages.mean()\n",
        "        advantages = advantages - baseline\n",
        "        adv_mean = advantages.mean()\n",
        "        adv_std = advantages.std(unbiased=False)\n",
        "        advantages = (advantages - adv_mean) / (adv_std + 1e-8)\n",
        "\n",
        "    # Loss = - sum(log_pi * advantage)\n",
        "    loss = - (logp_tensor * advantages).sum()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Return scalar loss for logging\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def run_training(env_name, iterations, batch_size, lr, gamma,\n",
        "                 reward_to_go, advantage_norm, render, reward_scale,\n",
        "                 reward_clip, hidden_dim, seed, device):\n",
        "    # Seed everything\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "    env = gym.make(env_name)\n",
        "    obs_space = env.observation_space\n",
        "    act_space = env.action_space\n",
        "\n",
        "    # Simple checks\n",
        "    if not isinstance(act_space, gym.spaces.Discrete):\n",
        "        raise NotImplementedError(\"This script currently supports only discrete action spaces (e.g., CartPole/LunarLander).\")\n",
        "\n",
        "    obs_dim = obs_space.shape[0]\n",
        "    act_dim = act_space.n\n",
        "\n",
        "    policy = Policy(obs_dim, act_dim, hidden_dim=hidden_dim).to(device)\n",
        "    optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
        "\n",
        "    avg_returns_history = []\n",
        "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "    fname = f\"{env_name}_PG_iters{iterations}_batch{batch_size}_g_lr_{'rtg' if reward_to_go else 'tot'}_{'advnorm' if advantage_norm else 'noadv'}_.png\"\n",
        "\n",
        "    print(f\"Environment: {env_name}\")\n",
        "    print(obs_space)\n",
        "    print(act_space)\n",
        "    print(f\"Device: {device}\")\n",
        "    print(\"Policy Params:\", sum(p.numel() for p in policy.parameters()))\n",
        "\n",
        "    for it in range(iterations):\n",
        "        batch_episode_returns = []\n",
        "        batch_log_probs = []  # list per episode of list of log_probs\n",
        "        batch_returns = []    # list per episode of list of returns (per timestep)\n",
        "\n",
        "        for ep in range(batch_size):\n",
        "            obs = env.reset()\n",
        "            obs = get_obs(obs)\n",
        "            ep_rewards = []\n",
        "            ep_log_probs = []\n",
        "\n",
        "            for t in count():\n",
        "                if render:\n",
        "                    env.render()\n",
        "\n",
        "                obs_tensor = torch.from_numpy(obs).float().unsqueeze(0).to(device)  # (1, obs_dim)\n",
        "                probs = policy(obs_tensor).squeeze(0)  # (act_dim,)\n",
        "                dist = Categorical(probs)\n",
        "                action = dist.sample()\n",
        "                logp = dist.log_prob(action)  # scalar tensor\n",
        "\n",
        "                # step\n",
        "                action_item = action.item()\n",
        "                next_obs, reward, done, info = step_env(env, action_item)\n",
        "                # reward tweaks:\n",
        "                if reward_scale != 1.0:\n",
        "                    reward = reward * reward_scale\n",
        "                if reward_clip is not None:\n",
        "                    reward = float(np.clip(reward, reward_clip[0], reward_clip[1]))\n",
        "\n",
        "                ep_rewards.append(float(reward))\n",
        "                ep_log_probs.append(logp)\n",
        "\n",
        "                obs = get_obs(next_obs)\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            # compute returns\n",
        "            if reward_to_go:\n",
        "                returns = discount_cumsum(ep_rewards, gamma)  # per-timestep return-to-go\n",
        "            else:\n",
        "                total_R = sum([ (gamma**k) * r for k, r in enumerate(ep_rewards)])  # discounted total return\n",
        "                returns = [total_R for _ in ep_rewards]\n",
        "\n",
        "            batch_episode_returns.append(sum(ep_rewards))  # undiscounted episode return for logging\n",
        "            batch_log_probs.append(ep_log_probs)\n",
        "            batch_returns.append(returns)\n",
        "\n",
        "        # Optimize using batch collected\n",
        "        loss_val = optimize(policy, optimizer, batch_log_probs, batch_returns,\n",
        "                            args=argparse.Namespace(advantage_norm=advantage_norm), device=device)\n",
        "\n",
        "        avg_return = float(np.mean(batch_episode_returns))\n",
        "        avg_returns_history.append(avg_return)\n",
        "\n",
        "        if (it + 1) % max(1, int(iterations/10)) == 0 or it < 5:\n",
        "            print(f\"Iter {it+1}/{iterations} | AvgReturn {avg_return:.3f} | Loss {loss_val:.3f}\")\n",
        "\n",
        "    env.close()\n",
        "    plot_rewards(avg_returns_history, fname)\n",
        "    print(f\"Training finished. Plot saved to {fname}\")\n",
        "    return fname, avg_returns_history\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# CLI\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--environment\", type=str, default=\"LunarLander-v3\", help=\"Gym environment\")\n",
        "    parser.add_argument(\"--iterations\", type=int, default=500, help=\"Number of policy updates (iterations)\")\n",
        "    parser.add_argument(\"--batch\", \"--batch_size\", dest=\"batch\", type=int, default=20, help=\"Number of episodes per update\")\n",
        "    parser.add_argument(\"--learning_rate\", \"--lr\", dest=\"lr\", type=float, default=1e-3)\n",
        "    parser.add_argument(\"--gamma\", type=float, default=0.99)\n",
        "    parser.add_argument(\"--reward_to_go\", action=\"store_true\", help=\"Use reward-to-go for returns\")\n",
        "    parser.add_argument(\"--advantage_norm\", action=\"store_true\", help=\"Use baseline subtraction + normalization of advantages\")\n",
        "    parser.add_argument(\"--render\", action=\"store_true\", help=\"Render environment (slow)\")\n",
        "    parser.add_argument(\"--reward_scale\", type=float, default=1.0, help=\"Scale rewards (useful for LunarLander tuning)\")\n",
        "    parser.add_argument(\"--reward_clip_min\", type=float, default=None)\n",
        "    parser.add_argument(\"--reward_clip_max\", type=float, default=None)\n",
        "    parser.add_argument(\"--hidden_dim\", type=int, default=128)\n",
        "    parser.add_argument(\"--seed\", type=int, default=0)\n",
        "    # parser.add_argument(\"--device\", type=str, default=\"cpu\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # reward clip arg\n",
        "    reward_clip = None\n",
        "    if (args.reward_clip_min is not None) or (args.reward_clip_max is not None):\n",
        "        lo = -np.inf if args.reward_clip_min is None else args.reward_clip_min\n",
        "        hi = np.inf if args.reward_clip_max is None else args.reward_clip_max\n",
        "        reward_clip = (lo, hi)\n",
        "\n",
        "    # run\n",
        "    out_png, history = run_training(\n",
        "        env_name=args.environment,\n",
        "        iterations=args.iterations,\n",
        "        batch_size=args.batch,\n",
        "        lr=args.lr,\n",
        "        gamma=args.gamma,\n",
        "        reward_to_go=args.reward_to_go,\n",
        "        advantage_norm=args.advantage_norm,\n",
        "        render=args.render,\n",
        "        reward_scale=args.reward_scale,\n",
        "        reward_clip=reward_clip,\n",
        "        hidden_dim=args.hidden_dim,\n",
        "        seed=args.seed,\n",
        "        device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    )\n",
        "    print(\"Saved plot:\", out_png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJlEcsvA2TAP",
        "outputId": "067b4229-a5d3-4bdd-d826-d79d2f5f9fb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment: LunarLander-v3\n",
            "Box([ -2.5        -2.5       -10.        -10.         -6.2831855 -10.\n",
            "  -0.         -0.       ], [ 2.5        2.5       10.        10.         6.2831855 10.\n",
            "  1.         1.       ], (8,), float32)\n",
            "Discrete(4)\n",
            "Device: cpu\n",
            "Policy Params: 18180\n",
            "Iter 1/5 | AvgReturn -205.115 | Loss 1.150\n",
            "Iter 2/5 | AvgReturn -174.408 | Loss -5.880\n",
            "Iter 3/5 | AvgReturn -177.800 | Loss -3.110\n",
            "Iter 4/5 | AvgReturn -176.663 | Loss 12.527\n",
            "Iter 5/5 | AvgReturn -189.604 | Loss -10.015\n",
            "Training finished. Plot saved to LunarLander-v3_PG_iters5_batch40_g_lr_rtg_advnorm_.png\n",
            "Saved plot: LunarLander-v3_PG_iters5_batch40_g_lr_rtg_advnorm_.png\n"
          ]
        }
      ],
      "source": [
        "# !python3 CartPole-PG-b.py --environment CartPole-v1 --iterations 50 --batch 32 --gamma 0.99 --learning_rate 1e-3 --reward_to_go --advantage_norm\n",
        "!python3 LunarLander-PG-b.py --environment LunarLander-v3 --iterations 5 --batch 40 --gamma 0.99 --learning_rate 5e-4 --reward_to_go --advantage_norm --reward_scale 1.0\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ml5F9kc9Ubse"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}