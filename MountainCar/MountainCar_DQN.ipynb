{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hL6senMep85",
        "outputId": "a203e486-7831-40bf-d959-50bfb40a1afd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing MountainCar-DQN-a.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile MountainCar-DQN-a.py\n",
        "\n",
        "import argparse\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import random\n",
        "from itertools import count\n",
        "\n",
        "def parse_args():\n",
        "    p = argparse.ArgumentParser(description=\"Running with different episodic counts and mean rewards\")\n",
        "    p.add_argument(\"--environment\", type=str, default=\"MountainCar-v0\", help=\"Gym environment id\")\n",
        "    p.add_argument(\"--episodes\", type=int, default=100, help=\"Number of episodes to run\")\n",
        "    p.add_argument(\"--mean_n\", type=int, default=5, help=\"n for rolling mean plot\")\n",
        "    p.add_argument(\"--seed\", type=int, default=None, help=\"Random seed (optional)\")\n",
        "    p.add_argument(\"--render\", action=\"store_true\", help=\"Render environment (slows down execution)\")\n",
        "    return p.parse_args()\n",
        "\n",
        "\n",
        "def safe_reset(env):\n",
        "    \"\"\"Handle gym vs gymnasium reset return types.\"\"\"\n",
        "    out = env.reset()\n",
        "    if isinstance(out, tuple) and len(out) >= 1:\n",
        "        return out[0]\n",
        "    return out\n",
        "\n",
        "def safe_step(env, action):\n",
        "    \"\"\"Handle gym vs gymnasium step return types.\"\"\"\n",
        "    out = env.step(action)\n",
        "    if len(out) == 4:\n",
        "        obs, reward, done, info = out\n",
        "        return obs, reward, done, info\n",
        "    elif len(out) == 5:\n",
        "        obs, reward, terminated, truncated, info = out\n",
        "        done = terminated or truncated\n",
        "        return obs, reward, done, info\n",
        "    else:\n",
        "        raise RuntimeError(\"Unexpected step output format: len = {}\".format(len(out)))\n",
        "\n",
        "def get_state_tensor(obs):\n",
        "    \"\"\"Return numpy array representation for plotting / visualization. Kept simple.\"\"\"\n",
        "    return np.array(obs, dtype=np.float32)\n",
        "\n",
        "def select_random_action(action_space):\n",
        "    \"\"\"Return a single integer action chosen uniformly at random.\"\"\"\n",
        "    return action_space.sample()\n",
        "\n",
        "def plot_results(rewards_mean, steps, best_rewards_mean, env_name, file_name, action_scatter):\n",
        "    \"\"\"Create a 2-panel plot: performance and action choices scatter.\"\"\"\n",
        "    fig = plt.figure(figsize=(12,5))\n",
        "\n",
        "    ax1 = fig.add_subplot(121)\n",
        "    ax1.plot(steps, rewards_mean, label=f\"{len(steps)}-point rolling mean\")\n",
        "    ax1.plot(steps, best_rewards_mean, label=\"Best mean reward\")\n",
        "    ax1.grid(True)\n",
        "    ax1.set_xlabel(\"Total environment steps\")\n",
        "    ax1.set_ylabel(\"Reward (higher is better)\")\n",
        "    ax1.legend()\n",
        "    ax1.set_title(f\"Performance of random agent on {env_name}\")\n",
        "\n",
        "    ax2 = fig.add_subplot(122)\n",
        "    if len(action_scatter) > 0:\n",
        "        arr = np.array(action_scatter)\n",
        "        X = arr[:,0].astype(float)\n",
        "        Y = arr[:,1].astype(float)\n",
        "        Z = arr[:,2].astype(int)\n",
        "        # color map for 3 discrete actions\n",
        "        cmap = {0: 'lime', 1: 'red', 2: 'blue'}\n",
        "        colors = [cmap[int(a)] for a in Z]\n",
        "        ax2.scatter(X, Y, c=colors, s=12, alpha=0.7)\n",
        "        action_names = ['Left (0)', 'No-Op (1)', 'Right (2)']\n",
        "        # legend patches\n",
        "        legend_recs = [mpatches.Rectangle((0,0),1,1,fc=cmap[i]) for i in range(3)]\n",
        "        ax2.legend(legend_recs, action_names, loc='best')\n",
        "    ax2.set_title(\"Random agent action choices (sampled states)\")\n",
        "    ax2.set_xlabel(\"Position\")\n",
        "    ax2.set_ylabel(\"Velocity\")\n",
        "\n",
        "    plt.suptitle(f\"{env_name} - Random Agent Analysis\")\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.savefig(file_name, dpi=200)\n",
        "    print(f\"Saved plot to {file_name}\")\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "    return\n",
        "\n",
        "def run_random_agent(env_id, episodes, mean_n, seed=None, render=False):\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "        random.seed(seed)\n",
        "\n",
        "    env = gym.make(env_id)\n",
        "    print(\"Environment:\", env_id)\n",
        "    print(\"Observation space:\", env.observation_space)\n",
        "    print(\"Action space:\", env.action_space)\n",
        "\n",
        "    episode_rewards = []\n",
        "    best_reward = -float('inf')\n",
        "    rewards_mean = []\n",
        "    best_rewards_mean = []\n",
        "    steps = []\n",
        "\n",
        "    total_steps = 0\n",
        "    success_count = 0\n",
        "\n",
        "    action_scatter = []  # store (pos, vel, action) samples for plotting\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        obs = safe_reset(env)\n",
        "        state = get_state_tensor(obs)\n",
        "        total_reward = 0.0\n",
        "\n",
        "        for t in count():\n",
        "            if render:\n",
        "                env.render()\n",
        "\n",
        "            action = select_random_action(env.action_space)\n",
        "            if len(action_scatter) < 2000 and (total_steps % max(1, int(100/episodes)) == 0):\n",
        "                action_scatter.append((state[0], state[1], action))\n",
        "\n",
        "            obs, reward, done, info = safe_step(env, action)\n",
        "            state = get_state_tensor(obs)\n",
        "            total_reward += reward\n",
        "            total_steps += 1\n",
        "\n",
        "            if done or t >= 10000:  # safety cap\n",
        "                # success condition: position >= 0.5 at termination\n",
        "                try:\n",
        "                    pos = state[0]\n",
        "                except:\n",
        "                    pos = None\n",
        "                if pos is not None and pos >= 0.5:\n",
        "                    success_count += 1\n",
        "                break\n",
        "\n",
        "        episode_rewards.append(total_reward)\n",
        "\n",
        "        if len(episode_rewards) >= mean_n:\n",
        "            present_mean = float(np.mean(episode_rewards[-mean_n:]))\n",
        "            rewards_mean.append(present_mean)\n",
        "            best_reward = max(present_mean, best_reward)\n",
        "            best_rewards_mean.append(best_reward)\n",
        "            steps.append(total_steps)\n",
        "\n",
        "        print(f\"Episode {ep+1}/{episodes} | Reward = {total_reward:.2f} | Successes so far = {success_count}\")\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    fn = f\"{env_id}_random_{episodes}ep_mean{mean_n}.png\"\n",
        "    plot_results(rewards_mean, steps, best_rewards_mean, env_id, fn, action_scatter)\n",
        "\n",
        "    # Summary\n",
        "    summary = {\n",
        "        \"total_episodes\": episodes,\n",
        "        \"mean_n\": mean_n,\n",
        "        \"final_mean_reward\": rewards_mean[-1] if rewards_mean else None,\n",
        "        \"best_mean_reward\": best_rewards_mean[-1] if best_rewards_mean else None,\n",
        "        \"success_count\": success_count,\n",
        "        \"total_steps\": total_steps\n",
        "    }\n",
        "    return summary\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = parse_args()\n",
        "    summary = run_random_agent(args.environment, args.episodes, args.mean_n, args.seed, args.render)\n",
        "    print()\n",
        "    print(\"=\"*10 +\"Summary\" + \"=\"*10)\n",
        "    for k, v in summary.items():\n",
        "        print(f\"{k}: {v}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4xqnADvfPmZ",
        "outputId": "1b0f2c0e-470c-435d-b0fb-9567ba89c551"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment: MountainCar-v0\n",
            "Observation space: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
            "Action space: Discrete(3)\n",
            "Episode 1/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 2/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 3/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 4/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 5/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 6/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 7/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 8/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 9/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 10/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 11/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 12/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 13/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 14/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 15/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 16/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 17/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 18/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 19/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 20/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 21/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 22/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 23/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 24/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 25/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 26/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 27/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 28/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 29/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 30/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 31/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 32/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 33/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 34/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 35/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 36/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 37/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 38/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 39/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 40/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 41/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 42/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 43/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 44/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 45/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 46/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 47/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 48/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 49/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 50/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 51/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 52/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 53/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 54/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 55/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 56/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 57/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 58/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 59/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 60/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 61/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 62/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 63/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 64/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 65/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 66/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 67/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 68/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 69/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 70/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 71/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 72/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 73/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 74/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 75/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 76/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 77/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 78/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 79/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 80/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 81/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 82/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 83/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 84/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 85/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 86/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 87/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 88/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 89/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 90/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 91/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 92/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 93/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 94/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 95/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 96/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 97/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 98/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 99/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 100/100 | Reward = -200.00 | Successes so far = 0\n",
            "Saved plot to MountainCar-v0_random_100ep_mean5.png\n",
            "Figure(1200x500)\n",
            "\n",
            "=== Summary ===\n",
            "total_episodes: 100\n",
            "mean_n: 5\n",
            "final_mean_reward: -200.0\n",
            "best_mean_reward: -200.0\n",
            "success_count: 0\n",
            "total_steps: 20000\n"
          ]
        }
      ],
      "source": [
        "# !python3 MountainCar-DQN-a.py --environment \"MountainCar-v0\" --episodes 100 --mean_n 5 --seed 303\n",
        "# run the python CMD via CLI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djzEyIi4jE2u",
        "outputId": "c9eb1c80-fa3d-4bbb-88ec-775fa6ce4cab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing MountainCar-DQN-b.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile MountainCar-DQN-b.py\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import warnings\n",
        "import argparse\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    \"\"\"Deep Q-Network: 3-layer MLP for state-action value prediction.\"\"\"\n",
        "    def __init__(self, state_dimension, action_dimension):\n",
        "        super().__init__()\n",
        "        self.network_layers = nn.Sequential(\n",
        "            nn.Linear(state_dimension, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 128), nn.ReLU(),\n",
        "            nn.Linear(128, action_dimension)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        return self.network_layers(state)\n",
        "\n",
        "\n",
        "def calculate_shaped_reward(state, environment_reward):\n",
        "    \"\"\"Enhances the reward signal by adding a velocity-based bonus.\"\"\"\n",
        "    car_velocity = state[1]\n",
        "    velocity_bonus = abs(car_velocity) * 10  # Encourages maintaining speed\n",
        "    return environment_reward + velocity_bonus\n",
        "\n",
        "\n",
        "def train_agent(config):\n",
        "    warnings.filterwarnings(\"ignore\", category=UserWarning, module='torch.nn.functional')\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    # Environment setup\n",
        "    env = gym.make('MountainCar-v0')\n",
        "    state_dimension = env.observation_space.shape[0]\n",
        "    action_dimension = env.action_space.n\n",
        "\n",
        "    # Initialize networks\n",
        "    online_network = QNetwork(state_dimension, action_dimension).to(device)\n",
        "    target_network = QNetwork(state_dimension, action_dimension).to(device)\n",
        "    target_network.load_state_dict(online_network.state_dict())\n",
        "    target_network.eval()\n",
        "\n",
        "    optimizer = optim.Adam(online_network.parameters(), lr=config.learning_rate)\n",
        "    criterion = nn.MSELoss()\n",
        "    experience_buffer = deque(maxlen=config.memory_size)\n",
        "\n",
        "    # Training metrics\n",
        "    episode_rewards = []\n",
        "    exploration_rate = config.epsilon_start\n",
        "    total_timesteps = 0\n",
        "\n",
        "\n",
        "    for episode in range(config.num_episodes):\n",
        "        if episode % 10 == 0:\n",
        "            print(f\"Episode {episode}/{config.num_episodes}\")\n",
        "\n",
        "        state_np, _ = env.reset()\n",
        "        state_tensor = torch.FloatTensor(state_np).unsqueeze(0).to(device)\n",
        "        episode_reward = 0\n",
        "        episode_steps = 0\n",
        "\n",
        "        while True:\n",
        "            if random.random() < exploration_rate:\n",
        "                action = env.action_space.sample()  # Random exploration\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    action = online_network(state_tensor).argmax().item()  # Greedy exploitation\n",
        "\n",
        "            next_state_np, env_reward, terminated, truncated, _ = env.step(action)\n",
        "            shaped_reward = calculate_shaped_reward(next_state_np, env_reward)\n",
        "            next_state_tensor = torch.FloatTensor(next_state_np).unsqueeze(0).to(device)\n",
        "            episode_done = terminated or truncated\n",
        "\n",
        "            experience_buffer.append((state_tensor, action, shaped_reward, next_state_tensor, episode_done))\n",
        "\n",
        "            state_tensor = next_state_tensor\n",
        "            episode_reward += shaped_reward\n",
        "            episode_steps += 1\n",
        "            total_timesteps += 1\n",
        "\n",
        "            if len(experience_buffer) > config.batch_size:\n",
        "                batch = random.sample(experience_buffer, config.batch_size)\n",
        "                batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = zip(*batch)\n",
        "\n",
        "                states_tensor = torch.cat(batch_states).to(device)\n",
        "                next_states_tensor = torch.cat(batch_next_states).to(device)\n",
        "                actions_tensor = torch.tensor(batch_actions).unsqueeze(1).to(device)\n",
        "                rewards_tensor = torch.tensor(batch_rewards, dtype=torch.float32).to(device)\n",
        "                dones_tensor = torch.tensor(batch_dones, dtype=torch.bool).to(device)\n",
        "\n",
        "                # Compute current Q-values\n",
        "                current_q_values = online_network(states_tensor).gather(1, actions_tensor)\n",
        "\n",
        "                # Compute target Q-values using target network\n",
        "                with torch.no_grad():\n",
        "                    next_q_values = target_network(next_states_tensor).max(1)[0]\n",
        "                    target_q_values = rewards_tensor + config.gamma * next_q_values * (~dones_tensor).float()\n",
        "\n",
        "                # Compute loss and update online network\n",
        "                loss = criterion(current_q_values, target_q_values.unsqueeze(1))\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            if episode_done:\n",
        "                break\n",
        "\n",
        "        if exploration_rate > config.epsilon_min:\n",
        "            exploration_rate *= config.epsilon_decay_factor\n",
        "\n",
        "        if (episode + 1) % config.target_update_episodes == 0:\n",
        "            target_network.load_state_dict(online_network.state_dict())\n",
        "\n",
        "        episode_rewards.append(episode_reward)\n",
        "\n",
        "        if len(episode_rewards) >= config.n_mean_episodes and episode % 10 == 0:\n",
        "            recent_mean_reward = np.mean(episode_rewards[-config.n_mean_episodes:])\n",
        "            print(f\"Mean {config.n_mean_episodes}-episode reward: {recent_mean_reward:.2f}\")\n",
        "\n",
        "    env.close()\n",
        "    return episode_rewards, online_network, device\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def visualize_training_results(config, training_rewards, trained_network, device):\n",
        "    # Prepare training metrics\n",
        "    episode_count = config.num_episodes\n",
        "    window_size = config.n_mean_episodes\n",
        "    batch_size = config.batch_size\n",
        "\n",
        "    episodes = list(range(1, len(training_rewards) + 1))\n",
        "    smoothed_rewards = []\n",
        "    peak_rewards = []\n",
        "\n",
        "    highest_mean = -float('inf')\n",
        "    for i in range(len(training_rewards)):\n",
        "        if i >= window_size:\n",
        "            window_mean = np.mean(training_rewards[i - window_size:i])\n",
        "            smoothed_rewards.append(window_mean)\n",
        "            highest_mean = max(highest_mean, window_mean)\n",
        "            peak_rewards.append(highest_mean)\n",
        "        else:\n",
        "            smoothed_rewards.append(None)\n",
        "            peak_rewards.append(None)\n",
        "\n",
        "    output_filename = f\"{config.environment}_DQN_{episode_count}_episodes_{batch_size}batch.png\"\n",
        "\n",
        "    _, (performance_plot, policy_plot) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    performance_plot.plot(episodes, smoothed_rewards, label=f\"{window_size}-episode mean\", color=\"deepskyblue\")\n",
        "    performance_plot.plot(episodes, peak_rewards, label=\"Best mean reward\", color=\"orange\", linestyle=\"--\")\n",
        "    performance_plot.set_xlabel(\"Episode\")\n",
        "    performance_plot.set_ylabel(\"Reward\")\n",
        "    performance_plot.set_title(f\"Learning Progress on {config.environment}\")\n",
        "    performance_plot.grid(True)\n",
        "    performance_plot.legend()\n",
        "\n",
        "    position_range = np.linspace(-1.5, 0.6, 50)  # Car position range\n",
        "    velocity_range = np.linspace(-1, 1, 50)      # Car velocity range\n",
        "    positions, velocities = np.meshgrid(position_range, velocity_range)\n",
        "    states = np.array(list(zip(positions.flatten(), velocities.flatten())))\n",
        "\n",
        "    state_tensor = torch.tensor(states, dtype=torch.float32, device=device)\n",
        "    with torch.no_grad():\n",
        "        action_choices = trained_network(state_tensor).argmax(dim=1).cpu().numpy()\n",
        "\n",
        "    action_colors = ['lime', 'red', 'blue']  # left, no-op, right\n",
        "    policy_plot.scatter(states[:, 0], states[:, 1], \n",
        "                       c=[action_colors[a] for a in action_choices], \n",
        "                       s=1, alpha=0.7)\n",
        "    policy_plot.set_xlabel(\"Car Position\")\n",
        "    policy_plot.set_ylabel(\"Car Velocity\")\n",
        "    policy_plot.set_title(\"Learned Policy Map\")\n",
        "\n",
        "    action_labels = [\"Left\", \"No Action\", \"Right\"]\n",
        "    legend_patches = [mpatches.Patch(color=action_colors[i], \n",
        "                                   label=action_labels[i]) \n",
        "                     for i in range(3)]\n",
        "    policy_plot.legend(handles=legend_patches)\n",
        "    policy_plot.set_xlim([-1.5, 0.6])\n",
        "    policy_plot.set_ylim([-1, 1])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_filename, dpi=200)\n",
        "    plt.close()\n",
        "    print(f\"Visualization saved as {output_filename}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Set up command-line argument parser\n",
        "    parser = argparse.ArgumentParser(description=\"Deep Q-Learning for Mountain Car Environment\")\n",
        "\n",
        "    # Training configuration parameters\n",
        "    parser.add_argument(\"--environment\", type=str, default=\"MountainCar-v0\",help=\"Gymnasium environment to train on\")\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=5e-4,help=\"Learning rate for the neural network\")\n",
        "    parser.add_argument(\"--gamma\", type=float, default=0.99,help=\"Discount factor for future rewards\")\n",
        "    parser.add_argument(\"--epsilon_start\", type=float, default=1.0,help=\"Initial exploration rate\")\n",
        "    parser.add_argument(\"--epsilon_min\", type=float, default=0.01,help=\"Minimum exploration rate\")\n",
        "    parser.add_argument(\"--epsilon_decay_factor\", type=float, default=0.997,help=\"Rate at which exploration decreases\")\n",
        "    parser.add_argument(\"--memory_size\", type=int, default=10000,help=\"Size of experience replay buffer\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=64,help=\"Number of experiences to learn from at once\")\n",
        "    parser.add_argument(\"--target_update_episodes\", type=int, default=20,help=\"Episodes between target network updates\")\n",
        "    parser.add_argument(\"--num_episodes\", type=int, default=1000,help=\"Total number of training episodes\")\n",
        "    parser.add_argument(\"--n_mean_episodes\", type=int, default=50,help=\"Window size for calculating mean reward\")\n",
        "\n",
        "    # Parse arguments and start training\n",
        "    config = parser.parse_args()\n",
        "\n",
        "    # Train the agent\n",
        "    training_history, trained_network, device = train_agent(config)\n",
        "    \n",
        "    # Visualize and save results\n",
        "    visualize_training_results(config, training_history, trained_network, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pl7h-u9DjUwH",
        "outputId": "0d8bbd26-c096-4c34-eaa9-2aa17dc0d6fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1/100 | Reward: -200.00 | Success: 0 | ε=0.784\n",
            "Episode 2/100 | Reward: -200.00 | Success: 0 | ε=0.615\n",
            "Episode 3/100 | Reward: -200.00 | Success: 0 | ε=0.483\n",
            "Episode 4/100 | Reward: -200.00 | Success: 0 | ε=0.381\n",
            "Episode 5/100 | Reward: -200.00 | Success: 0 | ε=0.301\n",
            "Episode 6/100 | Reward: -200.00 | Success: 0 | ε=0.239\n",
            "Episode 7/100 | Reward: -200.00 | Success: 0 | ε=0.191\n",
            "Episode 8/100 | Reward: -200.00 | Success: 0 | ε=0.153\n",
            "Episode 9/100 | Reward: -200.00 | Success: 0 | ε=0.123\n",
            "Episode 10/100 | Reward: -200.00 | Success: 0 | ε=0.101\n",
            "Episode 11/100 | Reward: -200.00 | Success: 0 | ε=0.083\n",
            "Episode 12/100 | Reward: -200.00 | Success: 0 | ε=0.069\n",
            "Episode 13/100 | Reward: -200.00 | Success: 0 | ε=0.058\n",
            "Episode 14/100 | Reward: -200.00 | Success: 0 | ε=0.050\n",
            "Episode 15/100 | Reward: -200.00 | Success: 0 | ε=0.043\n",
            "Episode 16/100 | Reward: -200.00 | Success: 0 | ε=0.038\n",
            "Episode 17/100 | Reward: -200.00 | Success: 0 | ε=0.034\n",
            "Episode 18/100 | Reward: -200.00 | Success: 0 | ε=0.031\n",
            "Episode 19/100 | Reward: -200.00 | Success: 0 | ε=0.028\n",
            "Episode 20/100 | Reward: -200.00 | Success: 0 | ε=0.027\n",
            "Episode 21/100 | Reward: -200.00 | Success: 0 | ε=0.025\n",
            "Episode 22/100 | Reward: -200.00 | Success: 0 | ε=0.024\n",
            "Episode 23/100 | Reward: -200.00 | Success: 0 | ε=0.023\n",
            "Episode 24/100 | Reward: -200.00 | Success: 0 | ε=0.022\n",
            "Episode 25/100 | Reward: -200.00 | Success: 0 | ε=0.022\n",
            "Episode 26/100 | Reward: -200.00 | Success: 0 | ε=0.021\n",
            "Episode 27/100 | Reward: -200.00 | Success: 0 | ε=0.021\n",
            "Episode 28/100 | Reward: -200.00 | Success: 0 | ε=0.021\n",
            "Episode 29/100 | Reward: -200.00 | Success: 0 | ε=0.021\n",
            "Episode 30/100 | Reward: -200.00 | Success: 0 | ε=0.021\n",
            "Episode 31/100 | Reward: -166.00 | Success: 1 | ε=0.020\n",
            "Episode 32/100 | Reward: -200.00 | Success: 1 | ε=0.020\n",
            "Episode 33/100 | Reward: -200.00 | Success: 1 | ε=0.020\n",
            "Episode 34/100 | Reward: -194.00 | Success: 2 | ε=0.020\n",
            "Episode 35/100 | Reward: -200.00 | Success: 2 | ε=0.020\n",
            "Episode 36/100 | Reward: -200.00 | Success: 2 | ε=0.020\n",
            "Episode 37/100 | Reward: -200.00 | Success: 2 | ε=0.020\n",
            "Episode 38/100 | Reward: -200.00 | Success: 2 | ε=0.020\n",
            "Episode 39/100 | Reward: -200.00 | Success: 2 | ε=0.020\n",
            "Episode 40/100 | Reward: -200.00 | Success: 2 | ε=0.020\n",
            "Episode 41/100 | Reward: -200.00 | Success: 2 | ε=0.020\n",
            "Episode 42/100 | Reward: -200.00 | Success: 2 | ε=0.020\n",
            "Episode 43/100 | Reward: -142.00 | Success: 3 | ε=0.020\n",
            "Episode 44/100 | Reward: -200.00 | Success: 3 | ε=0.020\n",
            "Episode 45/100 | Reward: -200.00 | Success: 3 | ε=0.020\n",
            "Episode 46/100 | Reward: -200.00 | Success: 3 | ε=0.020\n",
            "Episode 47/100 | Reward: -170.00 | Success: 4 | ε=0.020\n",
            "Episode 48/100 | Reward: -148.00 | Success: 5 | ε=0.020\n",
            "Episode 49/100 | Reward: -196.00 | Success: 6 | ε=0.020\n",
            "Episode 50/100 | Reward: -154.00 | Success: 7 | ε=0.020\n",
            "Episode 51/100 | Reward: -200.00 | Success: 7 | ε=0.020\n",
            "Episode 52/100 | Reward: -200.00 | Success: 7 | ε=0.020\n",
            "Episode 53/100 | Reward: -200.00 | Success: 7 | ε=0.020\n",
            "Episode 54/100 | Reward: -200.00 | Success: 7 | ε=0.020\n",
            "Episode 55/100 | Reward: -200.00 | Success: 7 | ε=0.020\n",
            "Episode 56/100 | Reward: -200.00 | Success: 7 | ε=0.020\n",
            "Episode 57/100 | Reward: -200.00 | Success: 7 | ε=0.020\n",
            "Episode 58/100 | Reward: -200.00 | Success: 7 | ε=0.020\n",
            "Episode 59/100 | Reward: -122.00 | Success: 8 | ε=0.020\n",
            "Episode 60/100 | Reward: -142.00 | Success: 9 | ε=0.020\n",
            "Episode 61/100 | Reward: -131.00 | Success: 10 | ε=0.020\n",
            "Episode 62/100 | Reward: -132.00 | Success: 11 | ε=0.020\n",
            "Episode 63/100 | Reward: -200.00 | Success: 11 | ε=0.020\n",
            "Episode 64/100 | Reward: -138.00 | Success: 12 | ε=0.020\n",
            "Episode 65/100 | Reward: -200.00 | Success: 12 | ε=0.020\n",
            "Episode 66/100 | Reward: -124.00 | Success: 13 | ε=0.020\n",
            "Episode 67/100 | Reward: -155.00 | Success: 14 | ε=0.020\n",
            "Episode 68/100 | Reward: -141.00 | Success: 15 | ε=0.020\n",
            "Episode 69/100 | Reward: -119.00 | Success: 16 | ε=0.020\n",
            "Episode 70/100 | Reward: -145.00 | Success: 17 | ε=0.020\n",
            "Episode 71/100 | Reward: -173.00 | Success: 18 | ε=0.020\n",
            "Episode 72/100 | Reward: -164.00 | Success: 19 | ε=0.020\n",
            "Episode 73/100 | Reward: -141.00 | Success: 20 | ε=0.020\n",
            "Episode 74/100 | Reward: -148.00 | Success: 21 | ε=0.020\n",
            "Episode 75/100 | Reward: -200.00 | Success: 21 | ε=0.020\n",
            "Episode 76/100 | Reward: -146.00 | Success: 22 | ε=0.020\n",
            "Episode 77/100 | Reward: -160.00 | Success: 23 | ε=0.020\n",
            "Episode 78/100 | Reward: -159.00 | Success: 24 | ε=0.020\n",
            "Episode 79/100 | Reward: -200.00 | Success: 24 | ε=0.020\n",
            "Episode 80/100 | Reward: -162.00 | Success: 25 | ε=0.020\n",
            "Episode 81/100 | Reward: -185.00 | Success: 26 | ε=0.020\n",
            "Episode 82/100 | Reward: -144.00 | Success: 27 | ε=0.020\n",
            "Episode 83/100 | Reward: -109.00 | Success: 28 | ε=0.020\n",
            "Episode 84/100 | Reward: -135.00 | Success: 29 | ε=0.020\n",
            "Episode 85/100 | Reward: -151.00 | Success: 30 | ε=0.020\n",
            "Episode 86/100 | Reward: -148.00 | Success: 31 | ε=0.020\n",
            "Episode 87/100 | Reward: -154.00 | Success: 32 | ε=0.020\n",
            "Episode 88/100 | Reward: -119.00 | Success: 33 | ε=0.020\n",
            "Episode 89/100 | Reward: -151.00 | Success: 34 | ε=0.020\n",
            "Episode 90/100 | Reward: -152.00 | Success: 35 | ε=0.020\n",
            "Episode 91/100 | Reward: -151.00 | Success: 36 | ε=0.020\n",
            "Episode 92/100 | Reward: -128.00 | Success: 37 | ε=0.020\n",
            "Episode 93/100 | Reward: -160.00 | Success: 38 | ε=0.020\n",
            "Episode 94/100 | Reward: -154.00 | Success: 39 | ε=0.020\n",
            "Episode 95/100 | Reward: -159.00 | Success: 40 | ε=0.020\n",
            "Episode 96/100 | Reward: -137.00 | Success: 41 | ε=0.020\n",
            "Episode 97/100 | Reward: -150.00 | Success: 42 | ε=0.020\n",
            "Episode 98/100 | Reward: -150.00 | Success: 43 | ε=0.020\n",
            "Episode 99/100 | Reward: -151.00 | Success: 44 | ε=0.020\n",
            "Episode 100/100 | Reward: -140.00 | Success: 45 | ε=0.020\n",
            "Saved plot to MountainCar-v0_DQN_100_episodes.png\n"
          ]
        }
      ],
      "source": [
        "# !python3 MountainCar-DQN-b.py --environment=\"MountainCar-v0\" --num_episodes=100 --batch=8 --gamma=0.9 --learning_rate=1e-4 --mean_n=5\n",
        "# run the python CMD via CLI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile MountainCar-DQN-c.py\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Fixed settings\n",
        "environment = \"MountainCar-v0\"\n",
        "num_episodes = 1000\n",
        "\n",
        "\n",
        "batch_sizes = [16, 32, 64, 128]\n",
        "\n",
        "batch_rewards = {}\n",
        "\n",
        "for b in batch_sizes:\n",
        "    print(f\"\\nRunning with batch size = {b}\")\n",
        "    result = subprocess.run(\n",
        "        [\n",
        "            sys.executable,\n",
        "            \"MountainCar-DQN-b.py\",\n",
        "            \"--environment\", environment,\n",
        "            \"--num_episodes\", str(num_episodes),\n",
        "            \"--batch_size\", str(b),\n",
        "        ],\n",
        "        capture_output=True, text=True\n",
        "    )\n",
        "\n",
        "    batch_rewards[b] = result.stdout\n",
        "    print(\"-\"*10)\n",
        "    print(result.stdout)\n",
        "    if result.returncode != 0:\n",
        "        print(\"--- STDERR (error) ---\")\n",
        "        print(result.stderr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# run the python CMD via CLI\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part(a)\n",
        "A random agent in the MountainCar-v0 environment performs poorly, consistently receiving rewards around -200 without any improvement over time. Its actions are uniformly random across all states, which is an ineffective strategy for building the momentum required to escape the valley and reach the goal.\n",
        "\n",
        "--- \n",
        "\n",
        "![Random Agent Performance and Policy Map](part(a)/MountainCar-v0_random_100ep_mean5.png)\n",
        "<center>\n",
        "Figure-1: Random Agent Performance (left) and its Policy Map (right) for 100 episodes of random actions.\n",
        "</center>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part(b)\n",
        "The DQN agent successfully learned to solve the MountainCar-v0 environment. This is demonstrated by three key outcomes:\n",
        "\n",
        "- Training Loss: The loss initially rose and then steadily decreased, indicating successful convergence of the Q-network.\n",
        "- Reward Improvement: The agent's average reward consistently increased throughout training, showing it learned to perform better.\n",
        "- Effective Policy: The final policy map reveals the agent learned the essential momentum strategy, choosing actions that build momentum to reach the goal.\n",
        "\n",
        "--- \n",
        "\n",
        "![DQN Agent Performance and Policy Map](part(b)/MountainCar-v0_DQN_1000_episodes_64batch.png)\n",
        "<center>\n",
        "Figure-2: DQN Agent Performance (left) and its Policy Map (right) after training for 1000 episodes with a batch size of 64.\n",
        "</center>\n",
        "\n",
        "---\n",
        "\n",
        "![Agent's Training Loss](part(b)/MountainCar-v0_DQN_1000_episodes_64batch_loss.png)\n",
        "<center>\n",
        "Figure-3: DQN Agent Training Loss over 1000 episodes with a batch size of 64.\n",
        "</center>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part(c)\n",
        "- Small batch sizes yield greater update randomness and learning curve variance, potentially helping escape local minima but also risking instability.\n",
        "- Large batch sizes offer steadier convergence and cleaner policy maps, but is observed that that 'No action' option isn't learned as effectively with larger batches.\n",
        "- For MountainCar-v0, all tested batch sizes enable successful DQN convergence.\n",
        "\n",
        "---\n",
        "\n",
        "![](part(c)/MountainCar-v0_DQN_1000_episodes_16batch.png)\n",
        "![](part(c)/MountainCar-v0_DQN_1000_episodes_32batch.png)\n",
        "![](part(c)/MountainCar-v0_DQN_1000_episodes_64batch.png)\n",
        "![](part(c)/MountainCar-v0_DQN_1000_episodes_128batch.png)\n",
        "<center>\n",
        "Figures 4,5,6,7: DQN Agent Performance for batch sizes of 16, 32, 64, and 128 over 1000 episodes.\n",
        "</center>\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
