{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hL6senMep85",
        "outputId": "a203e486-7831-40bf-d959-50bfb40a1afd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing MountainCar-DQN-a.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile MountainCar-DQN-a.py\n",
        "\n",
        "import argparse\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import random\n",
        "from itertools import count\n",
        "import os\n",
        "\n",
        "def parse_args():\n",
        "    p = argparse.ArgumentParser(description=\"Running with different episodic counts and mean rewards\")\n",
        "    p.add_argument(\"--environment\", type=str, default=\"MountainCar-v0\", help=\"Gym environment id\")\n",
        "    p.add_argument(\"--episodes\", type=int, default=100, help=\"Number of episodes to run\")\n",
        "    p.add_argument(\"--mean_n\", type=int, default=5, help=\"n for rolling mean plot\")\n",
        "    p.add_argument(\"--seed\", type=int, default=None, help=\"Random seed (optional)\")\n",
        "    p.add_argument(\"--render\", action=\"store_true\", help=\"Render environment (slows down execution)\")\n",
        "    return p.parse_args()\n",
        "\n",
        "\n",
        "def safe_reset(env):\n",
        "    \"\"\"Handle gym vs gymnasium reset return types.\"\"\"\n",
        "    out = env.reset()\n",
        "    if isinstance(out, tuple) and len(out) >= 1:\n",
        "        return out[0]\n",
        "    return out\n",
        "\n",
        "def safe_step(env, action):\n",
        "    \"\"\"Handle gym vs gymnasium step return types.\"\"\"\n",
        "    out = env.step(action)\n",
        "    if len(out) == 4:\n",
        "        obs, reward, done, info = out\n",
        "        return obs, reward, done, info\n",
        "    elif len(out) == 5:\n",
        "        obs, reward, terminated, truncated, info = out\n",
        "        done = terminated or truncated\n",
        "        return obs, reward, done, info\n",
        "    else:\n",
        "        raise RuntimeError(\"Unexpected step output format: len = {}\".format(len(out)))\n",
        "\n",
        "def get_state_tensor(obs):\n",
        "    \"\"\"Return numpy array representation for plotting / visualization. Kept simple.\"\"\"\n",
        "    return np.array(obs, dtype=np.float32)\n",
        "\n",
        "def select_random_action(action_space):\n",
        "    \"\"\"Return a single integer action chosen uniformly at random.\"\"\"\n",
        "    return action_space.sample()\n",
        "\n",
        "def plot_results(rewards_mean, steps, best_rewards_mean, env_name, file_name, action_scatter):\n",
        "    \"\"\"Create a 2-panel plot: performance and action choices scatter.\"\"\"\n",
        "    fig = plt.figure(figsize=(12,5))\n",
        "\n",
        "    ax1 = fig.add_subplot(121)\n",
        "    ax1.plot(steps, rewards_mean, label=f\"{len(steps)}-point rolling mean\")\n",
        "    ax1.plot(steps, best_rewards_mean, label=\"Best mean reward\")\n",
        "    ax1.grid(True)\n",
        "    ax1.set_xlabel(\"Total environment steps\")\n",
        "    ax1.set_ylabel(\"Reward (higher is better)\")\n",
        "    ax1.legend()\n",
        "    ax1.set_title(f\"Performance of random agent on {env_name}\")\n",
        "\n",
        "    ax2 = fig.add_subplot(122)\n",
        "    if len(action_scatter) > 0:\n",
        "        arr = np.array(action_scatter)\n",
        "        X = arr[:,0].astype(float)\n",
        "        Y = arr[:,1].astype(float)\n",
        "        Z = arr[:,2].astype(int)\n",
        "        # color map for 3 discrete actions\n",
        "        cmap = {0: 'lime', 1: 'red', 2: 'blue'}\n",
        "        colors = [cmap[int(a)] for a in Z]\n",
        "        ax2.scatter(X, Y, c=colors, s=12, alpha=0.7)\n",
        "        action_names = ['Left (0)', 'No-Op (1)', 'Right (2)']\n",
        "        # legend patches\n",
        "        legend_recs = [mpatches.Rectangle((0,0),1,1,fc=cmap[i]) for i in range(3)]\n",
        "        ax2.legend(legend_recs, action_names, loc='best')\n",
        "    ax2.set_title(\"Random agent action choices (sampled states)\")\n",
        "    ax2.set_xlabel(\"Position\")\n",
        "    ax2.set_ylabel(\"Velocity\")\n",
        "\n",
        "    plt.suptitle(f\"{env_name} - Random Agent Analysis\")\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.savefig(file_name, dpi=200)\n",
        "    print(f\"Saved plot to {file_name}\")\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "    return\n",
        "\n",
        "def run_random_agent(env_id, episodes, mean_n, seed=None, render=False):\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "        random.seed(seed)\n",
        "\n",
        "    env = gym.make(env_id)\n",
        "    print(\"Environment:\", env_id)\n",
        "    print(\"Observation space:\", env.observation_space)\n",
        "    print(\"Action space:\", env.action_space)\n",
        "\n",
        "    episode_rewards = []\n",
        "    best_reward = -float('inf')\n",
        "    rewards_mean = []\n",
        "    best_rewards_mean = []\n",
        "    steps = []\n",
        "\n",
        "    total_steps = 0\n",
        "    success_count = 0\n",
        "\n",
        "    action_scatter = []  # store (pos, vel, action) samples for plotting\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        obs = safe_reset(env)\n",
        "        state = get_state_tensor(obs)\n",
        "        total_reward = 0.0\n",
        "\n",
        "        for t in count():\n",
        "            if render:\n",
        "                env.render()\n",
        "\n",
        "            action = select_random_action(env.action_space)\n",
        "            if len(action_scatter) < 2000 and (total_steps % max(1, int(100/episodes)) == 0):\n",
        "                action_scatter.append((state[0], state[1], action))\n",
        "\n",
        "            obs, reward, done, info = safe_step(env, action)\n",
        "            state = get_state_tensor(obs)\n",
        "            total_reward += reward\n",
        "            total_steps += 1\n",
        "\n",
        "            if done or t >= 10000:  # safety cap\n",
        "                # success condition: position >= 0.5 at termination\n",
        "                try:\n",
        "                    pos = state[0]\n",
        "                except:\n",
        "                    pos = None\n",
        "                if pos is not None and pos >= 0.5:\n",
        "                    success_count += 1\n",
        "                break\n",
        "\n",
        "        episode_rewards.append(total_reward)\n",
        "\n",
        "        if len(episode_rewards) >= mean_n:\n",
        "            present_mean = float(np.mean(episode_rewards[-mean_n:]))\n",
        "            rewards_mean.append(present_mean)\n",
        "            best_reward = max(present_mean, best_reward)\n",
        "            best_rewards_mean.append(best_reward)\n",
        "            steps.append(total_steps)\n",
        "\n",
        "        print(f\"Episode {ep+1}/{episodes} | Reward = {total_reward:.2f} | Successes so far = {success_count}\")\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    fn = f\"{env_id}_random_{episodes}ep_mean{mean_n}.png\"\n",
        "    plot_results(rewards_mean, steps, best_rewards_mean, env_id, fn, action_scatter)\n",
        "\n",
        "    # Summary\n",
        "    summary = {\n",
        "        \"total_episodes\": episodes,\n",
        "        \"mean_n\": mean_n,\n",
        "        \"final_mean_reward\": rewards_mean[-1] if rewards_mean else None,\n",
        "        \"best_mean_reward\": best_rewards_mean[-1] if best_rewards_mean else None,\n",
        "        \"success_count\": success_count,\n",
        "        \"total_steps\": total_steps\n",
        "    }\n",
        "    return summary\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = parse_args()\n",
        "    summary = run_random_agent(args.environment, args.episodes, args.mean_n, args.seed, args.render)\n",
        "    print()\n",
        "    print(\"=\"*10 +\"Summary\" + \"=\"*10)\n",
        "    for k, v in summary.items():\n",
        "        print(f\"{k}: {v}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4xqnADvfPmZ",
        "outputId": "1b0f2c0e-470c-435d-b0fb-9567ba89c551"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment: MountainCar-v0\n",
            "Observation space: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
            "Action space: Discrete(3)\n",
            "Episode 1/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 2/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 3/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 4/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 5/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 6/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 7/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 8/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 9/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 10/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 11/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 12/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 13/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 14/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 15/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 16/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 17/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 18/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 19/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 20/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 21/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 22/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 23/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 24/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 25/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 26/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 27/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 28/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 29/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 30/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 31/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 32/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 33/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 34/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 35/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 36/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 37/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 38/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 39/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 40/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 41/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 42/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 43/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 44/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 45/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 46/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 47/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 48/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 49/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 50/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 51/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 52/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 53/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 54/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 55/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 56/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 57/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 58/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 59/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 60/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 61/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 62/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 63/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 64/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 65/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 66/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 67/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 68/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 69/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 70/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 71/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 72/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 73/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 74/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 75/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 76/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 77/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 78/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 79/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 80/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 81/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 82/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 83/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 84/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 85/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 86/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 87/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 88/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 89/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 90/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 91/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 92/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 93/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 94/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 95/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 96/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 97/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 98/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 99/100 | Reward = -200.00 | Successes so far = 0\n",
            "Episode 100/100 | Reward = -200.00 | Successes so far = 0\n",
            "Saved plot to MountainCar-v0_random_100ep_mean5.png\n",
            "Figure(1200x500)\n",
            "\n",
            "=== Summary ===\n",
            "total_episodes: 100\n",
            "mean_n: 5\n",
            "final_mean_reward: -200.0\n",
            "best_mean_reward: -200.0\n",
            "success_count: 0\n",
            "total_steps: 20000\n"
          ]
        }
      ],
      "source": [
        "# !python3 MountainCar-DQN-a.py --environment \"MountainCar-v0\" --episodes 100 --mean_n 5 --seed 303\n",
        "# run the python CMD via CLI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djzEyIi4jE2u",
        "outputId": "c9eb1c80-fa3d-4bbb-88ec-775fa6ce4cab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing MountainCar-DQN-b.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile MountainCar-DQN-b.py\n",
        "\n",
        "import math\n",
        "import random\n",
        "from itertools import count\n",
        "from collections import namedtuple, deque\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import argparse\n",
        "import pandas as pd\n",
        "\n",
        "#################################\n",
        "# Argument Parsing\n",
        "#################################\n",
        "parser = argparse.ArgumentParser(description=\"DQN for MountainCar-v0\")\n",
        "parser.add_argument(\"--environment\", type=str, default=\"MountainCar-v0\")\n",
        "parser.add_argument(\"--num_episodes\", type=int, default=200)\n",
        "parser.add_argument(\"--batch\", type=int, default=64)\n",
        "parser.add_argument(\"--gamma\", type=float, default=0.99)\n",
        "parser.add_argument(\"--learning_rate\", type=float, default=1e-3)\n",
        "parser.add_argument(\"--mean_n\", type=int, default=5)\n",
        "parser.add_argument(\"--seed\", type=int, default=0)\n",
        "args = parser.parse_args()\n",
        "\n",
        "###############\n",
        "# Setup\n",
        "###############\n",
        "env = gym.make(args.environment)\n",
        "env.reset(seed=args.seed)\n",
        "np.random.seed(args.seed)\n",
        "random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "######################\n",
        "# DQN Model Definition\n",
        "##############3#######\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_dim=2, hidden_dim=200, action_dim=3):\n",
        "        super(DQN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, action_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "####################\n",
        "# Replay Memory\n",
        "####################\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "class ReplayMemory:\n",
        "    def __init__(self, capacity=10000):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "#####################\n",
        "# Helper Functions\n",
        "#####################\n",
        "def get_state(obs):\n",
        "    \"\"\"Convert observation to tensor.\"\"\"\n",
        "    s = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "    return s\n",
        "\n",
        "def select_action(state, eps_threshold):\n",
        "    \"\"\"ε-greedy action selection.\"\"\"\n",
        "    if random.random() > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            return policy_net(state).argmax(dim=1).view(1, 1)\n",
        "    else:\n",
        "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
        "\n",
        "def optimize_model():\n",
        "    \"\"\"Perform a single optimization step.\"\"\"\n",
        "    if len(memory) < batch_size:\n",
        "        return\n",
        "\n",
        "    transitions = memory.sample(batch_size)\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    state_batch = torch.cat(batch.state).to(dtype=torch.float32, device=device)\n",
        "    action_batch = torch.cat(batch.action).to(dtype=torch.long, device=device)\n",
        "    reward_batch = torch.cat(batch.reward).to(dtype=torch.float32, device=device)\n",
        "\n",
        "    non_final_mask = torch.tensor(\n",
        "        tuple(map(lambda s: s is not None, batch.next_state)),\n",
        "        device=device, dtype=torch.bool\n",
        "    )\n",
        "\n",
        "    non_final_next_states = torch.cat(\n",
        "        [s for s in batch.next_state if s is not None]\n",
        "    ).to(dtype=torch.float32, device=device)\n",
        "\n",
        "    # Q(s_t, a)\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # V(s_{t+1})\n",
        "    next_state_values = torch.zeros(batch_size, device=device, dtype=torch.float32)\n",
        "    with torch.no_grad():\n",
        "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
        "\n",
        "    # expected Q values\n",
        "    expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
        "\n",
        "    # loss\n",
        "    loss = F.mse_loss(state_action_values.squeeze(), expected_state_action_values)\n",
        "\n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "##################\n",
        "# Training Loop\n",
        "###################\n",
        "n_actions = env.action_space.n\n",
        "batch_size = args.batch\n",
        "gamma = args.gamma\n",
        "num_episodes = args.num_episodes\n",
        "lr = args.learning_rate\n",
        "mean_n = args.mean_n\n",
        "\n",
        "policy_net = DQN().to(device)\n",
        "target_net = DQN().to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
        "memory = ReplayMemory(10000)\n",
        "\n",
        "eps_start, eps_end, eps_decay = 1.0, 0.02, 800\n",
        "target_update = 10\n",
        "initial_memory = 1000\n",
        "total_steps = 0\n",
        "\n",
        "rewards_list, mean_rewards, best_mean_rewards, steps_list = [], [], [], []\n",
        "best_mean = -float(\"inf\")\n",
        "successes = 0\n",
        "\n",
        "for i_episode in range(num_episodes):\n",
        "    obs, _ = env.reset()\n",
        "    state = get_state(obs)\n",
        "    total_reward = 0.0\n",
        "\n",
        "    for t in count():\n",
        "        eps_threshold = eps_end + (eps_start - eps_end) * math.exp(-1. * total_steps / eps_decay)\n",
        "        action = select_action(state, eps_threshold)\n",
        "        obs_next, reward, done, truncated, _ = env.step(action.item())\n",
        "        done = done or truncated\n",
        "\n",
        "        # --- Reward shaping: ---\n",
        "        # Original reward is always -1 until goal is reached\n",
        "        # We shape it to encourage progress towards the goal\n",
        "        shaped_reward = obs_next[0] + 0.5 + abs(obs_next[1]*10)\n",
        "\n",
        "        # Additional reward for reaching the goal\n",
        "        if obs_next[0] >= 0.5:\n",
        "            shaped_reward += 100 # Large bonus for success\n",
        "\n",
        "\n",
        "        # Convert to tensor\n",
        "        reward_tensor = torch.tensor([shaped_reward], device=device)\n",
        "\n",
        "        next_state = None if done else get_state(obs_next)\n",
        "        memory.push(state, action, next_state, reward_tensor)\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward # Keep track of the TRUE environment reward\n",
        "        total_steps += 1\n",
        "\n",
        "        if total_steps > initial_memory:\n",
        "            optimize_model()\n",
        "\n",
        "        if done:\n",
        "            if obs_next[0] >= 0.5:\n",
        "                successes += 1\n",
        "            break\n",
        "\n",
        "    rewards_list.append(total_reward)\n",
        "\n",
        "    if i_episode >= mean_n:\n",
        "        mean_r = np.mean(rewards_list[-mean_n:])\n",
        "        mean_rewards.append(mean_r)\n",
        "        best_mean = max(best_mean, mean_r)\n",
        "        best_mean_rewards.append(best_mean)\n",
        "        steps_list.append(total_steps)\n",
        "\n",
        "    if i_episode % target_update == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "    print(f\"Episode {i_episode+1}/{num_episodes} | Reward: {total_reward:.2f} | \"\n",
        "          f\"Success: {successes} | epsilon={eps_threshold:.3f}\")\n",
        "\n",
        "###############\n",
        "# Plotting\n",
        "###############\n",
        "env.close()\n",
        "\n",
        "file_name = f\"{args.environment}_DQN_{num_episodes}_episodes_{args.batch}batch.png\"\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Performance Plot\n",
        "# Ensure steps_list and mean_rewards are not empty\n",
        "if steps_list and mean_rewards:\n",
        "    ax1.plot(steps_list, mean_rewards, label=f\"{mean_n}-episode mean\")\n",
        "    ax1.plot(steps_list, best_mean_rewards, label=\"Best mean reward\")\n",
        "ax1.set_xlabel(\"Steps\")\n",
        "ax1.set_ylabel(\"Reward\")\n",
        "ax1.set_title(\"DQN Performance on MountainCar-v0\")\n",
        "ax1.grid(True)\n",
        "ax1.legend()\n",
        "\n",
        "\n",
        "X = np.linspace(-1.5, 0.6, 50)\n",
        "Y = np.linspace(-1, 1, 50)\n",
        "X, Y = np.meshgrid(X, Y)\n",
        "X, Y = X.flatten(), Y.flatten()\n",
        "\n",
        "\n",
        "Z = []\n",
        "# Using policy_net to determine the action for each state\n",
        "states_to_evaluate = torch.tensor(np.array(list(zip(X, Y))), dtype=torch.float32, device=device)\n",
        "with torch.no_grad():\n",
        "    actions_tensor = policy_net(states_to_evaluate).argmax(dim=1).cpu().numpy() # Get index of max Q-value\n",
        "Z = actions_tensor\n",
        "\n",
        "colors = ['lime', 'red', 'blue'] # 0: left, 1: no-op, 2: right\n",
        "ax2.scatter(X, Y, c=[colors[z] for z in Z], s=1, alpha=0.7)\n",
        "ax2.set_xlabel(\"Position\")\n",
        "ax2.set_ylabel(\"Velocity\")\n",
        "ax2.set_title(\"Trained DQN Action Choices\")\n",
        "legend_recs = [mpatches.Patch(color=colors[i], label=f\"Action {i}\") for i in range(3)]\n",
        "ax2.legend(handles=legend_recs)\n",
        "ax2.set_xlim([-1.5, 0.6])\n",
        "ax2.set_ylim([-1, 1])\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(file_name, dpi=200)\n",
        "print(f\"Saved plot to {file_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pl7h-u9DjUwH",
        "outputId": "0d8bbd26-c096-4c34-eaa9-2aa17dc0d6fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1/100 | Reward: -200.00 | Success: 0 | ε=0.784\n",
            "Episode 2/100 | Reward: -200.00 | Success: 0 | ε=0.615\n",
            "Episode 3/100 | Reward: -200.00 | Success: 0 | ε=0.483\n",
            "Episode 4/100 | Reward: -200.00 | Success: 0 | ε=0.381\n",
            "Episode 5/100 | Reward: -200.00 | Success: 0 | ε=0.301\n",
            "Episode 6/100 | Reward: -200.00 | Success: 0 | ε=0.239\n",
            "Episode 7/100 | Reward: -200.00 | Success: 0 | ε=0.191\n",
            "Episode 8/100 | Reward: -200.00 | Success: 0 | ε=0.153\n",
            "Episode 9/100 | Reward: -200.00 | Success: 0 | ε=0.123\n",
            "Episode 10/100 | Reward: -200.00 | Success: 0 | ε=0.101\n",
            "Episode 11/100 | Reward: -200.00 | Success: 0 | ε=0.083\n",
            "Episode 12/100 | Reward: -200.00 | Success: 0 | ε=0.069\n",
            "Episode 13/100 | Reward: -200.00 | Success: 0 | ε=0.058\n",
            "Episode 14/100 | Reward: -200.00 | Success: 0 | ε=0.050\n",
            "Episode 15/100 | Reward: -200.00 | Success: 0 | ε=0.043\n",
            "Episode 16/100 | Reward: -200.00 | Success: 0 | ε=0.038\n",
            "Episode 17/100 | Reward: -200.00 | Success: 0 | ε=0.034\n",
            "Episode 18/100 | Reward: -200.00 | Success: 0 | ε=0.031\n",
            "Episode 19/100 | Reward: -200.00 | Success: 0 | ε=0.028\n",
            "Episode 20/100 | Reward: -200.00 | Success: 0 | ε=0.027\n",
            "Episode 21/100 | Reward: -200.00 | Success: 0 | ε=0.025\n",
            "Episode 22/100 | Reward: -200.00 | Success: 0 | ε=0.024\n",
            "Episode 23/100 | Reward: -200.00 | Success: 0 | ε=0.023\n",
            "Episode 24/100 | Reward: -200.00 | Success: 0 | ε=0.022\n",
            "Episode 25/100 | Reward: -200.00 | Success: 0 | ε=0.022\n",
            "Episode 26/100 | Reward: -200.00 | Success: 0 | ε=0.021\n",
            "Episode 27/100 | Reward: -200.00 | Success: 0 | ε=0.021\n",
            "Episode 28/100 | Reward: -200.00 | Success: 0 | ε=0.021\n",
            "Episode 29/100 | Reward: -200.00 | Success: 0 | ε=0.021\n",
            "Episode 30/100 | Reward: -200.00 | Success: 0 | ε=0.021\n",
            "Episode 31/100 | Reward: -166.00 | Success: 1 | ε=0.020\n",
            "Episode 32/100 | Reward: -200.00 | Success: 1 | ε=0.020\n",
            "Episode 33/100 | Reward: -200.00 | Success: 1 | ε=0.020\n",
            "Episode 34/100 | Reward: -194.00 | Success: 2 | ε=0.020\n",
            "Episode 35/100 | Reward: -200.00 | Success: 2 | ε=0.020\n",
            "Episode 36/100 | Reward: -200.00 | Success: 2 | ε=0.020\n",
            "Episode 37/100 | Reward: -200.00 | Success: 2 | ε=0.020\n",
            "Episode 38/100 | Reward: -200.00 | Success: 2 | ε=0.020\n",
            "Episode 39/100 | Reward: -200.00 | Success: 2 | ε=0.020\n",
            "Episode 40/100 | Reward: -200.00 | Success: 2 | ε=0.020\n",
            "Episode 41/100 | Reward: -200.00 | Success: 2 | ε=0.020\n",
            "Episode 42/100 | Reward: -200.00 | Success: 2 | ε=0.020\n",
            "Episode 43/100 | Reward: -142.00 | Success: 3 | ε=0.020\n",
            "Episode 44/100 | Reward: -200.00 | Success: 3 | ε=0.020\n",
            "Episode 45/100 | Reward: -200.00 | Success: 3 | ε=0.020\n",
            "Episode 46/100 | Reward: -200.00 | Success: 3 | ε=0.020\n",
            "Episode 47/100 | Reward: -170.00 | Success: 4 | ε=0.020\n",
            "Episode 48/100 | Reward: -148.00 | Success: 5 | ε=0.020\n",
            "Episode 49/100 | Reward: -196.00 | Success: 6 | ε=0.020\n",
            "Episode 50/100 | Reward: -154.00 | Success: 7 | ε=0.020\n",
            "Episode 51/100 | Reward: -200.00 | Success: 7 | ε=0.020\n",
            "Episode 52/100 | Reward: -200.00 | Success: 7 | ε=0.020\n",
            "Episode 53/100 | Reward: -200.00 | Success: 7 | ε=0.020\n",
            "Episode 54/100 | Reward: -200.00 | Success: 7 | ε=0.020\n",
            "Episode 55/100 | Reward: -200.00 | Success: 7 | ε=0.020\n",
            "Episode 56/100 | Reward: -200.00 | Success: 7 | ε=0.020\n",
            "Episode 57/100 | Reward: -200.00 | Success: 7 | ε=0.020\n",
            "Episode 58/100 | Reward: -200.00 | Success: 7 | ε=0.020\n",
            "Episode 59/100 | Reward: -122.00 | Success: 8 | ε=0.020\n",
            "Episode 60/100 | Reward: -142.00 | Success: 9 | ε=0.020\n",
            "Episode 61/100 | Reward: -131.00 | Success: 10 | ε=0.020\n",
            "Episode 62/100 | Reward: -132.00 | Success: 11 | ε=0.020\n",
            "Episode 63/100 | Reward: -200.00 | Success: 11 | ε=0.020\n",
            "Episode 64/100 | Reward: -138.00 | Success: 12 | ε=0.020\n",
            "Episode 65/100 | Reward: -200.00 | Success: 12 | ε=0.020\n",
            "Episode 66/100 | Reward: -124.00 | Success: 13 | ε=0.020\n",
            "Episode 67/100 | Reward: -155.00 | Success: 14 | ε=0.020\n",
            "Episode 68/100 | Reward: -141.00 | Success: 15 | ε=0.020\n",
            "Episode 69/100 | Reward: -119.00 | Success: 16 | ε=0.020\n",
            "Episode 70/100 | Reward: -145.00 | Success: 17 | ε=0.020\n",
            "Episode 71/100 | Reward: -173.00 | Success: 18 | ε=0.020\n",
            "Episode 72/100 | Reward: -164.00 | Success: 19 | ε=0.020\n",
            "Episode 73/100 | Reward: -141.00 | Success: 20 | ε=0.020\n",
            "Episode 74/100 | Reward: -148.00 | Success: 21 | ε=0.020\n",
            "Episode 75/100 | Reward: -200.00 | Success: 21 | ε=0.020\n",
            "Episode 76/100 | Reward: -146.00 | Success: 22 | ε=0.020\n",
            "Episode 77/100 | Reward: -160.00 | Success: 23 | ε=0.020\n",
            "Episode 78/100 | Reward: -159.00 | Success: 24 | ε=0.020\n",
            "Episode 79/100 | Reward: -200.00 | Success: 24 | ε=0.020\n",
            "Episode 80/100 | Reward: -162.00 | Success: 25 | ε=0.020\n",
            "Episode 81/100 | Reward: -185.00 | Success: 26 | ε=0.020\n",
            "Episode 82/100 | Reward: -144.00 | Success: 27 | ε=0.020\n",
            "Episode 83/100 | Reward: -109.00 | Success: 28 | ε=0.020\n",
            "Episode 84/100 | Reward: -135.00 | Success: 29 | ε=0.020\n",
            "Episode 85/100 | Reward: -151.00 | Success: 30 | ε=0.020\n",
            "Episode 86/100 | Reward: -148.00 | Success: 31 | ε=0.020\n",
            "Episode 87/100 | Reward: -154.00 | Success: 32 | ε=0.020\n",
            "Episode 88/100 | Reward: -119.00 | Success: 33 | ε=0.020\n",
            "Episode 89/100 | Reward: -151.00 | Success: 34 | ε=0.020\n",
            "Episode 90/100 | Reward: -152.00 | Success: 35 | ε=0.020\n",
            "Episode 91/100 | Reward: -151.00 | Success: 36 | ε=0.020\n",
            "Episode 92/100 | Reward: -128.00 | Success: 37 | ε=0.020\n",
            "Episode 93/100 | Reward: -160.00 | Success: 38 | ε=0.020\n",
            "Episode 94/100 | Reward: -154.00 | Success: 39 | ε=0.020\n",
            "Episode 95/100 | Reward: -159.00 | Success: 40 | ε=0.020\n",
            "Episode 96/100 | Reward: -137.00 | Success: 41 | ε=0.020\n",
            "Episode 97/100 | Reward: -150.00 | Success: 42 | ε=0.020\n",
            "Episode 98/100 | Reward: -150.00 | Success: 43 | ε=0.020\n",
            "Episode 99/100 | Reward: -151.00 | Success: 44 | ε=0.020\n",
            "Episode 100/100 | Reward: -140.00 | Success: 45 | ε=0.020\n",
            "Saved plot to MountainCar-v0_DQN_100_episodes.png\n"
          ]
        }
      ],
      "source": [
        "# !python3 MountainCar-DQN-b.py --environment=\"MountainCar-v0\" --num_episodes=100 --batch=8 --gamma=0.9 --learning_rate=1e-4 --mean_n=5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile MountainCar-DQN-c.py\n",
        "import subprocess\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import random\n",
        "from itertools import count\n",
        "from collections import namedtuple, deque\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import argparse\n",
        "import pandas as pd\n",
        "\n",
        "# Fixed settings\n",
        "environment = \"MountainCar-v0\"\n",
        "num_episodes = 100\n",
        "\n",
        "learning_rate = 1e-4\n",
        "gamma = 0.9\n",
        "mean_n = 5\n",
        "\n",
        "# Batch sizes to compare\n",
        "batch_sizes = [8, 16, 32, 64]\n",
        "\n",
        "# Store results\n",
        "batch_rewards = {}\n",
        "\n",
        "for b in batch_sizes:\n",
        "    print(f\"\\nRunning with batch size = {b}\")\n",
        "    result = subprocess.run(\n",
        "        [\n",
        "            \"python3\",\n",
        "            \"MountainCar-DQN-b.py\",\n",
        "            \"--environment\", environment,\n",
        "            \"--num_episodes\", str(num_episodes),\n",
        "            \"--batch\", str(b),\n",
        "            \"--learning_rate\", str(learning_rate),\n",
        "            \"--gamma\", str(gamma),\n",
        "            \"--mean_n\", str(mean_n)\n",
        "        ],\n",
        "        capture_output=True, text=True\n",
        "    )\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
