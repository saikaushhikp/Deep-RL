{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRNQhyWeyxHb",
        "outputId": "af64b4f6-f3cb-4dbb-a945-be76983a5125"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting LunarLander-PG-a.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile LunarLander-PG-a.py\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import argparse\n",
        "from itertools import count\n",
        "\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(description=\"Policy Gradient for LunarLander-v3\")\n",
        "parser.add_argument(\"--environment\", type=str, required=True,\n",
        "                    help=\"Gym environment name (e.g., LunarLander-v3\")\n",
        "parser.add_argument(\"--iterations\", type=int, default=50,\n",
        "                    help=\"Number of iterations (each iteration = batch of episodes)\")\n",
        "parser.add_argument(\"--batch\", type=int, default=32,\n",
        "                    help=\"Batch size (number of episodes per iteration)\")\n",
        "args = parser.parse_args()\n",
        "\n",
        "\n",
        "\n",
        "def plot_rewards(rewards, filename):\n",
        "    \"\"\"Plot the average total rewards per iteration.\"\"\"\n",
        "    plt.figure(figsize=(10, 5), dpi=100)\n",
        "    plt.plot(rewards, label='Average Reward per Iteration')\n",
        "    plt.title(f\"Random Agent Performance on {args.environment}\")\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Average Total Reward\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def get_state(obs):\n",
        "    \"\"\"Convert observation (state) to PyTorch tensor.\"\"\"\n",
        "    state = torch.from_numpy(np.array(obs)).float().unsqueeze(0)\n",
        "    return state\n",
        "\n",
        "\n",
        "def select_random_action(env):\n",
        "    \"\"\"Select a random valid action from the environment.\"\"\"\n",
        "    return env.action_space.sample()\n",
        "\n",
        "\n",
        "def run_random_agent(env, iterations, batch_size):\n",
        "    \"\"\"Run random agent for given environment and collect average rewards.\"\"\"\n",
        "    all_rewards = []\n",
        "\n",
        "    for it in range(iterations):\n",
        "        batch_rewards = []\n",
        "\n",
        "        for ep in range(batch_size):\n",
        "            obs = env.reset()\n",
        "            # Compatible with Gym v0.26+\n",
        "            if isinstance(obs, tuple):\n",
        "                obs = obs[0]\n",
        "\n",
        "            total_reward = 0\n",
        "            for t in count():\n",
        "                action = select_random_action(env)\n",
        "                step_output = env.step(action)\n",
        "\n",
        "                if len(step_output) == 5:\n",
        "                    obs, reward, terminated, truncated, _ = step_output\n",
        "                    done = terminated or truncated\n",
        "                else:\n",
        "                    obs, reward, done, _ = step_output\n",
        "\n",
        "                total_reward += reward\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            batch_rewards.append(total_reward)\n",
        "\n",
        "        avg_reward = np.mean(batch_rewards)\n",
        "        print(f\"Iteration {it+1}/{iterations}: Average Reward = {avg_reward:.2f}\")\n",
        "        all_rewards.append(avg_reward)\n",
        "\n",
        "    env.close()\n",
        "    return all_rewards\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load Gym environment\n",
        "    env = gym.make(args.environment)\n",
        "    print(f\"\\nLoaded Environment: {args.environment}\")\n",
        "    print(f\"Observation space: {env.observation_space}\")\n",
        "    print(f\"Action space: {env.action_space}\\n\")\n",
        "\n",
        "    # Run random agent\n",
        "    rewards = run_random_agent(env, args.iterations, args.batch)\n",
        "\n",
        "    # Save plot\n",
        "    file_name = f\"{args.environment}_{args.iterations}_{args.batch}_random_agent.png\"\n",
        "    plot_rewards(rewards, file_name)\n",
        "    print(f\"Plot saved as {file_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SPe-ToezaNN",
        "outputId": "f851d5d3-cf53-48a0-e1aa-d2d2d5cd5386"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded Environment: LunarLander-v3\n",
            "Observation space: Box([ -2.5        -2.5       -10.        -10.         -6.2831855 -10.\n",
            "  -0.         -0.       ], [ 2.5        2.5       10.        10.         6.2831855 10.\n",
            "  1.         1.       ], (8,), float32)\n",
            "Action space: Discrete(4)\n",
            "\n",
            "Iteration 1/50: Average Reward = -171.23\n",
            "Iteration 2/50: Average Reward = -182.92\n",
            "Iteration 3/50: Average Reward = -213.44\n",
            "Iteration 4/50: Average Reward = -170.92\n",
            "Iteration 5/50: Average Reward = -174.31\n",
            "Iteration 6/50: Average Reward = -151.22\n",
            "Iteration 7/50: Average Reward = -206.65\n",
            "Iteration 8/50: Average Reward = -160.01\n",
            "Iteration 9/50: Average Reward = -182.23\n",
            "Iteration 10/50: Average Reward = -174.77\n",
            "Iteration 11/50: Average Reward = -218.77\n",
            "Iteration 12/50: Average Reward = -154.10\n",
            "Iteration 13/50: Average Reward = -181.44\n",
            "Iteration 14/50: Average Reward = -141.88\n",
            "Iteration 15/50: Average Reward = -176.98\n",
            "Iteration 16/50: Average Reward = -152.25\n",
            "Iteration 17/50: Average Reward = -204.13\n",
            "Iteration 18/50: Average Reward = -188.76\n",
            "Iteration 19/50: Average Reward = -152.85\n",
            "Iteration 20/50: Average Reward = -190.59\n",
            "Iteration 21/50: Average Reward = -198.43\n",
            "Iteration 22/50: Average Reward = -196.68\n",
            "Iteration 23/50: Average Reward = -201.49\n",
            "Iteration 24/50: Average Reward = -217.16\n",
            "Iteration 25/50: Average Reward = -199.03\n",
            "Iteration 26/50: Average Reward = -183.29\n",
            "Iteration 27/50: Average Reward = -157.04\n",
            "Iteration 28/50: Average Reward = -201.39\n",
            "Iteration 29/50: Average Reward = -174.38\n",
            "Iteration 30/50: Average Reward = -168.22\n",
            "Iteration 31/50: Average Reward = -194.17\n",
            "Iteration 32/50: Average Reward = -157.45\n",
            "Iteration 33/50: Average Reward = -192.06\n",
            "Iteration 34/50: Average Reward = -193.67\n",
            "Iteration 35/50: Average Reward = -184.22\n",
            "Iteration 36/50: Average Reward = -182.56\n",
            "Iteration 37/50: Average Reward = -239.88\n",
            "Iteration 38/50: Average Reward = -160.99\n",
            "Iteration 39/50: Average Reward = -225.96\n",
            "Iteration 40/50: Average Reward = -183.31\n",
            "Iteration 41/50: Average Reward = -237.68\n",
            "Iteration 42/50: Average Reward = -145.81\n",
            "Iteration 43/50: Average Reward = -198.92\n",
            "Iteration 44/50: Average Reward = -199.42\n",
            "Iteration 45/50: Average Reward = -183.99\n",
            "Iteration 46/50: Average Reward = -188.13\n",
            "Iteration 47/50: Average Reward = -203.76\n",
            "Iteration 48/50: Average Reward = -207.33\n",
            "Iteration 49/50: Average Reward = -189.99\n",
            "Iteration 50/50: Average Reward = -151.26\n",
            "Plot saved as LunarLander-v3_50_16_random_agent.png\n"
          ]
        }
      ],
      "source": [
        "# !python3 LunarLander-PG-a.py --environment LunarLander-v3 --iterations 50 --batch 16\n",
        "# run the python CMD via CLI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5k2TYtCzoN4",
        "outputId": "0f663b4b-3232-44ac-973b-87203f4ab05d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing LunarLander-PG-b.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile LunarLander-PG-b.py\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "\n",
        "\n",
        "class Policy(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
        "        super(Policy, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, action_dim)\n",
        "        )\n",
        "    \n",
        "        for layer in self.network:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                nn.init.orthogonal_(layer.weight, gain=np.sqrt(2))\n",
        "                nn.init.constant_(layer.bias, 0.0)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "def compute_returns(rewards, gamma, reward_to_go=False):\n",
        "    \"\"\"Compute discounted returns.\"\"\"\n",
        "    if reward_to_go:\n",
        "        returns = []\n",
        "        future_return = 0\n",
        "        for r in reversed(rewards):\n",
        "            future_return = r + gamma * future_return\n",
        "            returns.insert(0, future_return)\n",
        "        return returns\n",
        "    else:\n",
        "        total_return = sum([gamma**i * r for i, r in enumerate(rewards)])\n",
        "        return [total_return] * len(rewards)\n",
        "\n",
        "def plot_rewards(all_mean_rewards, fname):\n",
        "    k = 100\n",
        "    running_avg = np.convolve(all_mean_rewards, np.ones(k)/k, mode='valid')\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(all_mean_rewards, label=\"All Returns\", alpha=0.5)\n",
        "    plt.plot(np.arange(k-1, len(all_mean_rewards)), running_avg, label=f\"Running Average (k={k})\", color='r', linewidth=2)\n",
        "    plt.title(f\"{fname}\")\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Average Return\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(fname + \".png\")\n",
        "    print(f\"Plot saved as {fname}.png\\n\")\n",
        "    plt.close()\n",
        "\n",
        "def pg_training(env_name=\"CartPole-v1\", iterations=100, batch_size=2000,lr=1e-2, gamma=0.99, reward_to_go=True, advantage_norm=True,hidden_dim=128,):\n",
        "    \n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"PG training on {env_name} | reward_to_go={reward_to_go} advantage_norm={advantage_norm}  | device = {device}\")\n",
        "    env = gym.make(env_name)\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    n_actions = env.action_space.n\n",
        "    \n",
        "    policy = Policy(state_dim, n_actions, hidden_dim).to(device)\n",
        "    optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
        "\n",
        "    all_mean_rewards = []\n",
        "\n",
        "    for it in range(iterations):\n",
        "        batch_states, batch_actions, batch_weights, batch_episode_rewards = [], [], [], []\n",
        "        steps = 0\n",
        "\n",
        "        # collect trajectories until we reach desired batch size\n",
        "        while steps < batch_size:\n",
        "            obs, info = env.reset()\n",
        "            done = False\n",
        "            states, actions, rewards = [], [], []\n",
        "            while True:\n",
        "                s_t = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "                logits = policy(s_t)\n",
        "                dist = torch.distributions.Categorical(logits=logits)\n",
        "                action = int(dist.sample().item())\n",
        "\n",
        "                next_obs, reward, terminated, truncated, info = env.step(action)\n",
        "                done_flag = terminated or truncated\n",
        "\n",
        "                states.append(obs)\n",
        "                actions.append(action)\n",
        "                rewards.append(reward)\n",
        "                obs = next_obs\n",
        "\n",
        "                if done_flag:\n",
        "                    break\n",
        "\n",
        "            steps += len(states)\n",
        "            batch_states += states\n",
        "            batch_actions += actions\n",
        "            batch_episode_rewards.append(sum(rewards))\n",
        "            batch_weights += list(compute_returns(rewards, gamma, reward_to_go))\n",
        "\n",
        "        # prepare tensors\n",
        "        batch_states_t = torch.tensor(np.array(batch_states), dtype=torch.float32, device=device)\n",
        "        batch_actions_t = torch.tensor(batch_actions, dtype=torch.int64, device=device)\n",
        "        batch_weights_t = torch.tensor(batch_weights, dtype=torch.float32, device=device)\n",
        "\n",
        "        # advantage normalization\n",
        "        if advantage_norm:\n",
        "            mean = batch_weights_t.mean()\n",
        "            std = batch_weights_t.std() + 1e-8\n",
        "            batch_weights_t = (batch_weights_t - mean) / std\n",
        "\n",
        "        logits = policy(batch_states_t)\n",
        "        dists = torch.distributions.Categorical(logits=logits)\n",
        "        logp = dists.log_prob(batch_actions_t)\n",
        "        loss = -(logp * batch_weights_t).mean()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        mean_reward = np.mean(batch_episode_rewards)\n",
        "        all_mean_rewards.append(mean_reward)\n",
        "        print(f\"Iteration {it+1}/{iterations} | Mean Reward: {mean_reward:.2f} | Episodes: {len(batch_episode_rewards)}\")\n",
        "\n",
        "    return all_mean_rewards\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":    \n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--environment\", type=str, default=\"LunarLander-v3\")\n",
        "    parser.add_argument(\"--iterations\", type=int, default=300)\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=8000)\n",
        "    parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    parser.add_argument(\"--gamma\", type=float, default=0.99)\n",
        "    parser.add_argument(\"--reward_to_go\", action=\"store_true\")\n",
        "    parser.add_argument(\"--advantage_norm\", action=\"store_true\")\n",
        "    parser.add_argument(\"--reward_scale\", type=float, default=1.0)\n",
        "    parser.add_argument(\"--reward_clip_min\", type=float, default=None)\n",
        "    parser.add_argument(\"--reward_clip_max\", type=float, default=None)\n",
        "    parser.add_argument(\"--hidden_dim\", type=int, default=128)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    all_mean_rewards = pg_training(\n",
        "        env_name=args.environment,\n",
        "        iterations=args.iterations,\n",
        "        batch_size=args.batch_size,\n",
        "        lr=args.lr,\n",
        "        gamma=args.gamma,\n",
        "        reward_to_go=args.reward_to_go,\n",
        "        advantage_norm=args.advantage_norm,\n",
        "        hidden_dim=args.hidden_dim\n",
        "    )\n",
        "\n",
        "    fname = f\"{args.environment}_PG_iters{args.iterations}_bs{args.batch_size}_g_lr_{'rtg' if args.reward_to_go else 'tot'}_{'advnorm' if args.advantage_norm else 'noadv'}_\"\n",
        "    plot_rewards(all_mean_rewards, fname)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJlEcsvA2TAP",
        "outputId": "067b4229-a5d3-4bdd-d826-d79d2f5f9fb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment: LunarLander-v3\n",
            "Box([ -2.5        -2.5       -10.        -10.         -6.2831855 -10.\n",
            "  -0.         -0.       ], [ 2.5        2.5       10.        10.         6.2831855 10.\n",
            "  1.         1.       ], (8,), float32)\n",
            "Discrete(4)\n",
            "Device: cpu\n",
            "Policy Params: 18180\n",
            "Iter 1/5 | AvgReturn -205.115 | Loss 1.150\n",
            "Iter 2/5 | AvgReturn -174.408 | Loss -5.880\n",
            "Iter 3/5 | AvgReturn -177.800 | Loss -3.110\n",
            "Iter 4/5 | AvgReturn -176.663 | Loss 12.527\n",
            "Iter 5/5 | AvgReturn -189.604 | Loss -10.015\n",
            "Training finished. Plot saved to LunarLander-v3_PG_iters5_batch40_g_lr_rtg_advnorm_.png\n",
            "Saved plot: LunarLander-v3_PG_iters5_batch40_g_lr_rtg_advnorm_.png\n"
          ]
        }
      ],
      "source": [
        "# !python3 LunarLander-PG-b.py --environment LunarLander-v3 --iterations 5 --batch 40 --gamma 0.99 --learning_rate 5e-4 --reward_to_go --advantage_norm --reward_scale 1.0\n",
        "# run the python CMD via CLI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile LunarLander-PG-c.py\n",
        "\n",
        "import subprocess\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Fixed settings\n",
        "environment = \"LunarLander-v3\"\n",
        "iterations = 1200\n",
        "learning_rate = 1e-3\n",
        "gamma = 0.99\n",
        "\n",
        "# Batch sizes to compare\n",
        "batch_sizes = [80, 800, 8000]\n",
        "\n",
        "# Store results\n",
        "batch_rewards = {}\n",
        "\n",
        "for b in batch_sizes:\n",
        "    script_path = os.path.join(os.path.dirname(__file__), \"LunarLander-PG-b.py\")\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\n",
        "                sys.executable,\n",
        "                script_path,\n",
        "                \"--environment\", environment,\n",
        "                \"--iterations\", str(iterations),\n",
        "                \"--batch_size\", str(b),\n",
        "                \"--lr\", str(learning_rate),\n",
        "                \"--gamma\", str(gamma),\n",
        "                \"--reward_to_go\",\n",
        "                \"--advantage_norm\"\n",
        "            ],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            check=True,\n",
        "            cwd=os.path.dirname(__file__)\n",
        "        )\n",
        "        # show subprocess output for debugging / parsing\n",
        "        if result.stdout:\n",
        "            print(result.stdout)\n",
        "        if result.stderr:\n",
        "            print(\"Subprocess stderr:\", result.stderr)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(\"Subprocess failed with returncode\", e.returncode)\n",
        "        if e.stdout:\n",
        "            print(e.stdout)\n",
        "        if e.stderr:\n",
        "            print(e.stderr)\n",
        "        continue\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# run the python CMD via CLI\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
