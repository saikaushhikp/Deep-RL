{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRNQhyWeyxHb",
        "outputId": "af64b4f6-f3cb-4dbb-a945-be76983a5125"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting LunarLander-PG-a.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile LunarLander-PG-a.py\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import random\n",
        "import argparse\n",
        "from itertools import count\n",
        "\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(description=\"Policy Gradient for LunarLander-v3\")\n",
        "parser.add_argument(\"--environment\", type=str, required=True,\n",
        "                    help=\"Gym environment name (e.g., LunarLander-v3\")\n",
        "parser.add_argument(\"--iterations\", type=int, default=50,\n",
        "                    help=\"Number of iterations (each iteration = batch of episodes)\")\n",
        "parser.add_argument(\"--batch\", type=int, default=32,\n",
        "                    help=\"Batch size (number of episodes per iteration)\")\n",
        "args = parser.parse_args()\n",
        "\n",
        "\n",
        "\n",
        "def plot_rewards(rewards, filename):\n",
        "    \"\"\"Plot the average total rewards per iteration.\"\"\"\n",
        "    plt.figure(figsize=(10, 5), dpi=100)\n",
        "    plt.plot(rewards, label='Average Reward per Iteration')\n",
        "    plt.title(f\"Random Agent Performance on {args.environment}\")\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Average Total Reward\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def get_state(obs):\n",
        "    \"\"\"Convert observation (state) to PyTorch tensor.\"\"\"\n",
        "    state = torch.from_numpy(np.array(obs)).float().unsqueeze(0)\n",
        "    return state\n",
        "\n",
        "\n",
        "def select_random_action(env):\n",
        "    \"\"\"Select a random valid action from the environment.\"\"\"\n",
        "    return env.action_space.sample()\n",
        "\n",
        "\n",
        "def run_random_agent(env, iterations, batch_size):\n",
        "    \"\"\"Run random agent for given environment and collect average rewards.\"\"\"\n",
        "    all_rewards = []\n",
        "\n",
        "    for it in range(iterations):\n",
        "        batch_rewards = []\n",
        "\n",
        "        for ep in range(batch_size):\n",
        "            obs = env.reset()\n",
        "            # Compatible with Gym v0.26+\n",
        "            if isinstance(obs, tuple):\n",
        "                obs = obs[0]\n",
        "\n",
        "            total_reward = 0\n",
        "            for t in count():\n",
        "                action = select_random_action(env)\n",
        "                step_output = env.step(action)\n",
        "\n",
        "                if len(step_output) == 5:\n",
        "                    obs, reward, terminated, truncated, _ = step_output\n",
        "                    done = terminated or truncated\n",
        "                else:\n",
        "                    obs, reward, done, _ = step_output\n",
        "\n",
        "                total_reward += reward\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            batch_rewards.append(total_reward)\n",
        "\n",
        "        avg_reward = np.mean(batch_rewards)\n",
        "        print(f\"Iteration {it+1}/{iterations}: Average Reward = {avg_reward:.2f}\")\n",
        "        all_rewards.append(avg_reward)\n",
        "\n",
        "    env.close()\n",
        "    return all_rewards\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load Gym environment\n",
        "    env = gym.make(args.environment)\n",
        "    print(f\"\\nLoaded Environment: {args.environment}\")\n",
        "    print(f\"Observation space: {env.observation_space}\")\n",
        "    print(f\"Action space: {env.action_space}\\n\")\n",
        "\n",
        "    # Run random agent\n",
        "    rewards = run_random_agent(env, args.iterations, args.batch)\n",
        "\n",
        "    # Save plot\n",
        "    file_name = f\"{args.environment}_{args.iterations}_{args.batch}_random_agent.png\"\n",
        "    plot_rewards(rewards, file_name)\n",
        "    print(f\"Plot saved as {file_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SPe-ToezaNN",
        "outputId": "f851d5d3-cf53-48a0-e1aa-d2d2d5cd5386"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded Environment: LunarLander-v3\n",
            "Observation space: Box([ -2.5        -2.5       -10.        -10.         -6.2831855 -10.\n",
            "  -0.         -0.       ], [ 2.5        2.5       10.        10.         6.2831855 10.\n",
            "  1.         1.       ], (8,), float32)\n",
            "Action space: Discrete(4)\n",
            "\n",
            "Iteration 1/50: Average Reward = -171.23\n",
            "Iteration 2/50: Average Reward = -182.92\n",
            "Iteration 3/50: Average Reward = -213.44\n",
            "Iteration 4/50: Average Reward = -170.92\n",
            "Iteration 5/50: Average Reward = -174.31\n",
            "Iteration 6/50: Average Reward = -151.22\n",
            "Iteration 7/50: Average Reward = -206.65\n",
            "Iteration 8/50: Average Reward = -160.01\n",
            "Iteration 9/50: Average Reward = -182.23\n",
            "Iteration 10/50: Average Reward = -174.77\n",
            "Iteration 11/50: Average Reward = -218.77\n",
            "Iteration 12/50: Average Reward = -154.10\n",
            "Iteration 13/50: Average Reward = -181.44\n",
            "Iteration 14/50: Average Reward = -141.88\n",
            "Iteration 15/50: Average Reward = -176.98\n",
            "Iteration 16/50: Average Reward = -152.25\n",
            "Iteration 17/50: Average Reward = -204.13\n",
            "Iteration 18/50: Average Reward = -188.76\n",
            "Iteration 19/50: Average Reward = -152.85\n",
            "Iteration 20/50: Average Reward = -190.59\n",
            "Iteration 21/50: Average Reward = -198.43\n",
            "Iteration 22/50: Average Reward = -196.68\n",
            "Iteration 23/50: Average Reward = -201.49\n",
            "Iteration 24/50: Average Reward = -217.16\n",
            "Iteration 25/50: Average Reward = -199.03\n",
            "Iteration 26/50: Average Reward = -183.29\n",
            "Iteration 27/50: Average Reward = -157.04\n",
            "Iteration 28/50: Average Reward = -201.39\n",
            "Iteration 29/50: Average Reward = -174.38\n",
            "Iteration 30/50: Average Reward = -168.22\n",
            "Iteration 31/50: Average Reward = -194.17\n",
            "Iteration 32/50: Average Reward = -157.45\n",
            "Iteration 33/50: Average Reward = -192.06\n",
            "Iteration 34/50: Average Reward = -193.67\n",
            "Iteration 35/50: Average Reward = -184.22\n",
            "Iteration 36/50: Average Reward = -182.56\n",
            "Iteration 37/50: Average Reward = -239.88\n",
            "Iteration 38/50: Average Reward = -160.99\n",
            "Iteration 39/50: Average Reward = -225.96\n",
            "Iteration 40/50: Average Reward = -183.31\n",
            "Iteration 41/50: Average Reward = -237.68\n",
            "Iteration 42/50: Average Reward = -145.81\n",
            "Iteration 43/50: Average Reward = -198.92\n",
            "Iteration 44/50: Average Reward = -199.42\n",
            "Iteration 45/50: Average Reward = -183.99\n",
            "Iteration 46/50: Average Reward = -188.13\n",
            "Iteration 47/50: Average Reward = -203.76\n",
            "Iteration 48/50: Average Reward = -207.33\n",
            "Iteration 49/50: Average Reward = -189.99\n",
            "Iteration 50/50: Average Reward = -151.26\n",
            "Plot saved as LunarLander-v3_50_16_random_agent.png\n"
          ]
        }
      ],
      "source": [
        "# !python3 LunarLander-PG-a.py --environment LunarLander-v3 --iterations 50 --batch 16\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5k2TYtCzoN4",
        "outputId": "0f663b4b-3232-44ac-973b-87203f4ab05d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing LunarLander-PG-b.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile LunarLander-PG-b.py\n",
        "import argparse, os, time, random\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class Policy(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim, hidden_sizes=(128, 128)):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        prev_dim = obs_dim\n",
        "        for h in hidden_sizes:\n",
        "            layers += [nn.Linear(prev_dim, h), nn.ReLU()]\n",
        "            prev_dim = h\n",
        "        layers += [nn.Linear(prev_dim, act_dim), nn.Softmax(dim=-1)]\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, obs):\n",
        "        return self.net(obs)\n",
        "\n",
        "\n",
        "def discount_cumsum(rewards, gamma):\n",
        "    \"\"\"Compute discounted cumulative rewards.\"\"\"\n",
        "    discounted = np.zeros_like(rewards, dtype=np.float32)\n",
        "    running_add = 0\n",
        "    for t in reversed(range(len(rewards))):\n",
        "        running_add = rewards[t] + gamma * running_add\n",
        "        discounted[t] = running_add\n",
        "    return list(discounted)\n",
        "\n",
        "\n",
        "def get_obs(obs):\n",
        "    return np.array(obs[0] if isinstance(obs, (tuple, list)) else obs)\n",
        "\n",
        "\n",
        "def step_env(env, action):\n",
        "    out = env.step(action)\n",
        "    if len(out) == 5:\n",
        "        obs, reward, term, trunc, info = out\n",
        "        done = term or trunc\n",
        "    else:\n",
        "        obs, reward, done, info = out\n",
        "    return obs, reward, done, info\n",
        "\n",
        "\n",
        "def run_training(env_name, iterations, batch_size, lr, gamma,\n",
        "                 reward_to_go, advantage_norm, reward_scale,\n",
        "                 reward_clip, hidden_dim, device, fname):\n",
        "\n",
        "\n",
        "\n",
        "    env = gym.make(env_name)\n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "    act_dim = env.action_space.n\n",
        "\n",
        "    policy = Policy(obs_dim, act_dim, hidden_sizes=(hidden_dim, hidden_dim)).to(device)\n",
        "    optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
        "\n",
        "    print(f\"\\nTraining {env_name} | Î³={gamma} | lr={lr} | batch={batch_size} | device={device}\")\n",
        "\n",
        "    all_returns = []\n",
        "    best_return = float('-inf')\n",
        "    best_policy_state = None\n",
        "\n",
        "    for it in range(iterations):\n",
        "        # Evaluate current best policy\n",
        "        with torch.no_grad():\n",
        "            eval_obs = get_obs(env.reset())\n",
        "            eval_rewards = []\n",
        "            eval_done = False\n",
        "            while not eval_done:\n",
        "                eval_obs_tensor = torch.tensor(eval_obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "                eval_probs = policy(eval_obs_tensor)\n",
        "                # Use best action (no sampling)\n",
        "                eval_action = torch.argmax(eval_probs)\n",
        "                eval_next_obs, eval_reward, eval_done, _ = step_env(env, eval_action.item())\n",
        "                eval_rewards.append(eval_reward)\n",
        "                eval_obs = get_obs(eval_next_obs)\n",
        "            eval_return = sum(eval_rewards)\n",
        "            print(f\"Iter {it+1:4d}/{iterations} | Best Policy Return: {eval_return:8.2f}\")\n",
        "\n",
        "        # Collect training data with exploration\n",
        "        batch_obs, batch_acts, batch_weights = [], [], []\n",
        "        ep_returns = []\n",
        "        log_probs = []\n",
        "\n",
        "        steps_collected = 0\n",
        "        while steps_collected < batch_size:\n",
        "            obs = get_obs(env.reset())\n",
        "            ep_rewards, ep_logps, ep_obs, ep_acts = [], [], [], []\n",
        "\n",
        "            done = False\n",
        "            while not done:\n",
        "                obs_tensor = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "                probs = policy(obs_tensor)\n",
        "                dist = Categorical(probs)\n",
        "                action = dist.sample()\n",
        "\n",
        "\n",
        "                next_obs, reward, done, _ = step_env(env, action.item())\n",
        "                if reward_scale != 1.0:\n",
        "                    reward *= reward_scale\n",
        "                if reward_clip:\n",
        "                    reward = np.clip(reward, reward_clip[0], reward_clip[1])\n",
        "\n",
        "                ep_obs.append(obs)\n",
        "                ep_acts.append(action.item())\n",
        "                ep_logps.append(dist.log_prob(action))\n",
        "                ep_rewards.append(reward)\n",
        "                obs = get_obs(next_obs)\n",
        "\n",
        "            # Compute returns\n",
        "            if reward_to_go:\n",
        "                ep_returns = discount_cumsum(ep_rewards, gamma)\n",
        "            else:\n",
        "                G = sum([gamma**t * r for t, r in enumerate(ep_rewards)])\n",
        "                ep_returns = [G for _ in ep_rewards]\n",
        "\n",
        "            batch_obs += ep_obs\n",
        "            batch_acts += ep_acts\n",
        "            batch_weights += list(ep_returns)\n",
        "            log_probs += ep_logps\n",
        "            ep_returns_sum = sum(ep_rewards)\n",
        "            ep_returns.append(ep_returns_sum)\n",
        "            ep_returns = np.array(ep_returns)\n",
        "\n",
        "            steps_collected += len(ep_rewards)\n",
        "\n",
        "        # Convert to tensors\n",
        "        log_probs_tensor = torch.stack(log_probs).to(device)\n",
        "        advantages = torch.tensor(batch_weights, dtype=torch.float32, device=device)\n",
        "\n",
        "        # Ensure dimensions match\n",
        "        advantages = advantages.reshape(-1)  # Flatten to 1D\n",
        "        log_probs_tensor = log_probs_tensor.reshape(-1)  # Flatten to 1D\n",
        "\n",
        "        # Baseline + normalization\n",
        "        if advantage_norm:\n",
        "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "\n",
        "        loss = -(log_probs_tensor * advantages).mean()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(policy.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_return = np.mean(ep_returns)\n",
        "        all_returns.append(avg_return)\n",
        "\n",
        "\n",
        "        print(f\"Iter {it+1:4d}/{iterations} | AvgReturn: {avg_return:8.2f} | Loss: {loss:.4f}\")\n",
        "\n",
        "        # Evaluate current policy\n",
        "        with torch.no_grad():\n",
        "            eval_obs = get_obs(env.reset())\n",
        "            eval_rewards = []\n",
        "            eval_done = False\n",
        "            while not eval_done:\n",
        "                eval_obs_tensor = torch.tensor(eval_obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "                eval_probs = policy(eval_obs_tensor)\n",
        "                eval_action = torch.argmax(eval_probs)\n",
        "                eval_next_obs, eval_reward, eval_done, _ = step_env(env, eval_action.item())\n",
        "                eval_rewards.append(eval_reward)\n",
        "                eval_obs = get_obs(eval_next_obs)\n",
        "            current_eval_return = sum(eval_rewards)\n",
        "\n",
        "            # Update best policy if current one is better\n",
        "            if current_eval_return > best_return:\n",
        "                best_return = current_eval_return\n",
        "                best_policy_state = policy.state_dict().copy()\n",
        "                print(f\"New best policy found. Return: {best_return:8.2f}\")\n",
        "            else:\n",
        "                # Load best policy for next iteration\n",
        "                policy.load_state_dict(best_policy_state)\n",
        "                print(f\"Reverting to best policy (Return: {best_return:8.2f})\")\n",
        "\n",
        "    env.close()\n",
        "   \n",
        "    k = 100\n",
        "    running_avg = np.convolve(all_returns, np.ones(k)/k, mode='valid')\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(all_returns, label=\"All Returns\", alpha=0.5)\n",
        "    plt.plot(np.arange(k-1, len(all_returns)), running_avg, label=f\"Running Average (k={k})\", color='r', linewidth=2)\n",
        "    plt.title(f\"{fname}\")\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Average Return\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(fname + \".png\")\n",
        "    print(f\"Plot saved as {fname}\\n\")\n",
        "    return all_returns\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--environment\", type=str, default=\"LunarLander-v3\")\n",
        "    parser.add_argument(\"--iterations\", type=int, default=300)\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=8000)\n",
        "    parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    parser.add_argument(\"--gamma\", type=float, default=0.99)\n",
        "    parser.add_argument(\"--reward_to_go\", action=\"store_true\")\n",
        "    parser.add_argument(\"--advantage_norm\", action=\"store_true\")\n",
        "    parser.add_argument(\"--reward_scale\", type=float, default=1.0)\n",
        "    parser.add_argument(\"--reward_clip_min\", type=float, default=None)\n",
        "    parser.add_argument(\"--reward_clip_max\", type=float, default=None)\n",
        "    parser.add_argument(\"--hidden_dim\", type=int, default=128)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    clip_tuple = None\n",
        "    if args.reward_clip_min is not None or args.reward_clip_max is not None:\n",
        "        lo = -np.inf if args.reward_clip_min is None else args.reward_clip_min\n",
        "        hi = np.inf if args.reward_clip_max is None else args.reward_clip_max\n",
        "        clip_tuple = (lo, hi)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    fname = f\"{args.environment}_PG_iters{args.iterations}_bs{args.batch_size}_g_lr_{'rtg' if args.reward_to_go else 'tot'}_{'advnorm' if args.advantage_norm else 'noadv'}_\"\n",
        "    run_training(\n",
        "        env_name=args.environment,\n",
        "        iterations=args.iterations,\n",
        "        batch_size=args.batch_size,\n",
        "        lr=args.lr,\n",
        "        gamma=args.gamma,\n",
        "        reward_to_go=args.reward_to_go,\n",
        "        advantage_norm=args.advantage_norm,\n",
        "        reward_scale=args.reward_scale,\n",
        "        reward_clip=clip_tuple,\n",
        "        hidden_dim=args.hidden_dim,\n",
        "        device=device,\n",
        "        fname=fname\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJlEcsvA2TAP",
        "outputId": "067b4229-a5d3-4bdd-d826-d79d2f5f9fb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment: LunarLander-v3\n",
            "Box([ -2.5        -2.5       -10.        -10.         -6.2831855 -10.\n",
            "  -0.         -0.       ], [ 2.5        2.5       10.        10.         6.2831855 10.\n",
            "  1.         1.       ], (8,), float32)\n",
            "Discrete(4)\n",
            "Device: cpu\n",
            "Policy Params: 18180\n",
            "Iter 1/5 | AvgReturn -205.115 | Loss 1.150\n",
            "Iter 2/5 | AvgReturn -174.408 | Loss -5.880\n",
            "Iter 3/5 | AvgReturn -177.800 | Loss -3.110\n",
            "Iter 4/5 | AvgReturn -176.663 | Loss 12.527\n",
            "Iter 5/5 | AvgReturn -189.604 | Loss -10.015\n",
            "Training finished. Plot saved to LunarLander-v3_PG_iters5_batch40_g_lr_rtg_advnorm_.png\n",
            "Saved plot: LunarLander-v3_PG_iters5_batch40_g_lr_rtg_advnorm_.png\n"
          ]
        }
      ],
      "source": [
        "# !python3 LunarLander-PG-b.py --environment LunarLander-v3 --iterations 5 --batch 40 --gamma 0.99 --learning_rate 5e-4 --reward_to_go --advantage_norm --reward_scale 1.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile LunarLander-PG-c.py\n",
        "\n",
        "import subprocess\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse, os, time, random\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "\n",
        "# Fixed settings\n",
        "environment = \"CartPole-v1\"\n",
        "iterations = 2000\n",
        "learning_rate = 1e-3\n",
        "gamma = 0.99\n",
        "\n",
        "# Batch sizes to compare\n",
        "batch_sizes = [40, 400, 4000]\n",
        "\n",
        "# Store results\n",
        "batch_rewards = {}\n",
        "\n",
        "for b in batch_sizes:\n",
        "    script_path = os.path.join(os.path.dirname(__file__), \"LunarLander-PG-b.py\")\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\n",
        "                sys.executable,\n",
        "                script_path,\n",
        "                \"--environment\", environment,\n",
        "                \"--iterations\", str(iterations),\n",
        "                \"--batch_size\", str(b),\n",
        "                \"--lr\", str(learning_rate),\n",
        "                \"--gamma\", str(gamma),\n",
        "                \"--reward_to_go\",\n",
        "                \"--advantage_norm\"\n",
        "            ],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            check=True,\n",
        "            cwd=os.path.dirname(__file__)\n",
        "        )\n",
        "        # show subprocess output for debugging / parsing\n",
        "        if result.stdout:\n",
        "            print(result.stdout)\n",
        "        if result.stderr:\n",
        "            print(\"Subprocess stderr:\", result.stderr)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(\"Subprocess failed with returncode\", e.returncode)\n",
        "        if e.stdout:\n",
        "            print(e.stdout)\n",
        "        if e.stderr:\n",
        "            print(e.stderr)\n",
        "        continue\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
