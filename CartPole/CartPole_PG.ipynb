{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRNQhyWeyxHb",
        "outputId": "47b94f57-7c44-465f-c9f7-425ce75079a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing CartPole-PG-a.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile CartPole-PG-a.py\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import argparse\n",
        "from itertools import count\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(description=\"Policy Gradient for CartPole-v0/v1\")\n",
        "parser.add_argument(\"--environment\", type=str, required=True,\n",
        "                    help=\"Gym environment name (e.g., CartPole-v1\")\n",
        "parser.add_argument(\"--iterations\", type=int, default=50,\n",
        "                    help=\"Number of iterations (each iteration = batch of episodes)\")\n",
        "parser.add_argument(\"--batch\", type=int, default=32,\n",
        "                    help=\"Batch size (number of episodes per iteration)\")\n",
        "args = parser.parse_args()\n",
        "\n",
        "\n",
        "\n",
        "def plot_rewards(rewards, filename):\n",
        "    \"\"\"Plot the average total rewards per iteration.\"\"\"\n",
        "    plt.figure(figsize=(10, 5), dpi=100)\n",
        "    plt.plot(rewards, label='Average Reward per Iteration')\n",
        "    plt.title(f\"Random Agent Performance on {args.environment}\")\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Average Total Reward\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def get_state(obs):\n",
        "    \"\"\"Convert observation (state) to PyTorch tensor.\"\"\"\n",
        "    state = torch.from_numpy(np.array(obs)).float().unsqueeze(0)\n",
        "    return state\n",
        "\n",
        "\n",
        "def select_random_action(env):\n",
        "    \"\"\"Select a random valid action from the environment.\"\"\"\n",
        "    return env.action_space.sample()\n",
        "\n",
        "\n",
        "\n",
        "def run_random_agent(env, iterations, batch_size):\n",
        "    \"\"\"Run random agent for given environment and collect average rewards.\"\"\"\n",
        "    all_rewards = []\n",
        "\n",
        "    for it in range(iterations):\n",
        "        batch_rewards = []\n",
        "\n",
        "        for ep in range(batch_size):\n",
        "            obs = env.reset()\n",
        "            # Compatible with Gym v0.26+\n",
        "            if isinstance(obs, tuple):\n",
        "                obs = obs[0]\n",
        "\n",
        "            total_reward = 0\n",
        "            for t in count():\n",
        "                action = select_random_action(env)\n",
        "                step_output = env.step(action)\n",
        "\n",
        "                if len(step_output) == 5:\n",
        "                    obs, reward, terminated, truncated, _ = step_output\n",
        "                    done = terminated or truncated\n",
        "                else:\n",
        "                    obs, reward, done, _ = step_output\n",
        "\n",
        "                total_reward += reward\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            batch_rewards.append(total_reward)\n",
        "\n",
        "        avg_reward = np.mean(batch_rewards)\n",
        "        print(f\"Iteration {it+1}/{iterations}: Average Reward = {avg_reward:.2f}\")\n",
        "        all_rewards.append(avg_reward)\n",
        "\n",
        "    env.close()\n",
        "    return all_rewards\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load Gym environment\n",
        "    env = gym.make(args.environment)\n",
        "    print(f\"\\nLoaded Environment: {args.environment}\")\n",
        "    print(f\"Observation space: {env.observation_space}\")\n",
        "    print(f\"Action space: {env.action_space}\\n\")\n",
        "\n",
        "    # Run random agent\n",
        "    rewards = run_random_agent(env, args.iterations, args.batch)\n",
        "\n",
        "    # Save plot\n",
        "    file_name = f\"{args.environment}_{args.iterations}_{args.batch}_random_agent.png\"\n",
        "    plot_rewards(rewards, file_name)\n",
        "    print(f\"Plot saved as {file_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SPe-ToezaNN",
        "outputId": "ab65f2ae-19d0-4bb0-a2a3-c1cb1fb39808"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded Environment: CartPole-v1\n",
            "Observation space: Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
            "Action space: Discrete(2)\n",
            "\n",
            "Iteration 1/500: Average Reward = 20.12\n",
            "Iteration 2/500: Average Reward = 25.00\n",
            "Iteration 3/500: Average Reward = 20.62\n",
            "Iteration 4/500: Average Reward = 21.19\n",
            "Iteration 5/500: Average Reward = 19.56\n",
            "Iteration 6/500: Average Reward = 20.62\n",
            "Iteration 7/500: Average Reward = 26.44\n",
            "Iteration 8/500: Average Reward = 23.38\n",
            "Iteration 9/500: Average Reward = 26.44\n",
            "Iteration 10/500: Average Reward = 29.94\n",
            "Iteration 11/500: Average Reward = 18.44\n",
            "Iteration 12/500: Average Reward = 20.75\n",
            "Iteration 13/500: Average Reward = 23.38\n",
            "Iteration 14/500: Average Reward = 18.94\n",
            "Iteration 15/500: Average Reward = 20.19\n",
            "Iteration 16/500: Average Reward = 20.38\n",
            "Iteration 17/500: Average Reward = 27.50\n",
            "Iteration 18/500: Average Reward = 30.38\n",
            "Iteration 19/500: Average Reward = 25.62\n",
            "Iteration 20/500: Average Reward = 23.06\n",
            "Iteration 21/500: Average Reward = 20.44\n",
            "Iteration 22/500: Average Reward = 22.50\n",
            "Iteration 23/500: Average Reward = 25.75\n",
            "Iteration 24/500: Average Reward = 24.56\n",
            "Iteration 25/500: Average Reward = 19.69\n",
            "Iteration 26/500: Average Reward = 16.94\n",
            "Iteration 27/500: Average Reward = 19.88\n",
            "Iteration 28/500: Average Reward = 26.06\n",
            "Iteration 29/500: Average Reward = 22.12\n",
            "Iteration 30/500: Average Reward = 23.19\n",
            "Iteration 31/500: Average Reward = 22.69\n",
            "Iteration 32/500: Average Reward = 22.69\n",
            "Iteration 33/500: Average Reward = 29.19\n",
            "Iteration 34/500: Average Reward = 24.62\n",
            "Iteration 35/500: Average Reward = 22.00\n",
            "Iteration 36/500: Average Reward = 19.88\n",
            "Iteration 37/500: Average Reward = 22.44\n",
            "Iteration 38/500: Average Reward = 24.06\n",
            "Iteration 39/500: Average Reward = 21.50\n",
            "Iteration 40/500: Average Reward = 22.31\n",
            "Iteration 41/500: Average Reward = 22.62\n",
            "Iteration 42/500: Average Reward = 16.75\n",
            "Iteration 43/500: Average Reward = 19.81\n",
            "Iteration 44/500: Average Reward = 20.94\n",
            "Iteration 45/500: Average Reward = 16.69\n",
            "Iteration 46/500: Average Reward = 26.62\n",
            "Iteration 47/500: Average Reward = 24.94\n",
            "Iteration 48/500: Average Reward = 14.94\n",
            "Iteration 49/500: Average Reward = 20.56\n",
            "Iteration 50/500: Average Reward = 21.44\n",
            "Iteration 51/500: Average Reward = 24.62\n",
            "Iteration 52/500: Average Reward = 26.75\n",
            "Iteration 53/500: Average Reward = 24.88\n",
            "Iteration 54/500: Average Reward = 25.88\n",
            "Iteration 55/500: Average Reward = 22.44\n",
            "Iteration 56/500: Average Reward = 27.06\n",
            "Iteration 57/500: Average Reward = 27.00\n",
            "Iteration 58/500: Average Reward = 21.44\n",
            "Iteration 59/500: Average Reward = 23.38\n",
            "Iteration 60/500: Average Reward = 24.50\n",
            "Iteration 61/500: Average Reward = 20.38\n",
            "Iteration 62/500: Average Reward = 24.12\n",
            "Iteration 63/500: Average Reward = 20.00\n",
            "Iteration 64/500: Average Reward = 19.31\n",
            "Iteration 65/500: Average Reward = 20.12\n",
            "Iteration 66/500: Average Reward = 23.94\n",
            "Iteration 67/500: Average Reward = 19.81\n",
            "Iteration 68/500: Average Reward = 24.69\n",
            "Iteration 69/500: Average Reward = 18.44\n",
            "Iteration 70/500: Average Reward = 28.12\n",
            "Iteration 71/500: Average Reward = 27.94\n",
            "Iteration 72/500: Average Reward = 23.38\n",
            "Iteration 73/500: Average Reward = 24.25\n",
            "Iteration 74/500: Average Reward = 22.62\n",
            "Iteration 75/500: Average Reward = 22.56\n",
            "Iteration 76/500: Average Reward = 24.50\n",
            "Iteration 77/500: Average Reward = 24.25\n",
            "Iteration 78/500: Average Reward = 20.38\n",
            "Iteration 79/500: Average Reward = 23.38\n",
            "Iteration 80/500: Average Reward = 20.75\n",
            "Iteration 81/500: Average Reward = 25.38\n",
            "Iteration 82/500: Average Reward = 25.69\n",
            "Iteration 83/500: Average Reward = 21.44\n",
            "Iteration 84/500: Average Reward = 21.88\n",
            "Iteration 85/500: Average Reward = 21.19\n",
            "Iteration 86/500: Average Reward = 21.38\n",
            "Iteration 87/500: Average Reward = 21.69\n",
            "Iteration 88/500: Average Reward = 24.56\n",
            "Iteration 89/500: Average Reward = 27.44\n",
            "Iteration 90/500: Average Reward = 21.62\n",
            "Iteration 91/500: Average Reward = 17.94\n",
            "Iteration 92/500: Average Reward = 23.44\n",
            "Iteration 93/500: Average Reward = 25.69\n",
            "Iteration 94/500: Average Reward = 25.25\n",
            "Iteration 95/500: Average Reward = 19.31\n",
            "Iteration 96/500: Average Reward = 24.12\n",
            "Iteration 97/500: Average Reward = 18.50\n",
            "Iteration 98/500: Average Reward = 17.75\n",
            "Iteration 99/500: Average Reward = 22.44\n",
            "Iteration 100/500: Average Reward = 22.75\n",
            "Iteration 101/500: Average Reward = 22.38\n",
            "Iteration 102/500: Average Reward = 23.06\n",
            "Iteration 103/500: Average Reward = 26.62\n",
            "Iteration 104/500: Average Reward = 20.06\n",
            "Iteration 105/500: Average Reward = 22.31\n",
            "Iteration 106/500: Average Reward = 23.12\n",
            "Iteration 107/500: Average Reward = 19.25\n",
            "Iteration 108/500: Average Reward = 23.38\n",
            "Iteration 109/500: Average Reward = 23.12\n",
            "Iteration 110/500: Average Reward = 19.50\n",
            "Iteration 111/500: Average Reward = 30.38\n",
            "Iteration 112/500: Average Reward = 28.12\n",
            "Iteration 113/500: Average Reward = 24.06\n",
            "Iteration 114/500: Average Reward = 25.50\n",
            "Iteration 115/500: Average Reward = 20.38\n",
            "Iteration 116/500: Average Reward = 26.50\n",
            "Iteration 117/500: Average Reward = 20.81\n",
            "Iteration 118/500: Average Reward = 26.50\n",
            "Iteration 119/500: Average Reward = 24.62\n",
            "Iteration 120/500: Average Reward = 23.12\n",
            "Iteration 121/500: Average Reward = 20.38\n",
            "Iteration 122/500: Average Reward = 24.69\n",
            "Iteration 123/500: Average Reward = 22.31\n",
            "Iteration 124/500: Average Reward = 22.81\n",
            "Iteration 125/500: Average Reward = 22.56\n",
            "Iteration 126/500: Average Reward = 27.69\n",
            "Iteration 127/500: Average Reward = 26.88\n",
            "Iteration 128/500: Average Reward = 23.31\n",
            "Iteration 129/500: Average Reward = 18.06\n",
            "Iteration 130/500: Average Reward = 24.25\n",
            "Iteration 131/500: Average Reward = 20.62\n",
            "Iteration 132/500: Average Reward = 25.12\n",
            "Iteration 133/500: Average Reward = 26.56\n",
            "Iteration 134/500: Average Reward = 29.44\n",
            "Iteration 135/500: Average Reward = 18.75\n",
            "Iteration 136/500: Average Reward = 27.25\n",
            "Iteration 137/500: Average Reward = 20.31\n",
            "Iteration 138/500: Average Reward = 23.25\n",
            "Iteration 139/500: Average Reward = 23.44\n",
            "Iteration 140/500: Average Reward = 28.44\n",
            "Iteration 141/500: Average Reward = 21.69\n",
            "Iteration 142/500: Average Reward = 23.75\n",
            "Iteration 143/500: Average Reward = 21.88\n",
            "Iteration 144/500: Average Reward = 19.75\n",
            "Iteration 145/500: Average Reward = 22.12\n",
            "Iteration 146/500: Average Reward = 20.12\n",
            "Iteration 147/500: Average Reward = 21.38\n",
            "Iteration 148/500: Average Reward = 19.62\n",
            "Iteration 149/500: Average Reward = 24.38\n",
            "Iteration 150/500: Average Reward = 21.12\n",
            "Iteration 151/500: Average Reward = 22.62\n",
            "Iteration 152/500: Average Reward = 17.12\n",
            "Iteration 153/500: Average Reward = 20.38\n",
            "Iteration 154/500: Average Reward = 18.50\n",
            "Iteration 155/500: Average Reward = 19.94\n",
            "Iteration 156/500: Average Reward = 14.44\n",
            "Iteration 157/500: Average Reward = 23.06\n",
            "Iteration 158/500: Average Reward = 20.88\n",
            "Iteration 159/500: Average Reward = 22.00\n",
            "Iteration 160/500: Average Reward = 25.44\n",
            "Iteration 161/500: Average Reward = 19.69\n",
            "Iteration 162/500: Average Reward = 24.12\n",
            "Iteration 163/500: Average Reward = 19.25\n",
            "Iteration 164/500: Average Reward = 20.44\n",
            "Iteration 165/500: Average Reward = 18.50\n",
            "Iteration 166/500: Average Reward = 21.69\n",
            "Iteration 167/500: Average Reward = 26.25\n",
            "Iteration 168/500: Average Reward = 24.50\n",
            "Iteration 169/500: Average Reward = 21.44\n",
            "Iteration 170/500: Average Reward = 23.19\n",
            "Iteration 171/500: Average Reward = 24.62\n",
            "Iteration 172/500: Average Reward = 21.62\n",
            "Iteration 173/500: Average Reward = 25.94\n",
            "Iteration 174/500: Average Reward = 19.69\n",
            "Iteration 175/500: Average Reward = 30.25\n",
            "Iteration 176/500: Average Reward = 20.19\n",
            "Iteration 177/500: Average Reward = 21.69\n",
            "Iteration 178/500: Average Reward = 29.00\n",
            "Iteration 179/500: Average Reward = 27.56\n",
            "Iteration 180/500: Average Reward = 27.50\n",
            "Iteration 181/500: Average Reward = 20.69\n",
            "Iteration 182/500: Average Reward = 19.69\n",
            "Iteration 183/500: Average Reward = 24.06\n",
            "Iteration 184/500: Average Reward = 17.81\n",
            "Iteration 185/500: Average Reward = 24.56\n",
            "Iteration 186/500: Average Reward = 20.69\n",
            "Iteration 187/500: Average Reward = 22.06\n",
            "Iteration 188/500: Average Reward = 18.88\n",
            "Iteration 189/500: Average Reward = 23.44\n",
            "Iteration 190/500: Average Reward = 29.69\n",
            "Iteration 191/500: Average Reward = 20.12\n",
            "Iteration 192/500: Average Reward = 23.88\n",
            "Iteration 193/500: Average Reward = 21.56\n",
            "Iteration 194/500: Average Reward = 23.06\n",
            "Iteration 195/500: Average Reward = 32.00\n",
            "Iteration 196/500: Average Reward = 20.75\n",
            "Iteration 197/500: Average Reward = 21.44\n",
            "Iteration 198/500: Average Reward = 23.94\n",
            "Iteration 199/500: Average Reward = 19.19\n",
            "Iteration 200/500: Average Reward = 19.38\n",
            "Iteration 201/500: Average Reward = 22.19\n",
            "Iteration 202/500: Average Reward = 22.38\n",
            "Iteration 203/500: Average Reward = 28.06\n",
            "Iteration 204/500: Average Reward = 20.31\n",
            "Iteration 205/500: Average Reward = 20.69\n",
            "Iteration 206/500: Average Reward = 19.19\n",
            "Iteration 207/500: Average Reward = 25.81\n",
            "Iteration 208/500: Average Reward = 32.81\n",
            "Iteration 209/500: Average Reward = 26.06\n",
            "Iteration 210/500: Average Reward = 19.75\n",
            "Iteration 211/500: Average Reward = 18.31\n",
            "Iteration 212/500: Average Reward = 24.31\n",
            "Iteration 213/500: Average Reward = 17.88\n",
            "Iteration 214/500: Average Reward = 26.94\n",
            "Iteration 215/500: Average Reward = 26.62\n",
            "Iteration 216/500: Average Reward = 28.19\n",
            "Iteration 217/500: Average Reward = 22.56\n",
            "Iteration 218/500: Average Reward = 21.56\n",
            "Iteration 219/500: Average Reward = 22.44\n",
            "Iteration 220/500: Average Reward = 23.62\n",
            "Iteration 221/500: Average Reward = 23.94\n",
            "Iteration 222/500: Average Reward = 21.25\n",
            "Iteration 223/500: Average Reward = 22.81\n",
            "Iteration 224/500: Average Reward = 24.50\n",
            "Iteration 225/500: Average Reward = 22.50\n",
            "Iteration 226/500: Average Reward = 22.94\n",
            "Iteration 227/500: Average Reward = 18.69\n",
            "Iteration 228/500: Average Reward = 17.69\n",
            "Iteration 229/500: Average Reward = 21.50\n",
            "Iteration 230/500: Average Reward = 23.00\n",
            "Iteration 231/500: Average Reward = 22.75\n",
            "Iteration 232/500: Average Reward = 15.69\n",
            "Iteration 233/500: Average Reward = 20.81\n",
            "Iteration 234/500: Average Reward = 22.69\n",
            "Iteration 235/500: Average Reward = 21.88\n",
            "Iteration 236/500: Average Reward = 25.44\n",
            "Iteration 237/500: Average Reward = 23.81\n",
            "Iteration 238/500: Average Reward = 20.81\n",
            "Iteration 239/500: Average Reward = 22.81\n",
            "Iteration 240/500: Average Reward = 19.44\n",
            "Iteration 241/500: Average Reward = 22.44\n",
            "Iteration 242/500: Average Reward = 20.00\n",
            "Iteration 243/500: Average Reward = 27.94\n",
            "Iteration 244/500: Average Reward = 28.38\n",
            "Iteration 245/500: Average Reward = 24.19\n",
            "Iteration 246/500: Average Reward = 20.88\n",
            "Iteration 247/500: Average Reward = 19.50\n",
            "Iteration 248/500: Average Reward = 24.44\n",
            "Iteration 249/500: Average Reward = 22.81\n",
            "Iteration 250/500: Average Reward = 18.38\n",
            "Iteration 251/500: Average Reward = 21.50\n",
            "Iteration 252/500: Average Reward = 29.44\n",
            "Iteration 253/500: Average Reward = 18.94\n",
            "Iteration 254/500: Average Reward = 17.56\n",
            "Iteration 255/500: Average Reward = 22.56\n",
            "Iteration 256/500: Average Reward = 20.56\n",
            "Iteration 257/500: Average Reward = 19.44\n",
            "Iteration 258/500: Average Reward = 19.94\n",
            "Iteration 259/500: Average Reward = 19.06\n",
            "Iteration 260/500: Average Reward = 22.50\n",
            "Iteration 261/500: Average Reward = 20.38\n",
            "Iteration 262/500: Average Reward = 21.75\n",
            "Iteration 263/500: Average Reward = 24.19\n",
            "Iteration 264/500: Average Reward = 23.81\n",
            "Iteration 265/500: Average Reward = 20.94\n",
            "Iteration 266/500: Average Reward = 18.25\n",
            "Iteration 267/500: Average Reward = 20.88\n",
            "Iteration 268/500: Average Reward = 26.25\n",
            "Iteration 269/500: Average Reward = 23.31\n",
            "Iteration 270/500: Average Reward = 23.50\n",
            "Iteration 271/500: Average Reward = 19.44\n",
            "Iteration 272/500: Average Reward = 24.88\n",
            "Iteration 273/500: Average Reward = 18.06\n",
            "Iteration 274/500: Average Reward = 20.31\n",
            "Iteration 275/500: Average Reward = 27.69\n",
            "Iteration 276/500: Average Reward = 24.50\n",
            "Iteration 277/500: Average Reward = 26.69\n",
            "Iteration 278/500: Average Reward = 25.38\n",
            "Iteration 279/500: Average Reward = 19.12\n",
            "Iteration 280/500: Average Reward = 16.50\n",
            "Iteration 281/500: Average Reward = 21.94\n",
            "Iteration 282/500: Average Reward = 20.19\n",
            "Iteration 283/500: Average Reward = 19.25\n",
            "Iteration 284/500: Average Reward = 21.75\n",
            "Iteration 285/500: Average Reward = 22.75\n",
            "Iteration 286/500: Average Reward = 21.25\n",
            "Iteration 287/500: Average Reward = 23.50\n",
            "Iteration 288/500: Average Reward = 19.56\n",
            "Iteration 289/500: Average Reward = 25.31\n",
            "Iteration 290/500: Average Reward = 22.94\n",
            "Iteration 291/500: Average Reward = 19.00\n",
            "Iteration 292/500: Average Reward = 24.38\n",
            "Iteration 293/500: Average Reward = 27.25\n",
            "Iteration 294/500: Average Reward = 17.69\n",
            "Iteration 295/500: Average Reward = 26.88\n",
            "Iteration 296/500: Average Reward = 25.62\n",
            "Iteration 297/500: Average Reward = 21.12\n",
            "Iteration 298/500: Average Reward = 23.38\n",
            "Iteration 299/500: Average Reward = 20.81\n",
            "Iteration 300/500: Average Reward = 21.00\n",
            "Iteration 301/500: Average Reward = 21.25\n",
            "Iteration 302/500: Average Reward = 29.00\n",
            "Iteration 303/500: Average Reward = 22.69\n",
            "Iteration 304/500: Average Reward = 23.56\n",
            "Iteration 305/500: Average Reward = 21.44\n",
            "Iteration 306/500: Average Reward = 22.38\n",
            "Iteration 307/500: Average Reward = 22.56\n",
            "Iteration 308/500: Average Reward = 25.69\n",
            "Iteration 309/500: Average Reward = 20.44\n",
            "Iteration 310/500: Average Reward = 22.00\n",
            "Iteration 311/500: Average Reward = 21.31\n",
            "Iteration 312/500: Average Reward = 21.00\n",
            "Iteration 313/500: Average Reward = 16.38\n",
            "Iteration 314/500: Average Reward = 23.94\n",
            "Iteration 315/500: Average Reward = 25.81\n",
            "Iteration 316/500: Average Reward = 26.81\n",
            "Iteration 317/500: Average Reward = 23.00\n",
            "Iteration 318/500: Average Reward = 20.19\n",
            "Iteration 319/500: Average Reward = 21.44\n",
            "Iteration 320/500: Average Reward = 20.69\n",
            "Iteration 321/500: Average Reward = 23.31\n",
            "Iteration 322/500: Average Reward = 19.69\n",
            "Iteration 323/500: Average Reward = 27.69\n",
            "Iteration 324/500: Average Reward = 21.00\n",
            "Iteration 325/500: Average Reward = 28.62\n",
            "Iteration 326/500: Average Reward = 19.38\n",
            "Iteration 327/500: Average Reward = 21.94\n",
            "Iteration 328/500: Average Reward = 24.44\n",
            "Iteration 329/500: Average Reward = 21.44\n",
            "Iteration 330/500: Average Reward = 24.00\n",
            "Iteration 331/500: Average Reward = 20.12\n",
            "Iteration 332/500: Average Reward = 24.62\n",
            "Iteration 333/500: Average Reward = 21.88\n",
            "Iteration 334/500: Average Reward = 20.44\n",
            "Iteration 335/500: Average Reward = 26.06\n",
            "Iteration 336/500: Average Reward = 24.19\n",
            "Iteration 337/500: Average Reward = 20.81\n",
            "Iteration 338/500: Average Reward = 21.06\n",
            "Iteration 339/500: Average Reward = 21.94\n",
            "Iteration 340/500: Average Reward = 19.06\n",
            "Iteration 341/500: Average Reward = 24.31\n",
            "Iteration 342/500: Average Reward = 22.19\n",
            "Iteration 343/500: Average Reward = 25.75\n",
            "Iteration 344/500: Average Reward = 20.38\n",
            "Iteration 345/500: Average Reward = 20.62\n",
            "Iteration 346/500: Average Reward = 20.88\n",
            "Iteration 347/500: Average Reward = 17.88\n",
            "Iteration 348/500: Average Reward = 22.69\n",
            "Iteration 349/500: Average Reward = 20.50\n",
            "Iteration 350/500: Average Reward = 17.81\n",
            "Iteration 351/500: Average Reward = 20.31\n",
            "Iteration 352/500: Average Reward = 23.94\n",
            "Iteration 353/500: Average Reward = 26.94\n",
            "Iteration 354/500: Average Reward = 20.62\n",
            "Iteration 355/500: Average Reward = 26.56\n",
            "Iteration 356/500: Average Reward = 24.00\n",
            "Iteration 357/500: Average Reward = 21.81\n",
            "Iteration 358/500: Average Reward = 21.75\n",
            "Iteration 359/500: Average Reward = 21.38\n",
            "Iteration 360/500: Average Reward = 23.25\n",
            "Iteration 361/500: Average Reward = 18.00\n",
            "Iteration 362/500: Average Reward = 23.81\n",
            "Iteration 363/500: Average Reward = 25.56\n",
            "Iteration 364/500: Average Reward = 19.00\n",
            "Iteration 365/500: Average Reward = 22.25\n",
            "Iteration 366/500: Average Reward = 20.19\n",
            "Iteration 367/500: Average Reward = 23.81\n",
            "Iteration 368/500: Average Reward = 22.00\n",
            "Iteration 369/500: Average Reward = 24.25\n",
            "Iteration 370/500: Average Reward = 21.88\n",
            "Iteration 371/500: Average Reward = 19.31\n",
            "Iteration 372/500: Average Reward = 22.62\n",
            "Iteration 373/500: Average Reward = 25.25\n",
            "Iteration 374/500: Average Reward = 20.06\n",
            "Iteration 375/500: Average Reward = 22.12\n",
            "Iteration 376/500: Average Reward = 22.31\n",
            "Iteration 377/500: Average Reward = 25.44\n",
            "Iteration 378/500: Average Reward = 23.75\n",
            "Iteration 379/500: Average Reward = 24.88\n",
            "Iteration 380/500: Average Reward = 28.75\n",
            "Iteration 381/500: Average Reward = 24.81\n",
            "Iteration 382/500: Average Reward = 18.19\n",
            "Iteration 383/500: Average Reward = 19.81\n",
            "Iteration 384/500: Average Reward = 22.06\n",
            "Iteration 385/500: Average Reward = 23.06\n",
            "Iteration 386/500: Average Reward = 21.88\n",
            "Iteration 387/500: Average Reward = 24.44\n",
            "Iteration 388/500: Average Reward = 22.69\n",
            "Iteration 389/500: Average Reward = 24.44\n",
            "Iteration 390/500: Average Reward = 22.44\n",
            "Iteration 391/500: Average Reward = 24.75\n",
            "Iteration 392/500: Average Reward = 31.44\n",
            "Iteration 393/500: Average Reward = 26.00\n",
            "Iteration 394/500: Average Reward = 31.69\n",
            "Iteration 395/500: Average Reward = 19.19\n",
            "Iteration 396/500: Average Reward = 17.75\n",
            "Iteration 397/500: Average Reward = 22.25\n",
            "Iteration 398/500: Average Reward = 26.88\n",
            "Iteration 399/500: Average Reward = 20.31\n",
            "Iteration 400/500: Average Reward = 22.38\n",
            "Iteration 401/500: Average Reward = 22.75\n",
            "Iteration 402/500: Average Reward = 21.44\n",
            "Iteration 403/500: Average Reward = 21.56\n",
            "Iteration 404/500: Average Reward = 20.06\n",
            "Iteration 405/500: Average Reward = 23.38\n",
            "Iteration 406/500: Average Reward = 20.69\n",
            "Iteration 407/500: Average Reward = 26.62\n",
            "Iteration 408/500: Average Reward = 24.75\n",
            "Iteration 409/500: Average Reward = 27.50\n",
            "Iteration 410/500: Average Reward = 22.88\n",
            "Iteration 411/500: Average Reward = 22.38\n",
            "Iteration 412/500: Average Reward = 28.88\n",
            "Iteration 413/500: Average Reward = 21.56\n",
            "Iteration 414/500: Average Reward = 21.44\n",
            "Iteration 415/500: Average Reward = 20.56\n",
            "Iteration 416/500: Average Reward = 18.56\n",
            "Iteration 417/500: Average Reward = 22.50\n",
            "Iteration 418/500: Average Reward = 24.62\n",
            "Iteration 419/500: Average Reward = 19.25\n",
            "Iteration 420/500: Average Reward = 19.88\n",
            "Iteration 421/500: Average Reward = 20.69\n",
            "Iteration 422/500: Average Reward = 21.94\n",
            "Iteration 423/500: Average Reward = 20.94\n",
            "Iteration 424/500: Average Reward = 19.31\n",
            "Iteration 425/500: Average Reward = 27.81\n",
            "Iteration 426/500: Average Reward = 14.75\n",
            "Iteration 427/500: Average Reward = 20.44\n",
            "Iteration 428/500: Average Reward = 18.81\n",
            "Iteration 429/500: Average Reward = 21.19\n",
            "Iteration 430/500: Average Reward = 22.56\n",
            "Iteration 431/500: Average Reward = 19.38\n",
            "Iteration 432/500: Average Reward = 23.75\n",
            "Iteration 433/500: Average Reward = 23.06\n",
            "Iteration 434/500: Average Reward = 22.25\n",
            "Iteration 435/500: Average Reward = 21.31\n",
            "Iteration 436/500: Average Reward = 25.25\n",
            "Iteration 437/500: Average Reward = 19.75\n",
            "Iteration 438/500: Average Reward = 25.00\n",
            "Iteration 439/500: Average Reward = 22.06\n",
            "Iteration 440/500: Average Reward = 21.75\n",
            "Iteration 441/500: Average Reward = 21.25\n",
            "Iteration 442/500: Average Reward = 29.75\n",
            "Iteration 443/500: Average Reward = 25.00\n",
            "Iteration 444/500: Average Reward = 21.94\n",
            "Iteration 445/500: Average Reward = 21.12\n",
            "Iteration 446/500: Average Reward = 19.69\n",
            "Iteration 447/500: Average Reward = 20.56\n",
            "Iteration 448/500: Average Reward = 24.25\n",
            "Iteration 449/500: Average Reward = 26.62\n",
            "Iteration 450/500: Average Reward = 21.81\n",
            "Iteration 451/500: Average Reward = 22.19\n",
            "Iteration 452/500: Average Reward = 25.75\n",
            "Iteration 453/500: Average Reward = 27.06\n",
            "Iteration 454/500: Average Reward = 18.38\n",
            "Iteration 455/500: Average Reward = 19.19\n",
            "Iteration 456/500: Average Reward = 21.31\n",
            "Iteration 457/500: Average Reward = 25.69\n",
            "Iteration 458/500: Average Reward = 24.81\n",
            "Iteration 459/500: Average Reward = 21.06\n",
            "Iteration 460/500: Average Reward = 18.56\n",
            "Iteration 461/500: Average Reward = 20.94\n",
            "Iteration 462/500: Average Reward = 25.44\n",
            "Iteration 463/500: Average Reward = 19.81\n",
            "Iteration 464/500: Average Reward = 21.31\n",
            "Iteration 465/500: Average Reward = 21.31\n",
            "Iteration 466/500: Average Reward = 17.88\n",
            "Iteration 467/500: Average Reward = 24.00\n",
            "Iteration 468/500: Average Reward = 24.44\n",
            "Iteration 469/500: Average Reward = 21.25\n",
            "Iteration 470/500: Average Reward = 24.06\n",
            "Iteration 471/500: Average Reward = 19.38\n",
            "Iteration 472/500: Average Reward = 22.81\n",
            "Iteration 473/500: Average Reward = 23.69\n",
            "Iteration 474/500: Average Reward = 17.44\n",
            "Iteration 475/500: Average Reward = 20.25\n",
            "Iteration 476/500: Average Reward = 21.56\n",
            "Iteration 477/500: Average Reward = 24.81\n",
            "Iteration 478/500: Average Reward = 25.00\n",
            "Iteration 479/500: Average Reward = 24.00\n",
            "Iteration 480/500: Average Reward = 21.38\n",
            "Iteration 481/500: Average Reward = 26.31\n",
            "Iteration 482/500: Average Reward = 20.44\n",
            "Iteration 483/500: Average Reward = 24.06\n",
            "Iteration 484/500: Average Reward = 17.00\n",
            "Iteration 485/500: Average Reward = 24.62\n",
            "Iteration 486/500: Average Reward = 21.31\n",
            "Iteration 487/500: Average Reward = 26.31\n",
            "Iteration 488/500: Average Reward = 21.69\n",
            "Iteration 489/500: Average Reward = 21.44\n",
            "Iteration 490/500: Average Reward = 23.69\n",
            "Iteration 491/500: Average Reward = 22.69\n",
            "Iteration 492/500: Average Reward = 28.12\n",
            "Iteration 493/500: Average Reward = 20.88\n",
            "Iteration 494/500: Average Reward = 16.50\n",
            "Iteration 495/500: Average Reward = 25.88\n",
            "Iteration 496/500: Average Reward = 27.00\n",
            "Iteration 497/500: Average Reward = 22.81\n",
            "Iteration 498/500: Average Reward = 27.31\n",
            "Iteration 499/500: Average Reward = 25.19\n",
            "Iteration 500/500: Average Reward = 20.62\n",
            "Plot saved as CartPole-v1_500_16_random_agent.png\n"
          ]
        }
      ],
      "source": [
        "# !python3 CartPole-PG-a.py --environment CartPole-v1 --iterations 500 --batch 16\n",
        "# run the python CMD via CLI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5k2TYtCzoN4",
        "outputId": "ae9a5e16-ca73-4e42-9696-9cbc4c7a8efa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting CartPole-PG-b.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile CartPole-PG-b.py\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "\n",
        "\n",
        "class Policy(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
        "        super(Policy, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, action_dim)\n",
        "        )\n",
        "    \n",
        "        for layer in self.network:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                nn.init.orthogonal_(layer.weight, gain=np.sqrt(2))\n",
        "                nn.init.constant_(layer.bias, 0.0)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "def compute_returns(rewards, gamma, reward_to_go=False):\n",
        "    \"\"\"Compute discounted returns.\"\"\"\n",
        "    if reward_to_go:\n",
        "        returns = []\n",
        "        future_return = 0\n",
        "        for r in reversed(rewards):\n",
        "            future_return = r + gamma * future_return\n",
        "            returns.insert(0, future_return)\n",
        "        return returns\n",
        "    else:\n",
        "        total_return = sum([gamma**i * r for i, r in enumerate(rewards)])\n",
        "        return [total_return] * len(rewards)\n",
        "\n",
        "def plot_rewards(all_mean_rewards, fname):\n",
        "    \n",
        "    k = 50\n",
        "    running_avg = np.convolve(all_mean_rewards, np.ones(k)/k, mode='valid')\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(all_mean_rewards, label=\"All Returns\", alpha=0.5)\n",
        "    plt.plot(np.arange(k-1, len(all_mean_rewards)), running_avg, label=f\"Running Average (k={k})\", color='r', linewidth=2)\n",
        "    plt.title(f\"{fname}\")\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Average Return\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(fname + \".png\")\n",
        "    print(f\"Plot saved as {fname}.png\\n\")\n",
        "    plt.close()\n",
        "\n",
        "def run_training(env_name=\"CartPole-v1\", iterations=100, batch_size=2000,lr=1e-2, gamma=0.99, reward_to_go=True, advantage_norm=True,hidden_dim=128,):\n",
        "    \n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"PG training on {env_name} | reward_to_go={reward_to_go} advantage_norm={advantage_norm}  | device = {device}\")\n",
        "    env = gym.make(env_name)\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    n_actions = env.action_space.n\n",
        "    \n",
        "    policy = Policy(state_dim, n_actions, hidden_dim).to(device)\n",
        "    optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
        "\n",
        "    all_mean_rewards = []\n",
        "\n",
        "    for it in range(iterations):\n",
        "        batch_states, batch_actions, batch_weights, batch_episode_rewards = [], [], [], []\n",
        "        steps = 0\n",
        "\n",
        "        # collect trajectories until we reach desired batch size\n",
        "        while steps < batch_size:\n",
        "            obs, info = env.reset()\n",
        "            done = False\n",
        "            states, actions, rewards = [], [], []\n",
        "            while True:\n",
        "                s_t = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "                logits = policy(s_t)\n",
        "                dist = torch.distributions.Categorical(logits=logits)\n",
        "                action = int(dist.sample().item())\n",
        "\n",
        "                next_obs, reward, terminated, truncated, info = env.step(action)\n",
        "                done_flag = terminated or truncated\n",
        "\n",
        "                states.append(obs)\n",
        "                actions.append(action)\n",
        "                rewards.append(reward)\n",
        "                obs = next_obs\n",
        "\n",
        "                if done_flag:\n",
        "                    break\n",
        "\n",
        "            steps += len(states)\n",
        "            batch_states += states\n",
        "            batch_actions += actions\n",
        "            batch_episode_rewards.append(sum(rewards))\n",
        "            batch_weights += list(compute_returns(rewards, gamma, reward_to_go))\n",
        "\n",
        "        # prepare tensors\n",
        "        batch_states_t = torch.tensor(np.array(batch_states), dtype=torch.float32, device=device)\n",
        "        batch_actions_t = torch.tensor(batch_actions, dtype=torch.int64, device=device)\n",
        "        batch_weights_t = torch.tensor(batch_weights, dtype=torch.float32, device=device)\n",
        "\n",
        "        # advantage normalization\n",
        "        if advantage_norm:\n",
        "            mean = batch_weights_t.mean()\n",
        "            std = batch_weights_t.std() + 1e-8\n",
        "            batch_weights_t = (batch_weights_t - mean) / std\n",
        "\n",
        "        logits = policy(batch_states_t)\n",
        "        dists = torch.distributions.Categorical(logits=logits)\n",
        "        logp = dists.log_prob(batch_actions_t)\n",
        "        loss = -(logp * batch_weights_t).mean()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        mean_reward = np.mean(batch_episode_rewards)\n",
        "        all_mean_rewards.append(mean_reward)\n",
        "        print(f\"Iteration {it+1}/{iterations} | Mean Reward: {mean_reward:.2f} | Episodes: {len(batch_episode_rewards)}\")\n",
        "\n",
        "    return all_mean_rewards\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":    \n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--environment\", type=str, default=\"CartPole-v1\")\n",
        "    parser.add_argument(\"--iterations\", type=int, default=300)\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=8000)\n",
        "    parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    parser.add_argument(\"--gamma\", type=float, default=0.99)\n",
        "    parser.add_argument(\"--reward_to_go\", action=\"store_true\")\n",
        "    parser.add_argument(\"--advantage_norm\", action=\"store_true\")\n",
        "    parser.add_argument(\"--reward_scale\", type=float, default=1.0)\n",
        "    parser.add_argument(\"--reward_clip_min\", type=float, default=None)\n",
        "    parser.add_argument(\"--reward_clip_max\", type=float, default=None)\n",
        "    parser.add_argument(\"--hidden_dim\", type=int, default=128)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    all_mean_rewards = run_training(\n",
        "        env_name=args.environment,\n",
        "        iterations=args.iterations,\n",
        "        batch_size=args.batch_size,\n",
        "        lr=args.lr,\n",
        "        gamma=args.gamma,\n",
        "        reward_to_go=args.reward_to_go,\n",
        "        advantage_norm=args.advantage_norm,\n",
        "        hidden_dim=args.hidden_dim\n",
        "    )\n",
        "\n",
        "    fname = f\"{args.environment}_PG_iters{args.iterations}_bs{args.batch_size}_g_lr_{'rtg' if args.reward_to_go else 'tot'}_{'advnorm' if args.advantage_norm else 'noadv'}_\"\n",
        "    plot_rewards(all_mean_rewards, fname)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJlEcsvA2TAP",
        "outputId": "efe9acd7-0592-40ad-91c0-c2060cdb685f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment: CartPole-v1\n",
            "Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
            "Discrete(2)\n",
            "Device: cpu\n",
            "Policy Params: 17410\n",
            "Iter 1/50 | AvgReturn 22.000 | Loss -3.502\n",
            "Iter 2/50 | AvgReturn 22.031 | Loss -4.476\n",
            "Iter 3/50 | AvgReturn 27.250 | Loss -9.461\n",
            "Iter 4/50 | AvgReturn 26.906 | Loss -7.115\n",
            "Iter 5/50 | AvgReturn 26.844 | Loss -14.143\n",
            "Iter 10/50 | AvgReturn 40.250 | Loss -19.295\n",
            "Iter 15/50 | AvgReturn 45.156 | Loss -28.071\n",
            "Iter 20/50 | AvgReturn 71.375 | Loss -19.352\n",
            "Iter 25/50 | AvgReturn 126.750 | Loss -52.173\n",
            "Iter 30/50 | AvgReturn 179.188 | Loss -111.890\n",
            "Iter 35/50 | AvgReturn 254.062 | Loss -22.117\n",
            "Iter 40/50 | AvgReturn 324.875 | Loss -81.378\n",
            "Iter 45/50 | AvgReturn 376.438 | Loss -12.135\n",
            "Iter 50/50 | AvgReturn 421.000 | Loss -33.217\n",
            "Training finished. Plot saved to CartPole-v1_PG_iters50_batch32_g0.99_lr0.001_rtg_advnorm_20251028-183942.png\n",
            "Saved plot: CartPole-v1_PG_iters50_batch32_g0.99_lr0.001_rtg_advnorm_20251028-183942.png\n"
          ]
        }
      ],
      "source": [
        "# !python3 CartPole-PG-b.py --environment CartPole-v1 --iterations 50 --batch 32 --gamma 0.99 --learning_rate 1e-3 --reward_to_go --advantage_norm\n",
        "# run the python CMD via CLI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile CartPole-PG-c.py\n",
        "\n",
        "import subprocess\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Fixed settings\n",
        "environment = \"CartPole-v1\"\n",
        "iterations = 500\n",
        "learning_rate = 1e-3\n",
        "gamma = 0.99\n",
        "\n",
        "# Batch sizes to compare\n",
        "batch_sizes = [50, 500, 5000]\n",
        "\n",
        "# Store results\n",
        "batch_rewards = {}\n",
        "\n",
        "for b in batch_sizes:\n",
        "    script_path = os.path.join(os.path.dirname(__file__), \"CartPole-PG-b.py\")\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\n",
        "                sys.executable,\n",
        "                script_path,\n",
        "                \"--environment\", environment,\n",
        "                \"--iterations\", str(iterations),\n",
        "                \"--batch_size\", str(b),\n",
        "                \"--lr\", str(learning_rate),\n",
        "                \"--gamma\", str(gamma),\n",
        "                \"--reward_to_go\",\n",
        "                \"--advantage_norm\"\n",
        "            ],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            check=True,\n",
        "            cwd=os.path.dirname(__file__)\n",
        "        )\n",
        "        # show subprocess output for debugging / parsing\n",
        "        if result.stdout:\n",
        "            print(result.stdout)\n",
        "        if result.stderr:\n",
        "            print(\"Subprocess stderr:\", result.stderr)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(\"Subprocess failed with returncode\", e.returncode)\n",
        "        if e.stdout:\n",
        "            print(e.stdout)\n",
        "        if e.stderr:\n",
        "            print(e.stderr)\n",
        "        continue\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# run the python CMD via CLI\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part(a)\n",
        "\n",
        "A random agent in the CartPole-v1 Gym environment receives consistently low and volatile rewards, usually between 15 and 28 per episode across many iterations, means the agent cannot learn or exploit the environment, leading to frequent failure in balancing the pole. Occasional higher rewards occur only by chance.\n",
        "\n",
        "---\n",
        "\n",
        "![Random Agent Performance](part(a)/CartPole-v1_500_16_random_agent.png)\n",
        "<center>Figure-1: Rewards per episode for a random agent in CartPole-v1 environment over 500\n",
        "</center>\n",
        "\n",
        "---\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part(b)\n",
        "\n",
        "- Advantage Normalization speeds up learning and makes it more stable, helping the model hit near-optimal performance pretty quickly.\n",
        "\n",
        "- Reward-to-Go boosts performance too, but without advantage normalization, things get shaky, with some drops due to high variance.\n",
        "\n",
        "- No Reward-to-Go leads to a bumpy ride, with performance dropping a lot, showing that it's less efficient and struggles with variance.\n",
        "\n",
        "- Combining Both gives the best results, with faster learning, less fluctuation, and overall smoother training.\n",
        "\n",
        "- Advantage Normalization on its own is the best for stabilizing things, especially in noisy environments like CartPole-v1.\n",
        "\n",
        "- Without Either, you get a lot of instability, with performance going up and down, and its hard to maintain high returns.\n",
        "\n",
        "---\n",
        "![rTaT](part(b)/CartPole-v1_PG_iters500_bs5000_g_lr_rtg_advnorm_.png)\n",
        "![rTaF](part(b)/CartPole-v1_PG_iters500_bs5000_g_lr_rtg_noadv_.png)\n",
        "![rFaT](part(b)/CartPole-v1_PG_iters500_bs5000_g_lr_tot_advnorm_.png)\n",
        "![rFaF](part(b)/CartPole-v1_PG_iters500_bs5000_g_lr_tot_noadv_.png)\n",
        "<center>Figure-2,3,4,5: Average Return over 500 episodes for 5000 as batch size with different combinations of Reward-to-Go and Advantage Normalization\n",
        "</center>\n",
        "\n",
        "---\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part(c)\n",
        "\n",
        "- Small Batch (bs=50) is super noisy-lots of ups and downs, takes longer to stabilize, and even after a lot of training, the agent can still mess up.\n",
        "\n",
        "- Medium Batch (bs=500) smooths things out a bit-faster and more reliable learning, but still some random dips later on.\n",
        "\n",
        "- Large Batch (bs=5000) is the smoothest-consistent, stable, and hits optimal performance without big drops.\n",
        "\n",
        "- Smaller batches update faster but with higher variance, making it more unstable in the long run time.\n",
        "\n",
        "- Larger batches cut down on gradient noise, making the learning process steadier and more reliable.\n",
        "\n",
        "- Big trade-off: small batches are faster but unstable, while large batches are slower but way more consistent.\n",
        "\n",
        "---\n",
        "\n",
        "![bs50](part(c)/CartPole-v1_PG_iters500_bs50_g_lr_rtg_advnorm_.png)\n",
        "![bs500](part(c)/CartPole-v1_PG_iters500_bs500_g_lr_rtg_advnorm_.png)\n",
        "![bs5000](part(c)/CartPole-v1_PG_iters500_bs5000_g_lr_rtg_advnorm_.png)\n",
        "<center>Figure-6,7,8: Average Return over 500 episodes for different batch sizes with Reward-to-Go and Advantage Normalization enabled\n",
        "</center>\n",
        "\n",
        "---\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
